{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install rasterio\n",
    "! pip install pyresample\n",
    "! pip install netCDF4\n",
    "! pip install dvc dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import Affine\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pyproj import Transformer\n",
    "import gdown\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import dagshub\n",
    "from dagshub.upload import Repo\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#clone ACOLITE repository \n",
    "! git clone https://github.com/acolite/acolite.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ACOLITE_PATH = \"./acolite\"\n",
    "sys.path.append(ACOLITE_PATH)\n",
    "# Import acolite_run\n",
    "from acolite.acolite.acolite_run import acolite_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Downloading the Littern Windrows Catalog Annotaiton from Zenodo \n",
    "record_id = \"11045944\"\n",
    "netcdf_file_name = \"WASP_LW_SENT2_MED_L1C_B_201506_202109_10m_6y_NRT_v1.0.nc\" \n",
    "zenodo_url = f\"https://zenodo.org/api/records/{record_id}\"\n",
    "\n",
    "# Get the actual download URL\n",
    "\n",
    "r = requests.get(zenodo_url).json()\n",
    "download_url = None\n",
    "\n",
    "for file in r['files']:\n",
    "    print(file)\n",
    "    if file['key'] == netcdf_file_name:\n",
    "        download_url = file['links']['self']\n",
    "        break\n",
    "\n",
    "if download_url:\n",
    "    !wget -O /kaggle/working/annotations.nc {download_url}\n",
    "else:\n",
    "    print(\"File not found in Zenodo record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dagshub.auth import add_app_token\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "dagshub_username = user_secrets.get_secret(\"DAGSHUB_USERNAME\")\n",
    "dagshub_token = user_secrets.get_secret(\"DAGSHUB_TOKEN\")\n",
    "add_app_token(dagshub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! ls /kaggle/input/litter-windrows-batch-cala\n",
    "input_subdirs = [os.path.join('/kaggle/input', f) for f in os.listdir('/kaggle/input/') if 'windrows' in f]\n",
    "if len(input_subdirs) != 0:\n",
    "    safe_files_dir = input_subdirs[0]\n",
    "else:\n",
    "    print('No Litter Windrows Batch Dataset Found in /kaggle/input dir. Add a Litter Windrows Batch Dataset to the notebook !')\n",
    "#safe_files_dir ='/kaggle/input/litter-windrows-batch-cala'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_utm_limit(utm_x, utm_y,utm_crs, box_meters, pixel_size=10):\n",
    "    \"\"\"Compute WGS84 limit from UTM center for box_meters x box_meters.\"\"\"\n",
    "    wgs84_crs = \"EPSG:4326\"\n",
    "    utm_to_wgs = Transformer.from_crs(utm_crs, wgs84_crs, always_xy=True)\n",
    "    \n",
    "    half_box = box_meters / 2\n",
    "    box_left = utm_x - half_box\n",
    "    box_right = utm_x + half_box\n",
    "    box_bottom = utm_y - half_box\n",
    "    box_top = utm_y + half_box\n",
    "    \n",
    "    lon_min, lat_min = utm_to_wgs.transform(box_left, box_bottom)\n",
    "    lon_max, lat_max = utm_to_wgs.transform(box_right, box_top)\n",
    "    limit = [lat_min, lon_min, lat_max, lon_max]\n",
    "    \n",
    "    print(f\"UTM box: left={box_left:.1f}, bottom={box_bottom:.1f}, right={box_right:.1f}, top={box_top:.1f}\")\n",
    "    print(f\"WGS84 limit: {limit}\")\n",
    "    return limit, (box_left, box_bottom, box_right, box_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataset_tiles(safe_files_dir):\n",
    "    tiles = os.listdir(safe_files_dir)\n",
    "    tiles = [str(t)+'.SAFE' for t in tiles]\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List the tiles in the dataset\n",
    "#safe_files_dir = '/kaggle/input/litter-windrows-batch-cala'\n",
    "# tiles = os.listdir(safe_files_dir)\n",
    "# tiles = [str(t)+'.SAFE' for t in tiles]\n",
    "tiles = get_dataset_tiles(safe_files_dir)\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_crs_and_bounds(safe_file):\n",
    "    #granule_path = os.path.join(os.path.join(safe_files_dir, tiles[0]), tiles[0]+'.SAFE/GRANULE/')\n",
    "    granule_path = os.path.join(safe_file, 'GRANULE/')\n",
    "    #print(granule_path)\n",
    "    granule_subdirs = os.listdir(granule_path)  \n",
    "    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n",
    "    jpg2_files = os.listdir(img_data_path)\n",
    "    #print('jpg2_files')\n",
    "    #print(jpg2_files)\n",
    "    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n",
    "    #print('B02_files')\n",
    "    #print(B02_files)\n",
    "    band_path = os.path.join(img_data_path, B02_files[0])\n",
    "    print(band_path)\n",
    "    with rasterio.open(band_path) as src:\n",
    "        print(f\"CRS: {src.crs}\")  # Should be EPSG:32631\n",
    "        print(f\"Bounds: {src.bounds}\")  # Exact UTM coordinates\n",
    "        data = src.read(1)\n",
    "        print(f\"B02 Valid Pixels: {np.sum(data > 0)} / {data.size}\")\n",
    "    return src.crs, src.bounds, src.transform, src.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_all_tiles(files):\n",
    "    \"\"\"\n",
    "    Given the list of the parquet annotation file, builds the list of all the \n",
    "    annotated tiles.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    # Process 10 files per batch\n",
    "    batch_size = 10\n",
    "    tiles = []\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        \n",
    "        # Read batch with Dask (lazy loading)\n",
    "        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "        \n",
    "        # Compute to pandas (triggers parallel read)\n",
    "        batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "        tiles.append(batch_df['s2_product'].unique())\n",
    "       \n",
    "        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "    \n",
    "    tiles = np.unique(np.hstack(tiles))\n",
    "    tiles = [t.decode('utf-8') for t in tiles]\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_jp2_ref_file(safe_file):\n",
    "    granule_path = os.path.join(safe_file, 'GRANULE/')\n",
    "    granule_subdirs = os.listdir(granule_path)  \n",
    "    img_data_path = os.path.join(granule_path, os.path.join(granule_subdirs[0]),'IMG_DATA/')\n",
    "    jpg2_files = os.listdir(img_data_path)\n",
    "    B02_files = [f  for f in jpg2_files if f.endswith('B02.jp2')]\n",
    "    if len(B02_files) == 0:\n",
    "        raise ValueError(f'No B2 band jpg2 file retrieve in the safe file {safe_file}')\n",
    "    else :\n",
    "        return os.path.join(img_data_path, B02_files[0])\n",
    "        \n",
    "def check_for_invalid_data(file_path, bbox):\n",
    "    with rasterio.open(file_path) as src:\n",
    "        # Read the window\n",
    "        window = src.window(*bbox)\n",
    "        data = src.read(1, window=window)\n",
    "    \n",
    "    # Sentinel-2 often uses 0 for invalid pixels\n",
    "    invalid_mask = (data == 0)\n",
    "    \n",
    "    # Or sometimes very high values (like 65535 for uint16)\n",
    "    if data.dtype == np.uint16:\n",
    "        invalid_mask = invalid_mask | (data == 65535)\n",
    "    \n",
    "    has_invalid = invalid_mask.any()\n",
    "    \n",
    "    if has_invalid:\n",
    "        invalid_count = invalid_mask.sum()\n",
    "        print(f\"Found {invalid_count} invalid pixels in the bounding box\")\n",
    "    else:\n",
    "        print(\"No invalid pixels found in the bounding box\")\n",
    "    return has_invalid\n",
    "\n",
    "def generate_tile_regions(safe_file_path):\n",
    "    \"\"\"\n",
    "    Returns regions splitting the tile image\n",
    "    in the form of (xmin, ymin, xmax, ymax)\n",
    "    \"\"\"\n",
    "    w = 10980\n",
    "    h = 10980\n",
    "    stepx = 2700\n",
    "    stepy = 2700\n",
    "    sx = 3000\n",
    "    sy = 3000\n",
    "    regions = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            regions.append((i*stepx, j*stepy, min(w, i*stepx + sx), min(h, j*stepy + sy)))\n",
    "    ref_file = find_jp2_ref_file(safe_file_path)\n",
    "    filtered_regions = []\n",
    "   \n",
    "    for r in regions :\n",
    "        invalid = check_for_invalid_data(ref_file, r)\n",
    "        if not invalid:\n",
    "            filtered_regions.append(r)\n",
    "    regions = {i : r for i, r in enumerate(filtered_regions)}\n",
    "    return regions\n",
    "\n",
    "def assign_patches(tile_regions, patches):\n",
    "    splitted_regions = {}\n",
    "    for p in patches: \n",
    "        for idr, r in tile_regions.items():\n",
    "            inside = p[0] >= r[0] and p[1] >= r[1] and p[2] <= r[2] and p[3] <= r[3]\n",
    "            if inside:\n",
    "                if idr  not in splitted_regions:\n",
    "                    splitted_regions[idr] = [p]\n",
    "                else :\n",
    "                    splitted_regions[idr].append(p)\n",
    "                break\n",
    "    return splitted_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def annotations_from_netcdf(target, netcdf_file_path, output_dir) :\n",
    "    \n",
    "    # Create output directory if it does not exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Step 1: Open NetCDF file with optimized chunks\n",
    "    try:\n",
    "        # Chunk by filaments for better memory management\n",
    "        ds = xr.open_dataset(netcdf_file_path, engine='netcdf4', chunks={'n_filaments': 1000})\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening NetCDF file: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Step 2: Create filter for target product\n",
    "    selected_indices = [2, 4, 5]  \n",
    "    parts = target.split('_')\n",
    "    proc_target = '_'.join(parts[i] for i in selected_indices)\n",
    "    print(f'Target pattern: {proc_target}')\n",
    "\n",
    "    def is_target_product(x):\n",
    "        try:\n",
    "            #if isinstance(x, bytes):\n",
    "            #    x = x.decode('utf-8')\n",
    "            parts = str(x).strip().split('_')\n",
    "            return '_'.join([parts[i] for i in selected_indices]) == proc_target\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # Step 3: Vectorized filtering\n",
    "    # Convert s2_product to string format first\n",
    "    s2_strings = xr.apply_ufunc(\n",
    "        lambda x: str(x.decode('utf-8').strip()) if isinstance(x, bytes) else str(x).strip(),\n",
    "        ds['s2_product'],\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[object]\n",
    "    )\n",
    "\n",
    "    # Create boolean mask\n",
    "    mask = xr.apply_ufunc(\n",
    "        is_target_product,\n",
    "        s2_strings,\n",
    "        vectorize=True,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[bool]\n",
    "    )\n",
    "\n",
    "    # Step 4: Apply filtering\n",
    "    ds_filtered = ds.where(mask.compute(), drop=True)  # Compute mask before filtering\n",
    "\n",
    "    # Step 5: Convert to Dask DataFrame\n",
    "    df = ds_filtered[['pixel_x', 'pixel_y', 'lat_centroid', 'lon_centroid']].to_dask_dataframe()\n",
    "    # Step 6: Additional filtering\n",
    "    df = df[\n",
    "        (df['pixel_x'] != -999) & \n",
    "        (df['pixel_y'] != -999) &\n",
    "        df['pixel_x'].notnull() & \n",
    "        df['pixel_y'].notnull()\n",
    "    ]\n",
    "\n",
    "    # Step 7: Process in partitions\n",
    "    filtered_chunks = []\n",
    "    for partition in df.partitions:\n",
    "        chunk = partition.compute()\n",
    "        filtered_chunks.append(chunk)\n",
    "\n",
    "    filtered_pandas = pd.concat(filtered_chunks)\n",
    "\n",
    "    print(f\"Final filtered DataFrame shape: {filtered_pandas.shape}\")\n",
    "    print(filtered_pandas.head())\n",
    "\n",
    "    # Step 8: Save results\n",
    "    output_fname = target\n",
    "    if '.SAFE' in output_fname:\n",
    "        output_fname = output_fname[:-5]\n",
    "    output_csv = f\"{output_dir}/{output_fname}_LWC_annotations.csv\"\n",
    "    filtered_pandas.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved filtered DataFrame: {output_csv}\")\n",
    "    ds.close()\n",
    "    return filtered_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_tile_df(target, files):\n",
    "    \"\"\"\n",
    "    Retrieves all annotations for a given target tile.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    # Process 10 files per batch\n",
    "    batch_size = 10\n",
    "    tg_id_1 = target.split('_')[2]\n",
    "    tg_id_2 = '_'.join(target.split('_')[4:6])\n",
    "    for i in range(0, len(files), batch_size):\n",
    "        batch_files = files[i:i + batch_size]\n",
    "        \n",
    "        # Read batch with Dask (lazy loading)\n",
    "        ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "        \n",
    "        # Compute to pandas (triggers parallel read)\n",
    "        batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "        batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n",
    "        batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)]\n",
    "        if not batch_df.empty:\n",
    "            if df is None:\n",
    "                df = batch_df.copy()\n",
    "            else:\n",
    "                df = pd.concat([df, batch_df], ignore_index=True)\n",
    "        print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_tile_df(target, tile_parquet_dir):\n",
    "    fpathname = os.path.join(tile_parquet_dir, f'LWR_{target[:-5]}.parquet')\n",
    "    ddf = dd.read_parquet(fpathname, engine='pyarrow')\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_tiles_parquet_annotations(tiles, files):\n",
    "    \"\"\"\n",
    "    Retrieves all annotations for a given target tile.\n",
    "    \"\"\"\n",
    "    for target in tiles:\n",
    "        df = None\n",
    "        # Process 10 files per batch\n",
    "        batch_size = 10\n",
    "        tg_id_1 = target.split('_')[2]\n",
    "        tg_id_2 = '_'.join(target.split('_')[4:6])\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch_files = files[i:i + batch_size]\n",
    "            \n",
    "            # Read batch with Dask (lazy loading)\n",
    "            ddf = dd.read_parquet(batch_files, engine='pyarrow')\n",
    "            \n",
    "            # Compute to pandas (triggers parallel read)\n",
    "            batch_df = ddf.compute()  # Now contains data from 10 files\n",
    "            batch_df[\"s2_product\"] = batch_df[\"s2_product\"].str.decode('utf-8')\n",
    "            batch_df = batch_df[batch_df[\"s2_product\"].str.contains(tg_id_1) & batch_df[\"s2_product\"].str.contains(tg_id_2)  &\n",
    "    (~batch_df[\"pixel_x\"].isna()) &\n",
    "    (~batch_df[\"pixel_y\"].isna()) &\n",
    "    (batch_df[\"pixel_x\"] != -999) &\n",
    "    (batch_df[\"pixel_y\"] != -999) ]\n",
    "            if not batch_df.empty:\n",
    "                if df is None:\n",
    "                    df = batch_df.copy()\n",
    "                else:\n",
    "                    df = pd.concat([df, batch_df], ignore_index=True)\n",
    "            print(f\"Batch {i//batch_size + 1}: {len(batch_df)} rows (from {len(batch_files)} files)\")\n",
    "        # Save to Parquet\n",
    "        df.to_parquet(f'LWR_{target[:-5]}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_relative_path(target_path, start_path):\n",
    "    \"\"\"\n",
    "    Returns the relative path from start_path to target_path\n",
    "    \"\"\"\n",
    "    return os.path.relpath(target_path, start_path)\n",
    "\n",
    "\n",
    "def get_all_files_recursive(directory):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        #print(files)\n",
    "        for file in files:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            if os.path.isfile(full_path):  # Check to ensure it's a file\n",
    "                file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "def upload_to_dagshub(root_folder, src_folder, dst_folder, batch_id, dagshub_token):\n",
    "    \"\"\"\n",
    "    dst_folder: Must have the format './dest_folder',\n",
    "    where dst_folder is the destination folder for the images\n",
    "    relative to the root of the repository.\n",
    "    \n",
    "    root_folder: The relative path of src_folder with respect to root_folder\n",
    "    will determine the position of the uploaded file relative to the root\n",
    "    of the repository.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining the Repo & directory\n",
    "    add_app_token(dagshub_token)\n",
    "    repo = Repo(\"elena-andreini\", \"TriesteItalyChapter_PlasticDebrisDetection\")\n",
    "    ds = repo.directory(dst_folder)\n",
    "    files = get_all_files_recursive(src_folder)\n",
    "    print(f'uploading files {files}')\n",
    "    rel_files_paths = [get_relative_path(f, src_folder) for f in files]\n",
    "    print(f'relative files {rel_files_paths}')\n",
    "\n",
    "    for i in rel_files_paths:\n",
    "      ds.add(file=os.path.join(src_folder, i), path=f'./{i}')\n",
    "    \n",
    "    # Commit the changes\n",
    "    ds.commit(f'Adding batch {batch_id}  to {dst_folder} folder', versioning='dvc')\n",
    "    print(f'Uploaded batch {batch_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define settings form map-mapper paper\n",
    "settings = {\n",
    "    # Ensure TIFF export\n",
    "    'l2r_export_geotiff': True,  \n",
    "    # delete .nc once made to geotiff\n",
    "    'l1r_delete_netcdf' : True,\n",
    "    'l2w_delete_netcdf' : True,\n",
    "    'verbosity': 5,  # Detailed logging\n",
    "    's2_target_res': 10,  # 10m resolution\n",
    "    'resampling_method': 'nearest' , # Set to nearest neighbor\n",
    "    'atmospheric_correction_method': 'dark_spectrum',\n",
    "    'dsf_exclude_bands' : ['B9', 'B10'],\n",
    "    # resolves issue with none type see this thread - https://odnature.naturalsciences.be/remsem/acolite-forum/viewtopic.php?t=319\n",
    "    'geometry_type' : 'grids',\n",
    "    #masking\n",
    "    'l2w_mask' : False,\n",
    "    #sunglint\n",
    "    'glint_correction' : True,\n",
    "    'dsf_residual_glint_correction' : True,\n",
    "    'dsf_residual_glint_correction_method' : 'alternative', # index error occuring with default\n",
    "    'dsf_residual_glint_wave_range' : [1500,2400],\n",
    "    'glint_force_band' : None,\n",
    "    'glint_mask_rhos_wave' : 1600,\n",
    "    'glint_mask_rhos_threshold' : 0.11,\n",
    "    'reproject' : False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_tile_mask(tile_df):\n",
    "    \"\"\"\n",
    "    Generate mask for the whole tile.\n",
    "    Based on dhia's code.\n",
    "    Not filtering filaments by box_dims size at the moment\n",
    "    \"\"\"\n",
    "    mask = np.zeros((10980, 10980))\n",
    "    pixels = 0\n",
    "    #pb = tqdm(tile_df[tile_df[\"box_dims\"] == 3].iterrows())\n",
    "    pb = tqdm(tile_df.iterrows())\n",
    "    for index, row in pb:\n",
    "        x, y = row[\"pixel_x\"], row[\"pixel_y\"]\n",
    "        if not np.isnan(x) and not np.isnan(y):\n",
    "            pixels += 1\n",
    "            x, y = int(x), int(y)\n",
    "            mask[x, y] = 1\n",
    "    return mask, pixels\n",
    "    \n",
    "def find_subregions_efficient(binary_image, subregion_size, threshold):\n",
    "    \"\"\"\n",
    "    Finds regions with debris pixels in the whole tile annotation mask.\n",
    "    Using integral images for large binary images.\n",
    "    \"\"\"\n",
    "    h, w = subregion_size\n",
    "    img_h, img_w = binary_image.shape\n",
    "    print(f'input maks shape : {binary_image.shape}')\n",
    "    # Convert to binary and create integral image\n",
    "    binary = (binary_image == 1).astype(np.uint8)\n",
    "    integral = cv2.integral(binary)\n",
    "    \n",
    "    subregions = []\n",
    "    covered = np.zeros_like(binary, dtype=bool)\n",
    "    \n",
    "    for y in range(0, img_h - h + 1, h):\n",
    "        for x in range(0, img_w - w + 1, w):\n",
    "            #if covered[y:y+h, x:x+w].any():\n",
    "            #    continue\n",
    "                \n",
    "            # Calculate sum using integral image\n",
    "            total = integral[y+h, x+w] - integral[y, x+w] - integral[y+h, x] + integral[y, x]\n",
    "            white_fraction = total / (h * w)\n",
    "            #print(f'white_fraction {white_fraction}')\n",
    "            if total > threshold:\n",
    "                subregions.append((x, y,  x + w,  y + h, total))\n",
    "                covered[y:y+h, x:x+w] = True\n",
    "                \n",
    "    return subregions\n",
    "\n",
    "def minimal_1024_cover(small_regions):\n",
    "    \"\"\"\n",
    "    This function could be used to find larger regions around patches\n",
    "    for ACOLITE application.\n",
    "    DSF algorithm could be not completely reliable applied to regions of 2560x2560 m unless\n",
    "    they are very homogeneous\n",
    "    \"\"\"\n",
    "    # Converti le coordinate in celle della griglia 256×256\n",
    "    cells = set((x // 1024, y // 1024) for (y, x, _, _,_) in small_regions)\n",
    "    if not cells:\n",
    "        return []\n",
    "    large_regions = set()\n",
    "    for (i, j) in cells:\n",
    "        large_i = i * 1024\n",
    "        large_j = j * 1024\n",
    "        large_regions.add((large_i, large_j))\n",
    "    return large_regions\n",
    "\n",
    "\n",
    "def upload_to_drive(file_path, drive_folder_id):\n",
    "    \"\"\"Upload file to Google Drive.\"\"\"\n",
    "    output = gdown.upload(file_path, parent_id=drive_folder_id, quiet=False)\n",
    "    print(f\"Uploaded {file_path} to Drive folder {drive_folder_id}\")\n",
    "    os.remove(file_path)  # Clear Kaggle disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_image(image):\n",
    "    # Define invalid values (NaN, Inf, or negative)\n",
    "    invalid_mask = np.isnan(image) | np.isinf(image) | (image < 0)\n",
    "    total_pixels = image.size\n",
    "    invalid_count = invalid_mask.sum()\n",
    "    invalid_percentage = (invalid_count / total_pixels) * 100\n",
    "    print(f\"Percentage of invalid values (NaN, Inf, negative) across all bands: {invalid_percentage:.2f}%\")\n",
    "    if invalid_percentage > 10:\n",
    "        print('skipping region, too many invalid pixels ')\n",
    "        return False\n",
    "    else :\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#### Stacking \n",
    "\n",
    "\n",
    "def stack_bands(acolite_output_dir, stacked_dir, tile):\n",
    "    # Mapping wawelength to band names\n",
    "    wl_to_band  ={\n",
    "        443 : 'B1',\n",
    "        492 : 'B2',\n",
    "        560 : 'B3',\n",
    "        665 : 'B4',\n",
    "        704 : 'B5',\n",
    "        740 : 'B6',\n",
    "        783 : 'B7',\n",
    "        833 : 'B8',\n",
    "        865 : 'B8A',\n",
    "        1614 : 'B11',\n",
    "        2202 : 'B12'\n",
    "    }\n",
    "    rhos_files = [f for f in os.listdir(acolite_output_dir) if 'rhos' in f and '.tif' in f]\n",
    "    rhos_files = sorted(rhos_files, key=lambda x: int(x.split('_')[-1][:-4]))\n",
    "    band_files = [(wl_to_band[int(f.split('_')[-1][:-4])], f) for i,f in enumerate(rhos_files) ]\n",
    "\n",
    "    # Verify all files exist\n",
    "    for band, filename in band_files:\n",
    "        if not os.path.exists(os.path.join(acolite_output_dir, filename)):\n",
    "            raise FileNotFoundError(f\"Missing file: {filename}\")\n",
    "\n",
    "    # Reference band (use B4 for metadata, since all bands are 10m)\n",
    "    reference_band = 'B4'\n",
    "    reference_file = os.path.join(acolite_output_dir, [f for b, f in band_files if b == reference_band][0])\n",
    "\n",
    "    # Open the reference GeoTIFF to get metadata\n",
    "    with rasterio.open(reference_file) as ref:\n",
    "        ref_profile = ref.profile\n",
    "        ref_transform = ref.transform\n",
    "        ref_crs = ref.crs\n",
    "        ref_width = ref.width\n",
    "        ref_height = ref.height\n",
    "        ref_resolution = ref.res  # Should be (10.0, 10.0)\n",
    "\n",
    "    # Initialize an array to store all bands\n",
    "    stacked_data = np.zeros((len(band_files), ref_height, ref_width), dtype=np.float32)\n",
    "    \n",
    "    # Read each band (no resampling needed, all are 10m)\n",
    "    for i, (band, filename) in enumerate(band_files):\n",
    "        file_path = os.path.join(acolite_output_dir, filename)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Verify resolution matches\n",
    "            if src.res != ref_resolution:\n",
    "                raise ValueError(f\"Resolution mismatch in {filename}: expected {ref_resolution}, got {src.res}\")\n",
    "            # Verify dimensions match\n",
    "            if src.width != ref_width or src.height != ref_height:\n",
    "                raise ValueError(f\"Dimensions mismatch in {filename}: expected {ref_width}x{ref_height}, got {src.width}x{src.height}\")\n",
    "            # Read the band\n",
    "            data = src.read(1, out_dtype=np.float32)  # Single band, float32\n",
    "            stacked_data[i] = data\n",
    "\n",
    "    # Update the profile for the stacked GeoTIFF\n",
    "    stacked_profile = ref_profile.copy()\n",
    "    stacked_profile.update({\n",
    "        'count': len(band_files),  # 11 bands\n",
    "        'dtype': np.float32,  # For reflectance\n",
    "        'transform': ref_transform,\n",
    "        'crs': ref_crs,\n",
    "        'width': ref_width,\n",
    "        'height': ref_height,\n",
    "        'nodata': -999  # Optional: Set nodata value (adjust if ACOLITE uses NaN)\n",
    "    })\n",
    "    # Determine patch name similar to MARIDA : S2_DATE_TILE_REGION in folder S2_DATE_TILE\n",
    "    stacked_tif_fname =  'S2_'+('_'.join(tile.split('_')[2:-3]))+'_stacked.tiff'\n",
    "    stacked_tif = os.path.join(stacked_dir, stacked_tif_fname)\n",
    "    # Save the stacked GeoTIFF\n",
    "    with rasterio.open(stacked_tif, 'w', **stacked_profile) as dst:\n",
    "        dst.write(stacked_data)\n",
    "        # Set band descriptions in the usual order\n",
    "        for i, (band, _) in enumerate(band_files, 1):\n",
    "            dst.set_band_description(i, band)\n",
    "    \n",
    "    print(f\"Stacked GeoTIFF saved to: {stacked_tif}\")\n",
    "\n",
    "    # Verify the stacked GeoTIFF\n",
    "    with rasterio.open(stacked_tif) as stacked:\n",
    "        print(f\"Stacked GeoTIFF:\")\n",
    "        print(f\"Bounds: {stacked.bounds}\")\n",
    "        print(f\"Dimensions: {stacked.width} columns, {stacked.height} rows\")\n",
    "        print(f\"Resolution: {stacked.res}\")\n",
    "        print(f\"CRS: {stacked.crs}\")\n",
    "        print(f\"Number of bands: {stacked.count}\")\n",
    "        print(f\"Band descriptions: {stacked.descriptions}\")\n",
    "    valid = validate_image(stacked_data)\n",
    "    return stacked_tif, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "######## Cropping\n",
    "\n",
    "\n",
    "# Paths to the original GeoTIFF, stacked GeoTIFF, and output\n",
    "def crop_stacked_file(stacked_tif, cropped_tif, utm_bounds, crs, tile_res):\n",
    "    \n",
    "    # Computing metadata for the original patch (before ACOLITE correction)\n",
    "    ##########################################################\n",
    "    with rasterio.open(stacked_tif) as src:\n",
    "        # Calculate the window to read\n",
    "        window = from_bounds(*utm_bounds, transform=src.transform)\n",
    "\n",
    "        # Read the data in this window\n",
    "        data = src.read(window=window)\n",
    "    \n",
    "        # Update the transform for the new cropped image\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "    \n",
    "        # Get band descriptions to preserve metadata\n",
    "        band_descriptions = src.descriptions\n",
    "        \n",
    "        # Write the cropped image\n",
    "        with rasterio.open(\n",
    "            cropped_tif,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=window.height,\n",
    "            width=window.width,\n",
    "            count=src.count,\n",
    "            dtype=data.dtype,\n",
    "            crs=src.crs,\n",
    "            transform=transform,\n",
    "        ) as dst:\n",
    "            # Preserve band descriptions (e.g., B1, B2, ...)\n",
    "            for i, desc in enumerate(band_descriptions, 1):\n",
    "                if desc:  # Only set if description exists\n",
    "                    dst.set_band_description(i, desc)\n",
    "            dst.write(data)\n",
    "    \n",
    "    print(f\"Cropped GeoTIFF saved to: {cropped_tif}\")\n",
    "\n",
    "    # Verify the cropped GeoTIFF\n",
    "    with rasterio.open(cropped_tif) as cropped:\n",
    "        print(f\"Cropped GeoTIFF:\")\n",
    "        print(f\"Bounds: {cropped.bounds}\")\n",
    "        print(f\"Dimensions: {cropped.width} columns, {cropped.height} rows\")\n",
    "        print(f\"Resolution: {cropped.res}\")\n",
    "        print(f\"CRS: {cropped.crs}\")\n",
    "        print(f\"Number of bands: {cropped.count}\")\n",
    "        print(f\"Band descriptions: {cropped.descriptions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##### Run if you want to extract all tiles from the LW annotation file\n",
    "#%%time\n",
    "#all_tiles = get_all_tiles(annotations_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_tile(target, safe_files_dir, annotations_file, output_dir, remote_dataset_location = './LWC_DataSet/test_data', batch_id='', clean_patches = True, dagshub_token=''):\n",
    "    acolite_output_dir = os.path.join(output_dir, 'acolite_output/')\n",
    "    stacked_dir = os.path.join(output_dir, 'stacked/')\n",
    "    final_dir = os.path.join(output_dir, 'patches/')\n",
    "    os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "    os.makedirs(stacked_dir, exist_ok=True)\n",
    "    if clean_patches : \n",
    "        shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    print(f'building annotations dataframe for tile {target}')\n",
    "    #tile_df = build_tile_df(target, annotations_files)\n",
    "    tile_df = annotations_from_netcdf(target, annotations_file, output_dir)\n",
    "    mask, debris_pixels_count = generate_tile_mask(tile_df)\n",
    "    print(f'tile mask shape {mask.shape}')\n",
    "    if debris_pixels_count == 0:\n",
    "        print(f'no valid debris pixels in tile {target}')\n",
    "        return False\n",
    "    patches = find_subregions_efficient(mask, (256, 256), 25) #### CHANGED THRESHOLD TO 25 HERE TO EXLUDE MINIMAL PLASTIC PATCHES \n",
    "    print(f'patches with marine debris : {patches}')\n",
    "    target_file_path = os.path.join(os.path.join(safe_files_dir, target[:-5]), target)\n",
    "    crs, bounds, tile_transform, tile_res = extract_crs_and_bounds(target_file_path)       \n",
    "    print(f'crs : {crs}')\n",
    "    print(f'bounds : {bounds}')\n",
    "    res = tile_res[0] #sentinel 2 max resolution\n",
    "    tile_regions = generate_tile_regions(target_file_path)\n",
    "    patches_per_region = assign_patches(tile_regions, patches)\n",
    "    print(f'patches per region {patches_per_region}')\n",
    "    for i, ppr in patches_per_region.items(): #iterates over regions containing patches\n",
    "        shutil.rmtree(stacked_dir, ignore_errors=True)\n",
    "        os.makedirs(stacked_dir, exist_ok=True)\n",
    "        current_region = tile_regions[i]\n",
    "        current_center = ((current_region[0] + current_region[2])/2 * res + bounds.left,\n",
    "        (current_region[1] + current_region[3])/2 * (-res) + bounds.top)\n",
    "        #subregion_centers = [((int(p[1] + p[3])/2) * res + bounds.left, int((p[0] + p[2])/2) * res + bounds.bottom) for p in patches]\n",
    "        print(f'region center : {current_center}')\n",
    "        #subregion_size = (256 * res, 256 * res)\n",
    "        current_region_size = ((current_region[2] - current_region[0]) * res, \n",
    "                               (current_region[3] - current_region[1]) * res )\n",
    "        try:\n",
    "            # Check disk usage\n",
    "            print(\"Disk usage before run:\")\n",
    "            !du -sh /kaggle/input/*\n",
    "            !df -h /kaggle/tmp /kaggle/working\n",
    "        \n",
    "            # Clear output directory\n",
    "            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "            os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        # Process each subregion\n",
    "        #for i, (subregion_utm_x, subregion_utm_y) in enumerate(subregion_centers):\n",
    "            # Clear output directory\n",
    "            subregion_utm_x, subregion_utm_y = current_center\n",
    "            print(f'Cleaning previous acolite output')\n",
    "            shutil.rmtree(acolite_output_dir, ignore_errors=True)\n",
    "            os.makedirs(acolite_output_dir, exist_ok=True)\n",
    "            print(f\"\\nProcessing subregion {i+1}/{len(tile_regions)} centered at UTM ({subregion_utm_x}, {subregion_utm_y})\")\n",
    "    \n",
    "            # Compute UTM-based limit\n",
    "            limit, utm_box = get_utm_limit(subregion_utm_x, subregion_utm_y, crs, current_region_size[0]) #subregion_size[0])\n",
    "            print(f'setting limit : {limit}')\n",
    "            settings['limit'] = limit\n",
    "            print(f\"Settings for subregion {i+1}: {settings}\")\n",
    "            output_files = acolite_run(settings=settings, inputfile=target_file_path, output=acolite_output_dir)\n",
    "            print(f\"Generated output files {output_files}\")\n",
    "            print('stacking bands')\n",
    "            stacked_tiff, valid = stack_bands(acolite_output_dir, stacked_dir, target)\n",
    "            if not valid : \n",
    "                print(f'skipping region {current_region} : too many invalid pixels')\n",
    "                continue\n",
    "            patch_dir =  os.path.join(final_dir, '_'.join((stacked_tiff.split('/')[-1]).split('_')[:-1]))# '_'.join(stacked_tiff.split('_')[:-1])\n",
    "            print(f'patch dir {patch_dir}')\n",
    "            for p in ppr:\n",
    "                print(f'cropping  patch {p}')\n",
    "                patch_utm_bound = (p[0] * res + bounds.left, (p[3]) * (-res) + bounds.top,  ## CHANGED -1 removed\n",
    "                                   p[2] * res + bounds.left,  p[1] * (-res) + bounds.top)\n",
    "                patch_name_prefix = patch_dir.split('/')[-1]\n",
    "                patch_name = f'{patch_name_prefix}_{int(patch_utm_bound[0])}_{int(patch_utm_bound[1])}.tif'  ## CHANGED one f removed from .tiff\n",
    "                patch_pathname = os.path.join(patch_dir, patch_name)\n",
    "                os.makedirs(patch_dir, exist_ok=True)\n",
    "                print(f'patch pathname {patch_pathname}')\n",
    "                crop_stacked_file(stacked_tiff, os.path.join(patch_dir, patch_pathname), patch_utm_bound, crs, tile_res)\n",
    "                patch_mask = mask[p[1] : p[3], p[0] : p[2]]\n",
    "                print(f'patch mask shape {patch_mask.shape}')\n",
    "                try:\n",
    "                    #cv2.imwrite(os.path.join(patch_dir, f'{patch_pathname[:-4]}_cl.tif'), patch_mask)\n",
    "                    written_patch_path = os.path.join(patch_dir, patch_name)\n",
    "                    mask_output_path = os.path.join(patch_dir, f'{patch_name[:-4]}_cl.tif')\n",
    "                    print(f\"Writing mask for: {mask_output_path}\")\n",
    "                    print(f\"Mask shape: {patch_mask.shape}, dtype: {patch_mask.dtype}\")\n",
    "                    with rasterio.open(written_patch_path) as src:\n",
    "                        meta = src.meta.copy()\n",
    "                        print(f\"Original patch size: {(src.width, src.height)}\")\n",
    "                        if patch_mask.shape != (src.height, src.width):\n",
    "                            raise ValueError(\" Mask size does not match patch size\")\n",
    "                    meta.update({\n",
    "                        \"count\": 1,\n",
    "                        \"dtype\": \"uint8\",\n",
    "                        \"compress\": \"lzw\"\n",
    "                    })\n",
    "                    with rasterio.open(mask_output_path, \"w\", **meta) as dst:\n",
    "                        dst.write(patch_mask.astype(\"uint8\"), 1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error writing mask: {e}\")\n",
    "            # Insert Sanity Check for ACOLITE output ? \n",
    "            \n",
    "            # Upload to DagsHub\n",
    "            if dagshub_token:\n",
    "                upload_to_dagshub(output_dir, final_dir, remote_dataset_location, batch_id, dagshub_token)\n",
    "    \n",
    "    \n",
    "\n",
    "            # Final disk usage\n",
    "            print(\"\\nDisk usage after run:\")\n",
    "            ! du -sh /kaggle/input/*\n",
    "            ! df -h /kaggle/tmp /kaggle/working\n",
    "      \n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Exception captured: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Setting the directory of the annotations file split and converted to parquet format\n",
    "#parquet_annotations_dir = '/kaggle/input/lw-parquet/kaggle/working'\n",
    "#annotations_files = [os.path.join(parquet_annotations_dir, f) for f in os.listdir(parquet_annotations_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#build_tiles_parquet_annotations([tiles[5]], annotations_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_batch(tiles, safe_files_dir, annotations_file, batch_id='', dagshub_token=''):\n",
    "    \"\"\"\n",
    "    tiles :  list of tiles (SAFE files) in batch batch_id\n",
    "    \"\"\"\n",
    "    clean_patches = len(dagshub_token) != 0\n",
    "    for target in tiles:\n",
    "        process_tile(target, safe_files_dir, annotations_file, '/kaggle/working', batch_id, clean_patches =clean_patches, dagshub_token=dagshub_token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "process_batch(tiles,safe_files_dir, '/kaggle/working/annotations.nc',  batch_id='', dagshub_token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# will this be fully automated in process_tile with dagshubtoken = dagshubtoken?\n",
    "upload_to_dagshub('/kaggle/working', '/kaggle/working/patches', './LWC_DataSet/test_data', \"\", dagshub_token=dagshub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sagar's & Navodita's added matching visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_fdi_from_tiff(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        # Assuming band order follows your stacked TIFF (B1–B12, skipping B10 if needed)\n",
    "        # Band indices are 1-based in rasterio\n",
    "        R665 = src.read(4)    # B4\n",
    "        R859 = src.read(9)    # B8A\n",
    "        R1610 = src.read(10)  # B11\n",
    "        # Convert to float and mask invalid values\n",
    "        R665 = R665.astype(np.float32)\n",
    "        R859 = R859.astype(np.float32)\n",
    "        R1610 = R1610.astype(np.float32)\n",
    "        # Calculate FDI\n",
    "        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n",
    "        return FDI\n",
    "def compute_ndwi(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        Rgreen = src.read(3).astype(np.float32)  # Band 3 (Green)\n",
    "        Rnir = src.read(8).astype(np.float32)    # Band 8 (NIR)\n",
    "        ndwi = (Rgreen - Rnir) / (Rgreen + Rnir + 1e-6)  # avoid divide-by-zero\n",
    "    return ndwi\n",
    "def plot_fdi(fdi_array, ndwi, img_path, mask_path):\n",
    "    with rasterio.open(img_path) as src:\n",
    "        rgb = src.read([4, 3, 2])\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    # Normalization\n",
    "    rgb = rgb.astype(np.float32)\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask = src.read(1)\n",
    "    # Create binary mask\n",
    "    mask_binary = mask > 0\n",
    "    # Plot side-by-side\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Patch\")\n",
    "    axs[1].imshow(mask_binary)  #, cmap='gray')\n",
    "    axs[1].set_title(\"Binary Mask (._cl.tif)\")\n",
    "    axs[2].imshow(fdi_array)\n",
    "    axs[2].set_title(\"FDI\")\n",
    "    axs[3].imshow(ndwi)\n",
    "    axs[3].set_title(\"NDWI\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualises RGB, mask, FDI and NDWI\n",
    "# Edit paths to loop over all patches in folder\n",
    "\n",
    "image_path = '/kaggle/working/patches/S2_20190531T100031_N0500/S2_20190531T100031_N0500_305120_5002720.tif'\n",
    "mask_path = '/kaggle/working/patches/S2_20190531T100031_N0500/S2_20190531T100031_N0500_305120_5002720_cl.tif'\n",
    "fdi = compute_fdi_from_tiff(image_path)\n",
    "ndwi = compute_ndwi(image_path)\n",
    "plot_fdi(fdi, ndwi, image_path, mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualises RGB, mask, and matched overlay\n",
    "# Edit paths to loop over all patches in folder and combine with FDI \n",
    "patch_dir = \"/kaggle/working/patches/S2_20190531T100031_N0500\"\n",
    "patch_paths = sorted([p for p in glob(os.path.join(patch_dir, \"*.tif\")) if not p.endswith('*_cl.tif')])\n",
    "mask_paths = sorted(glob(os.path.join(patch_dir, \"*_cl.tif\")))\n",
    "# Ensure both have same count\n",
    "print(f\"Found {len(patch_paths)} RGB patches and {len(mask_paths)} masks.\")\n",
    "# Plot Patch, Mask, and Overlay side-by-side\n",
    "debris_pixel_counts = []\n",
    "plt.figure(figsize=(15, 5 * len(mask_paths)))\n",
    "for i, (patch_path, mask_path) in enumerate(zip(patch_paths, mask_paths)):\n",
    "    with rasterio.open(patch_path) as patch_src:\n",
    "        rgb = patch_src.read([4, 3, 2])  # Use bands B4, B3, B2 for RGB\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "        rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n",
    "    with rasterio.open(mask_path) as mask_src:\n",
    "        mask = mask_src.read(1)\n",
    "        mask_binary = (mask > 0).astype(np.uint8)\n",
    "    debris_pixel_counts.append(np.sum(mask_binary))\n",
    "    # Create overlay manually\n",
    "    overlay = rgb.copy()\n",
    "    overlay[mask_binary == 1] = [1.0, 0.0, 0.0]  # Red color on debris\n",
    "    # Plot\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+1)\n",
    "    plt.imshow(rgb)\n",
    "    plt.title(f\"Patch\\n{os.path.basename(patch_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+2)\n",
    "    plt.imshow(mask_binary, cmap='gray')\n",
    "    plt.title(f\"Mask\\nDebris pixels: {int(np.sum(mask_binary))}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(len(mask_paths), 3, 3*i+3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay: Mask on Patch\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Histogram for debris pixels\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(debris_pixel_counts, bins=10, color='teal', edgecolor='black')\n",
    "plt.title(\"Histogram of Debris Pixels per Patch\")\n",
    "plt.xlabel(\"Debris Pixels\")\n",
    "plt.ylabel(\"Number of Patches\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163561,
     "sourceId": 11436527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
