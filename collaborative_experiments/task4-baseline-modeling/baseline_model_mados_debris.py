# -*- coding: utf-8 -*-
"""Baseline_Model_Mados_debris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15HyINZRpNFtBij9UoDLKPIXSYFhEb6AI
"""

pip install -q rasterio folium geopandas shapely plotly kagglehub #necessary imports, some not present in colab

pip install --upgrade albumentations

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm.notebook import tqdm
import rasterio
from rasterio.plot import show
import folium
from folium.plugins import MarkerCluster
import geopandas as gpd
from shapely.geometry import Point
import glob
import cv2
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import plotly.graph_objects as go
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import kagglehub
import zipfile
import shutil
from datetime import datetime
import re

"""Driver Connection"""

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive





"""Download Mados Data from Driver

###Mados Data
"""

import zipfile
import os

# Path to the ZIP file
zip_path ="/content/drive/MyDrive/MADOS.zip"

# Directory to extract the files
extract_path = "/content"
os.makedirs(extract_path, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"Files extracted to: {extract_path}")

print("Dataset downloaded to:", extract_path)
print("Contents of the path:", os.listdir(extract_path))

import os

def get_folder_size_gb(folder_path):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(folder_path):
        for filename in filenames:
            file_path = os.path.join(dirpath, filename)
            total_size += os.path.getsize(file_path)

    # Convert bytes to gigabytes (1 GB = 1024 * 1024 * 1024 bytes)
    size_gb = total_size / (1024 ** 3)
    return size_gb

# Example usage:
folder_path = '/content/MADOS'
size_gb = get_folder_size_gb(folder_path)
print(f"Size of folder '{folder_path}': {size_gb:.2f} GB")

mados_path = "/content/MADOS/splits"
print("Contents of MADOS directory:", os.listdir(mados_path))

import os

extract_path = "/content/MADOS"  # Replace with the actual path
print("Contents of the extracted directory:")
print(os.listdir(extract_path))

# Set up the environment
import warnings
warnings.filterwarnings('ignore')

"""Delete DataBase"""

#import shutil

dataset_path = "/content/merged_marida_mados/processed" # Adjust if needed

if os.path.exists("/content/merged_marida_mados/processed"):
    shutil.rmtree("/content/merged_marida_mados/processed")

    print("MADOS dataset deleted successfully.")
else:
    print("Dataset not found, nothing to delete.")

"""Download Marida Database"""

import zipfile
import os

# Path to the ZIP file
zip_path1 ="/content/drive/MyDrive/MARIDA.zip"

# Directory to extract the files
extract_path1 = "/content/marida_data"
os.makedirs(extract_path, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_path1, 'r') as zip_ref:
    zip_ref.extractall(extract_path1)

print(f"Files extracted to: {extract_path1}")

print("Dataset downloaded to:", extract_path1)
print("Contents of the path:", os.listdir(extract_path1))

"""To Visualize images and content"""

import rasterio
import numpy as np
import matplotlib.pyplot as plt

# Open the file
file_path = "/content/MADOS/Scene_104/10/Scene_104_L2W_TUR_Nechad2016_665_9.tif"
with rasterio.open(file_path) as src:
    conf_data = src.read(1)  # Read the first band
    print("Metadata:", src.meta)  # Check dtype, shape, etc.
    print("Unique values:", np.unique(conf_data))
    print("Min/Max:", np.min(conf_data), np.max(conf_data))

    # Visualize
    plt.imshow(conf_data, cmap='gray', vmin=0, vmax=1)
    plt.colorbar(label='Confidence (0=low, 1=high)')
    plt.title("Confidence Mask")
    plt.show()

import rasterio

with rasterio.open('/content/marida_data/patches/S2_1-12-19_48MYU/S2_1-12-19_48MYU_1_cl.tif') as src:
        num_bands = src.count
        print("Number of bands:", num_bands)

import rasterio
import numpy as np
from matplotlib import pyplot as plt

# Open the classification label file
cl_file = "/content/MADOS/Scene_0/10/Scene_0_L2R_cl_1.tif"
with rasterio.open(cl_file) as src:
    cl_data = src.read(1)
    print("Label metadata:", src.meta)
    print("Unique label values:", np.unique(cl_data))

# Create a color map for visualization (adjust colors to match your classes)
from matplotlib.colors import ListedColormap
class_colors = ['black', 'green', 'blue', 'red', 'yellow', 'purple']  # Example for 6 classes
cmap = ListedColormap(class_colors)

plt.figure(figsize=(8, 6))
plt.imshow(cl_data, cmap=cmap, interpolation='none')
plt.colorbar(ticks=np.unique(cl_data), label='Class Labels')
plt.title("Classification Label Mask (_cl.tif)")
plt.show()

import rasterio
conf_data = rasterio.open("/content/MADOS/Scene_0/10/Scene_0_L2R_cl_1.tif").read(1)  # Reads the first band
print(conf_data[10, 20])  # Prints confidence at pixel (10, 20), e.g., `0.7`

'''import os
from pathlib import Path
import shutil
from tqdm import tqdm

# Base directory
base_dir = Path("/content/MADOS")
output_dir = Path("/content/extracted_data_Mados")

# Create main output subfolders
(output_dir / "images").mkdir(parents=True, exist_ok=True)
(output_dir / "conf").mkdir(parents=True, exist_ok=True)
(output_dir / "cl").mkdir(parents=True, exist_ok=True)

def process_scene(scene_path):
    """Copy files to organized subfolders without renaming"""
    for file_path in scene_path.glob("*"):
        # Handle both .png and .tif RGB files
        if "_rgb_" in file_path.name.lower() and file_path.suffix.lower() in ('.png', '.tif'):
            shutil.copy2(file_path, output_dir / "images" / file_path.name)
        elif "_conf_" in file_path.name.lower() and file_path.suffix.lower() == '.tif':
            shutil.copy2(file_path, output_dir / "conf" / file_path.name)
        elif "_cl_" in file_path.name.lower() and file_path.suffix.lower() == '.tif':
            shutil.copy2(file_path, output_dir / "cl" / file_path.name)

# Process all scenes from 0 to 173
for scene_num in tqdm(range(174), desc="Processing Scenes"):
    scene_path = base_dir / f"Scene_{scene_num}"

    if scene_path.exists():
        # Process each subdirectory (10, 20, etc.) within the scene
        for subdir in scene_path.glob("*/"):
            if subdir.is_dir():
                process_scene(subdir)
    else:
        print(f"Scene {scene_num} not found, skipping...")

print("\nExtraction complete!")
print(f"Files organized in:\n{output_dir}/")
print("├── images/  (All *_rgb_*.png and *_rgb_*.tif files)")
print("├── conf/    (All *_conf_*.tif files)")
print("└── cl/      (All *_cl_*.tif files)")'''

import os
from pathlib import Path
import shutil
from tqdm import tqdm

# Base directories
base_dir = Path("/content/marida_data")
patches_dir = base_dir / "patches"  # Note: Changed from 'patches' to 'patches'
output_dir = Path("/content/extracted_data_Marida")

# Create output folders
(output_dir / "images").mkdir(parents=True, exist_ok=True)
(output_dir / "conf").mkdir(parents=True, exist_ok=True)
(output_dir / "cl").mkdir(parents=True, exist_ok=True)

def classify_and_copy(file_path):
    """Strict classification and copying of files"""
    filename = file_path.name.lower()

    # Confidence maps (must contain '_conf')
    if '_conf' in filename:
        dest = output_dir / "conf"
        shutil.copy2(file_path, dest / file_path.name)
        return "conf"

    # CL maps (must contain '_cl')
    elif '_cl' in filename:
        dest = output_dir / "cl"
        shutil.copy2(file_path, dest / file_path.name)
        return "cl"

    # All other files go to images
    else:
        dest = output_dir / "images"
        shutil.copy2(file_path, dest / file_path.name)
        return "image"

# Process each patch subfolder
patch_subfolders = [d for d in patches_dir.iterdir() if d.is_dir()]
print(f"Found {len(patch_subfolders)} patch subfolders")

classification_counts = {"image": 0, "conf": 0, "cl": 0}

for patch_folder in tqdm(patch_subfolders, desc="Processing patches"):
    for file_path in patch_folder.glob("*"):
        if file_path.is_file():
            result = classify_and_copy(file_path)
            classification_counts[result] += 1

# Verification
print("\nFinal Classification:")
print(f"Images:      {classification_counts['image']} files")
print(f"Confidence:  {classification_counts['conf']} files")
print(f"CL maps:     {classification_counts['cl']} files")

# Show 3 samples from each folder
def show_samples(folder, count=3):
    files = list((output_dir / folder).glob("*"))
    print(f"\nSample {folder} files:")
    for f in files[:count]:
        print(f" - {f.name}")

show_samples("images")
show_samples("conf")
show_samples("cl")

"""Now we will explora how many images contain debris in marida and mados .

Mados
"""

'''import rasterio
import numpy as np
import os

# Define path to the directory containing the files
input_dir = "/content/extracted_data_Mados/cl"

# Define the class number for marine debris (assuming class 1 represents marine debris)
marine_debris_class = 1

# Initialize counter for images with marine debris
images_with_debris = 0

# Function to read and process a single file
def process_scene(file_path):
    global images_with_debris

    with rasterio.open(file_path) as src:
        image = src.read(1)  # Read the first band (assuming it's the class layer)

        # Count the number of marine debris pixels (class number 1)
        marine_debris_pixels = np.sum(image == marine_debris_class)

        # Only print and count if marine debris is detected
        if marine_debris_pixels > 0:
            images_with_debris += 1
           # print(f"{os.path.basename(file_path)}: {marine_debris_pixels} marine debris pixels")

# Process all files in the directory
for file_name in os.listdir(input_dir):
    if file_name.endswith('.tif'):
        file_path = os.path.join(input_dir, file_name)
        process_scene(file_path)

# Print total count of images with debris
print(f"\nTotal images with marine debris detected: {images_with_debris}")'''

"""Input: /content/MADOS

Scene_0/10

Scene_0/20

Scene_0/60

... up to Scene_173

Output:

/content/mados_resized_bilinear/ → All crops resized to 256×256 using bilinear interpolation

/content/mados_resized_padding/ → All crops resized to 256×256 using zero padding

/content/mados_stacked_bilinear/ → 11 bands stacked after bilinear resized

/content/mados_stacked_padding/ → 11 bands stacked after zero padded

Marida
"""

import rasterio
import numpy as np
import os

# Define path to the directory containing the files
input_dir = "/content/extracted_data_Marida/cl"

# Define the class number for marine debris (assuming class 1 represents marine debris)
marine_debris_class = 1

# Initialize counter for images with marine debris
images_with_debris = 0

# Function to read and process a single file
def process_scene(file_path):
    global images_with_debris

    with rasterio.open(file_path) as src:
        image = src.read(1)  # Read the first band (assuming it's the class layer)

        # Count the number of marine debris pixels (class number 1)
        marine_debris_pixels = np.sum(image == marine_debris_class)

        # Only print and count if marine debris is detected
        if marine_debris_pixels > 0:
            images_with_debris += 1
           # print(f"{os.path.basename(file_path)}: {marine_debris_pixels} marine debris pixels")

# Process all files in the directory
for file_name in os.listdir(input_dir):
    if file_name.endswith('.tif'):
        file_path = os.path.join(input_dir, file_name)
        process_scene(file_path)

# Print total count of images with debris
print(f"\nTotal images with marine debris detected: {images_with_debris}")

"""1-Extract_Marine_Debrise"""

import os
import shutil
import rasterio
import numpy as np
from glob import glob

# Input MADOS path
MADOS_PATH = '/content/MADOS'

# Output path for extracted marine debris scenes
OUTPUT_PATH = '/content/extracted_marine_debris'

# Create output folder
os.makedirs(OUTPUT_PATH, exist_ok=True)

# Define marine debris class
marine_debris_class = 1

# List all scenes
scene_paths = sorted(glob(os.path.join(MADOS_PATH, 'Scene_*')))

for scene_path in scene_paths:
    scene_id = os.path.basename(scene_path)
    found_marine_debris = False

    for resolution in ['10', '20', '60']:
        resolution_path = os.path.join(scene_path, resolution)
        if not os.path.exists(resolution_path):
            continue

        # Find classification (cl) files
        cl_files = glob(os.path.join(resolution_path, '*_cl_*.tif'))

        for cl_file in cl_files:
            with rasterio.open(cl_file) as src:
                cl_array = src.read(1)

            if marine_debris_class in np.unique(cl_array):
                found_marine_debris = True
                break  # Found in this scene, no need to check more

        if found_marine_debris:
            break  # Found in this scene, no need to check more

    # If marine debris found, copy all 10, 20, 60 folders
    if found_marine_debris:
        for resolution in ['10', '20', '60']:
            source_path = os.path.join(scene_path, resolution)
            if os.path.exists(source_path):
                dest_path = os.path.join(OUTPUT_PATH, scene_id, resolution)
                os.makedirs(dest_path, exist_ok=True)

                for file_name in os.listdir(source_path):
                    source_file = os.path.join(source_path, file_name)
                    dest_file = os.path.join(dest_path, file_name)
                    shutil.copy2(source_file, dest_file)

print("Extraction complete.")

"""2-Resize"""

import os
import rasterio
from rasterio.enums import Resampling
from glob import glob

# Paths
EXTRACTED_PATH = '/content/extracted_marine_debris'
RESIZED_PATH = '/content/step2_resize_mados_debris'
os.makedirs(RESIZED_PATH, exist_ok=True)

# Target size
TARGET_SIZE = (256, 256)

# Function to resize and save
def resize_and_save(input_path, output_path, resampling_method):
    with rasterio.open(input_path) as src:
        data = src.read(
            out_shape=(src.count, TARGET_SIZE[0], TARGET_SIZE[1]),
            resampling=resampling_method
        )
        profile = src.profile
        profile.update({
            'height': TARGET_SIZE[0],
            'width': TARGET_SIZE[1],
            'transform': rasterio.transform.from_origin(
                src.bounds.left,
                src.bounds.top,
                (src.bounds.right - src.bounds.left) / TARGET_SIZE[1],
                (src.bounds.bottom - src.bounds.top) / TARGET_SIZE[0]
            )
        })

        with rasterio.open(output_path, 'w', **profile) as dst:
            dst.write(data)

# Process each scene
scene_paths = sorted(glob(os.path.join(EXTRACTED_PATH, 'Scene_*')))

for scene_path in scene_paths:
    scene_id = os.path.basename(scene_path)

    for res in ['10', '20', '60']:
        res_input_path = os.path.join(scene_path, res)
        if not os.path.exists(res_input_path):
            continue

        res_output_path = os.path.join(RESIZED_PATH, scene_id, res)
        os.makedirs(res_output_path, exist_ok=True)

        # Process each file inside resolution folder
        tif_files = sorted(glob(os.path.join(res_input_path, '*.tif')))

        for tif_file in tif_files:
            filename = os.path.basename(tif_file)
            if 'cl' in filename or 'conf' in filename:
                resampling_method = Resampling.nearest
            else:
                resampling_method = Resampling.bilinear

            output_file_path = os.path.join(res_output_path, filename)

            resize_and_save(tif_file, output_file_path, resampling_method)

print("Finished resizing all files into step2_resize_mados_debris with original names.")

"""3-Stuck Bands"""

import os
import numpy as np
from PIL import Image
import rasterio
from rasterio.enums import Compression

def save_tif(image_array, save_path):
    # Ensure the image is in uint16 format and within the valid range
    image_array = np.clip(image_array, 0, 65535).astype(np.uint16)

    # If it's a multi-band image (3D shape)
    if len(image_array.shape) == 3:
        with rasterio.open(
            save_path, 'w', driver='GTiff',
            height=image_array.shape[1],
            width=image_array.shape[2],
            count=image_array.shape[0],  # number of bands
            dtype=image_array.dtype,
            compress=Compression.lzw
        ) as dst:
            for i in range(image_array.shape[0]):
                dst.write(image_array[i], i + 1)
    else:  # If it's a single-band image (2D shape)
        with rasterio.open(
            save_path, 'w', driver='GTiff',
            height=image_array.shape[0],
            width=image_array.shape[1],
            count=1,  # only one band
            dtype=image_array.dtype,
            compress=Compression.lzw
        ) as dst:
            dst.write(image_array, 1)  # Write the single band

# Paths
input_root = '/content/step2_resize_mados_debris'
output_root = '/content/step3_ready_mados_debris'

os.makedirs(output_root, exist_ok=True)

# Iterate scenes
for scene_name in sorted(os.listdir(input_root)):
    scene_path = os.path.join(input_root, scene_name)
    if not os.path.isdir(scene_path):
        continue

    print(f"Processing {scene_name}...")

    # Prepare paths
    res10_path = os.path.join(scene_path, '10')
    res20_path = os.path.join(scene_path, '20')
    res60_path = os.path.join(scene_path, '60')

    # Collect 10m rhorc bands
    rhorc_10_files = sorted([f for f in os.listdir(res10_path) if 'rhorc' in f and f.endswith('.tif')])[:4]
    cl_file = next((f for f in os.listdir(res10_path) if '_cl_' in f and f.endswith('.tif')), None)
    conf_file = next((f for f in os.listdir(res10_path) if '_conf_' in f and f.endswith('.tif')), None)

    # Collect 20m rhorc bands
    rhorc_20_files = sorted([f for f in os.listdir(res20_path) if 'rhorc' in f and f.endswith('.tif')])[:6]

    # Collect 60m rhorc band
    rhorc_60_files = sorted([f for f in os.listdir(res60_path) if 'rhorc' in f and f.endswith('.tif')])[:1]

    # Check if enough bands exist
    if len(rhorc_10_files) < 4 or len(rhorc_20_files) < 6 or len(rhorc_60_files) < 1 or cl_file is None:
        print(f"Scene {scene_name} is missing files, skipping...")
        continue

    # Load bands
    bands = []
    for f in rhorc_10_files:
        bands.append(np.array(Image.open(os.path.join(res10_path, f))))
    for f in rhorc_20_files:
        bands.append(np.array(Image.open(os.path.join(res20_path, f))))
    for f in rhorc_60_files:
        bands.append(np.array(Image.open(os.path.join(res60_path, f))))

    # Stack bands to shape (11, 256, 256)
    image = np.stack(bands, axis=0)

    # Check if image has 11 bands and shape (11, 256, 256)
    if image.shape == (11, 256, 256):
        print(f"Image from {scene_name} has the expected shape: {image.shape}")
    else:
        print(f"Warning: Image from {scene_name} does not have 11 bands, shape is {image.shape}")

    # Load mask and confidence, ensure uint16 type
    mask = np.array(Image.open(os.path.join(res10_path, cl_file)), dtype=np.uint16)
    conf = np.array(Image.open(os.path.join(res10_path, conf_file)), dtype=np.uint16) if conf_file else np.zeros_like(mask, dtype=np.uint16)

    # Extract scene number (e.g., "Scene_1")
    scene_number = scene_name.split('_')[-1]  # Assuming the scene name format is like "Scene_1"

    # Create output scene folder
    output_scene_path = os.path.join(output_root, scene_name)
    os.makedirs(output_scene_path, exist_ok=True)

    # Save stacked image as .tif
    stacked_image_path = os.path.join(output_scene_path, f'{scene_name}_{scene_number}_rhodc.tif')
    save_tif(image, stacked_image_path)

    # Save classification mask as .tif
    cl_mask_path = os.path.join(output_scene_path, f'{scene_name}_{scene_number}_rhodc_cl.tif')
    save_tif(mask, cl_mask_path)

    # Save confidence mask as .tif
    conf_mask_path = os.path.join(output_scene_path, f'{scene_name}_{scene_number}_rhodc_conf.tif')
    save_tif(conf, conf_mask_path)

print("Stacking and saving as .tif finished successfully.")

"""Step4 Extraction Mados"""

import os
from pathlib import Path
import shutil
from tqdm import tqdm

# Configuration
SOURCE_DIR = Path("/content/step3_ready_mados_debris")
DEST_DIR = Path("/content/extracted_data_Mados")
TOTAL_SCENES = 174  # Total possible scenes (only ~77 exist with debris)

# Create organized output folders
(DEST_DIR / "images").mkdir(parents=True, exist_ok=True)
(DEST_DIR / "cl").mkdir(parents=True, exist_ok=True)
(DEST_DIR / "conf").mkdir(parents=True, exist_ok=True)

def process_scene(scene_path):
    """Process all files in a single scene directory"""
    for file in scene_path.glob("*"):
        if not file.is_file():
            continue

        # Sort files into appropriate folders
        if "_rhodc.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "images" / file.name)
        elif "_rhodc_cl.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "cl" / file.name)
        elif "_rhodc_conf.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "conf" / file.name)

def main():
    processed_scenes = 0

    # Process available scenes with progress bar
    for scene_num in tqdm(range(1, TOTAL_SCENES + 1), desc="Processing Scenes"):
        scene_path = SOURCE_DIR / f"Scene_{scene_num}"
        if scene_path.exists():
            process_scene(scene_path)
            processed_scenes += 1

    # Print only essential summary
    print("\nExtraction Complete!")
    print(f"Processed {processed_scenes} scenes with debris")
    print(f"Images: {len(list((DEST_DIR / 'images').glob('*')))} files")
    print(f"CL: {len(list((DEST_DIR / 'cl').glob('*')))} files")
    print(f"Conf: {len(list((DEST_DIR / 'conf').glob('*')))} files")

if __name__ == "__main__":
    main()

"""This just to be sure of Mados Data"""

import os
from pathlib import Path
import shutil
from tqdm import tqdm

# Configuration
# Set your paths
base_dir = '/content/merged_marida_mados'
image_dir = os.path.join(base_dir, 'image')
mask_dir = os.path.join(base_dir, 'cl')
processed_dir = os.path.join(base_dir, 'processed')

# Define class mapping
class_mapping = {
    1: 'Marine Debris',
    2: 'Dense Sargassum',
    3: 'Sparse Sargassum',
    4: 'Natural Organic Material',
    5: 'Ship',
    6: 'Clouds',
    7: 'Marine Water',
    8: 'Sediment-Laden Water',
    9: 'Foam',
    10: 'Turbid Water',
    11: 'Shallow Water',
    12: 'Waves',
    13: 'Cloud Shadows',
    14: 'Wakes',
    15: 'Mixed Water'
}
def process_scene(scene_path):
    """Process all files in a single scene directory"""
    for file in scene_path.glob("*"):
        if not file.is_file():
            continue

        # Sort files into appropriate folders
        if "_rhodc.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "images" / file.name)
        elif "_rhodc_cl.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "cl" / file.name)
        elif "_rhodc_conf.tif" in file.name.lower():
            shutil.copy2(file, DEST_DIR / "conf" / file.name)

def main():
    processed_scenes = 0

    # Process available scenes with progress bar
    for scene_num in tqdm(range(1, TOTAL_SCENES + 1), desc="Processing Scenes"):
        scene_path = SOURCE_DIR / f"Scene_{scene_num}"
        if scene_path.exists():
            process_scene(scene_path)
            processed_scenes += 1

    # Print only essential summary
    print("\nExtraction Complete!")
    print(f"Processed {processed_scenes} scenes with debris")
    print(f"Images: {len(list((DEST_DIR / 'images').glob('*')))} files")
    print(f"CL: {len(list((DEST_DIR / 'cl').glob('*')))} files")
    print(f"Conf: {len(list((DEST_DIR / 'conf').glob('*')))} files")

if __name__ == "__main__":
    main()

"""Step5 Merge Marida and Mados"""

import os
import shutil
from tqdm import tqdm  # for progress bar

# Define source paths
marida_roots = {
    'cl': '/content/extracted_data_Marida/cl',
    'image': '/content/extracted_data_Marida/images',
    'conf': '/content/extracted_data_Marida/conf'
}

mados_roots = {
    'cl': '/content/extracted_data_Mados/cl',
    'image': '/content/extracted_data_Mados/images',
    'conf': '/content/extracted_data_Mados/conf'
}

# Define destination paths
merged_root = '/content/merged_marida_mados'
merged_folders = {
    'cl': os.path.join(merged_root, 'cl'),
    'image': os.path.join(merged_root, 'image'),
    'conf': os.path.join(merged_root, 'conf')
}

# Create destination folders
for folder in merged_folders.values():
    os.makedirs(folder, exist_ok=True)

def merge_datasets(source_roots, merged_root):
    """Merge files from source to destination with conflict checking"""
    for file_type in ['cl', 'image', 'conf']:
        src_folder = source_roots[file_type]
        dst_folder = merged_folders[file_type]

        print(f"\nProcessing {file_type} files from {src_folder}")

        for file_name in tqdm(os.listdir(src_folder)):
            src_path = os.path.join(src_folder, file_name)
            if not os.path.isfile(src_path):
                continue

            dst_path = os.path.join(dst_folder, file_name)

            # Handle filename conflicts by adding prefix
            if os.path.exists(dst_path):
                prefix = "MARIDA_" if "Marida" in src_folder else "MADOS_"
                new_name = prefix + file_name
                dst_path = os.path.join(dst_folder, new_name)

            shutil.copy2(src_path, dst_path)

# Merge both datasets
print("Starting MARIDA merge...")
merge_datasets(marida_roots, merged_folders)

print("\nStarting MADOS merge...")
merge_datasets(mados_roots, merged_folders)

print("\nMerge completed successfully!")
print(f"Total files in merged directory: {sum(len(files) for _, _, files in os.walk(merged_root))}")

"""Model Unet"""

import rasterio

# Path to your satellite image file (replace with your actual file path)
image_path = '/content/merged_marida_mados/image/Scene_90_90_rhodc.tif'

# Open the image file
with rasterio.open(image_path) as src:
    # Get the number of bands
    num_bands = src.count
    print(f'Number of bands in the image: {num_bands}')

    # Optionally: Print the band names or info
    for band in range(1, num_bands + 1):
        print(f'Band {band}: {src.descriptions[band - 1] if len(src.descriptions) >= band else "No description"}')

"""Model Net"""

# #18 Data Preprocessing for Attention U-Net Model Training

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import torchvision.transforms as transforms
import torch.nn.functional as F
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
import rasterio
from tqdm import tqdm
import pickle

# Set your base directories
base_dir = '/content/merged_marida_mados'
image_dir = os.path.join(base_dir, 'image')
mask_dir = os.path.join(base_dir, 'cl')
processed_dir = os.path.join(base_dir, 'processed')

# Define class mapping
class_mapping = {
    1: 'Marine Debris',
    2: 'Dense Sargassum',
    3: 'Sparse Sargassum',
    4: 'Natural Organic Material',
    5: 'Ship',
    6: 'Clouds',
    7: 'Marine Water',
    8: 'Sediment-Laden Water',
    9: 'Foam',
    10: 'Turbid Water',
    11: 'Shallow Water',
    12: 'Waves',
    13: 'Cloud Shadows',
    14: 'Wakes',
    15: 'Mixed Water'
}

# Define Albumentations transforms
train_transform = A.Compose([
    A.RandomCrop(256, 256),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.Normalize(),
    ToTensorV2()
])

val_transform = A.Compose([
    A.CenterCrop(256, 256),
    A.Normalize(),
    ToTensorV2()
])

def preprocess_data_for_training():
    """Preprocess the MARIDA dataset"""
    preprocessed_dir = os.path.join(processed_dir, "preprocessed")
    os.makedirs(preprocessed_dir, exist_ok=True)

    all_image_paths = []
    for f in os.listdir(image_dir):
        if f.endswith('.tif') and not f.endswith('_cl.tif'):
            mask_path = os.path.join(mask_dir, f.replace('.tif', '_cl.tif'))
            if os.path.exists(mask_path):
                all_image_paths.append(f)
            else:
                print(f"Warning: Missing mask for {f}")

    train_val_paths, test_paths = train_test_split(all_image_paths, test_size=0.2, random_state=42)
    train_paths, val_paths = train_test_split(train_val_paths, test_size=0.2, random_state=42)

    splits_dir = os.path.join(processed_dir, "splits")
    os.makedirs(splits_dir, exist_ok=True)

    for split_name, paths in zip(['train', 'val', 'test'], [train_paths, val_paths, test_paths]):
        with open(os.path.join(splits_dir, f"{split_name}_custom.txt"), 'w') as f:
            f.write('\n'.join(paths))

    print(f"Dataset splits: {len(train_paths)} training, {len(val_paths)} validation, {len(test_paths)} testing")

    for split_name, paths in zip(['train', 'val', 'test'], [train_paths, val_paths, test_paths]):
        analyze_split_class_distribution(paths, split_name, mask_dir, preprocessed_dir)

    split_paths = {
        'train': train_paths,
        'val': val_paths,
        'test': test_paths
    }

    with open(os.path.join(preprocessed_dir, "split_paths.pkl"), 'wb') as f:
        pickle.dump(split_paths, f)

    print("Preprocessing done.")
    return split_paths

def analyze_split_class_distribution(paths, split_name, mask_dir, output_dir):
    """Analyze the class distribution"""
    class_counts = {i: 0 for i in range(1, 16)}
    total_pixels = 0
    has_debris_count = 0

    for fname in tqdm(paths, desc=f"Analyzing {split_name} split"):
        mask_path = os.path.join(mask_dir, fname.replace('.tif', '_cl.tif'))

        if os.path.exists(mask_path):
            with rasterio.open(mask_path) as src:
                mask_data = src.read(1)

                for class_id in range(1, 16):
                    count = np.sum(mask_data == class_id)
                    class_counts[class_id] += count
                    if class_id == 1 and count > 0:
                        has_debris_count += 1

                total_pixels += mask_data.size

    class_percentages = {cid: (count / total_pixels * 100) for cid, count in class_counts.items()}

    with open(os.path.join(output_dir, f"{split_name}_class_distribution.txt"), 'w') as f:
        f.write(f"Total images: {len(paths)}\n")
        f.write(f"Images with debris: {has_debris_count} ({has_debris_count / len(paths) * 100:.2f}%)\n\n")
        for cid, percentage in sorted(class_percentages.items(), key=lambda x: x[1], reverse=True):
            f.write(f"{cid}: {class_mapping[cid]} - {percentage:.2f}%\n")

    class_weights = {cid: (total_pixels / (count + 1)) for cid, count in class_counts.items()}
    sum_weights = sum(class_weights.values())
    normalized_weights = {cid: weight / sum_weights for cid, weight in class_weights.items()}

    np.save(os.path.join(output_dir, f"{split_name}_class_weights.npy"),
            np.array([normalized_weights[i] for i in range(1, 16)]))

    return class_percentages

class MARIDADataset(Dataset):
    """Dataset for MARIDA data"""
    def __init__(self, paths, image_dir, mask_dir, transform=None, use_multispectral=True):
        self.paths = paths
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        self.use_multispectral = use_multispectral
        self.rgb_bands = [0, 1, 2]

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        fname = self.paths[idx]
        img_path = os.path.join(self.image_dir, fname)
        mask_path = os.path.join(self.mask_dir, fname.replace('.tif', '_cl.tif'))

        try:
            with rasterio.open(img_path) as src:
                image = src.read()

            with rasterio.open(mask_path) as src:
                mask = src.read(1)

            if not self.use_multispectral:
                image = image[self.rgb_bands]

            image = image.astype(np.float32)

            if self.use_multispectral:
                nir, red = 3, 2
                ndvi = (image[nir] - image[red]) / (image[nir] + image[red] + 1e-7)
                image = np.vstack([image, ndvi[np.newaxis]])

            for i in range(image.shape[0]):
                min_val, max_val = np.nanmin(image[i]), np.nanmax(image[i])
                if max_val > min_val:
                    image[i] = (image[i] - min_val) / (max_val - min_val)
                else:
                    image[i] = np.zeros_like(image[i])

            image = np.nan_to_num(image)

            binary_mask = (mask == 1).astype(np.float32)

            if self.transform:
                transformed = self.transform(image=np.transpose(image, (1, 2, 0)), mask=binary_mask)
                image = transformed['image']
                binary_mask = transformed['mask']
                image = image if torch.is_tensor(image) else torch.from_numpy(image.permute(2, 0, 1))
                binary_mask = binary_mask if torch.is_tensor(binary_mask) else torch.from_numpy(binary_mask).unsqueeze(0)
            else:
                image = torch.from_numpy(image)
                binary_mask = torch.from_numpy(binary_mask).unsqueeze(0)

            return image, binary_mask

        except Exception as e:
            print(f"Skipping sample {fname} due to error: {e}")
            return self.__getitem__((idx + 1) % len(self))

def prepare_data_loaders(split_paths, image_dir, mask_dir, batch_size=8, num_workers=4, use_multispectral=True):
    """Prepare DataLoaders"""
    sample_weights = []
    for fname in split_paths['train']:
        mask_path = os.path.join(mask_dir, fname.replace('.tif', '_cl.tif'))
        with rasterio.open(mask_path) as src:
            mask = src.read(1)
            sample_weights.append(3.0 if (mask == 1).any() else 1.0)

    sampler = WeightedRandomSampler(sample_weights, num_samples=len(split_paths['train']), replacement=True)

    train_dataset = MARIDADataset(split_paths['train'], image_dir, mask_dir, transform=train_transform, use_multispectral=use_multispectral)
    val_dataset = MARIDADataset(split_paths['val'], image_dir, mask_dir, transform=val_transform, use_multispectral=use_multispectral)
    test_dataset = MARIDADataset(split_paths['test'], image_dir, mask_dir, transform=val_transform, use_multispectral=use_multispectral)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)

    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Testing samples: {len(test_dataset)}")

    sample_img, _ = train_dataset[0]
    print(f"Sample input shape: {sample_img.shape}")
    print(f"Using {'Multispectral' if use_multispectral else 'RGB'} bands")

    return train_loader, val_loader, test_loader

# Preprocess and create DataLoaders
split_paths = preprocess_data_for_training()

train_loader, val_loader, test_loader = prepare_data_loaders(
    split_paths=split_paths,
    image_dir=image_dir,
    mask_dir=mask_dir,
    batch_size=8,
    num_workers=4,
    use_multispectral=True
)

#19
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.functional as F
import time
import copy
from tqdm import tqdm

# Attention Gate
class AttentionGate(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super(AttentionGate, self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.W_x = nn.Sequential(
            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(F_int)
        )

        self.psi = nn.Sequential(
            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )

        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x):
            g1 = self.W_g(g)
            x1 = self.W_x(x)
            psi = self.relu(g1 + x1)
            psi = self.psi(psi)
            return x * psi

# Band Attention Module
class BandAttention(nn.Module):
    def __init__(self, in_channels):
        super(BandAttention, self).__init__()
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size=1),
            nn.BatchNorm2d(in_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        weights = self.attention(x)
        return x * weights


# AttentionUNet with Band Attention
class AttentionUNet(nn.Module):
    def __init__(self, in_channels=10, out_channels=1):
        super(AttentionUNet, self).__init__()
        self.band_attention = BandAttention(in_channels)
        self.enc1 = self.conv_block(in_channels, 64)
        self.enc2 = self.conv_block(64, 128)
        self.enc3 = self.conv_block(128, 256)
        self.enc4 = self.conv_block(256, 512)
        self.bottleneck = self.conv_block(512, 1024)
        self.attention1 = AttentionGate(F_g=1024, F_l=512, F_int=256)
        self.attention2 = AttentionGate(F_g=512, F_l=256, F_int=128)
        self.attention3 = AttentionGate(F_g=256, F_l=128, F_int=64)
        self.attention4 = AttentionGate(F_g=128, F_l=64, F_int=32)
        self.dec1 = self.conv_block(1024 + 512, 512)
        self.dec2 = self.conv_block(512 + 256, 256)
        self.dec3 = self.conv_block(256 + 128, 128)
        self.dec4 = self.conv_block(128 + 64, 64)
        self.output = nn.Conv2d(64, out_channels, kernel_size=1)
        self.pool = nn.MaxPool2d(2)
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

    def conv_block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        x = self.band_attention(x)  # Apply band attention to imporve the model performence
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        bottleneck = self.bottleneck(self.pool(enc4))
        dec4 = self.upsample(bottleneck)
        att4 = self.attention1(dec4, enc4)
        dec4 = torch.cat((dec4, att4), dim=1)
        dec4 = self.dec1(dec4)
        dec3 = self.upsample(dec4)
        att3 = self.attention2(dec3, enc3)
        dec3 = torch.cat((dec3, att3), dim=1)
        dec3 = self.dec2(dec3)
        dec2 = self.upsample(dec3)
        att2 = self.attention3(dec2, enc2)
        dec2 = torch.cat((dec2, att2), dim=1)
        dec2 = self.dec3(dec2)
        dec1 = self.upsample(dec2)
        att1 = self.attention4(dec1, enc1)
        dec1 = torch.cat((dec1, att1), dim=1)
        dec1 = self.dec4(dec1)
        output = self.output(dec1)
        return torch.sigmoid(output)

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, predictions, targets):
        if predictions.dim() == 4 and targets.dim() == 3:
            targets = targets.unsqueeze(1)

        # Flatten predictions and targets
        predictions = predictions.view(-1)
        targets = targets.view(-1)

        # Clip predictions to avoid numerical instability
        predictions = torch.clamp(predictions, min=1e-7, max=1.0-1e-7)

        # Calculate Dice score
        intersection = (predictions * targets).sum()
        dice = (2.0 * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)

        return 1 - dice

# Update Focal Loss with tuned parameters
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.9, gamma=3.5):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.eps = 1e-7

    def forward(self, predictions, targets):
        if predictions.dim() == 4 and targets.dim() == 3:
            targets = targets.unsqueeze(1)

        predictions = torch.clamp(predictions, min=self.eps, max=1.0 - self.eps)
        bce_loss = -targets * torch.log(predictions) - (1 - targets) * torch.log(1 - predictions)
        pt = torch.exp(-bce_loss)
        focal_weight = (1 - pt) ** self.gamma
        alpha_weight = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        focal_weight = alpha_weight * focal_weight
        return (focal_weight * bce_loss).mean()


# Update Combined Loss with higher focal weight
class CombinedLoss(nn.Module):
    def __init__(self, dice_weight=0.5, focal_weight=0.7):  # Higher focal weight
        super(CombinedLoss, self).__init__()
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.dice_loss = DiceLoss()
        self.focal_loss = FocalLoss()

    def forward(self, predictions, targets):
        return self.dice_weight * self.dice_loss(predictions, targets) + \
               self.focal_weight * self.focal_loss(predictions, targets)

# Model training function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                num_epochs=50, device='cuda', save_dir='models'):
    # Create directory for saving model
    os.makedirs(save_dir, exist_ok=True)

    # Initialize best model and metrics
    best_model_wts = copy.deepcopy(model.state_dict())
    best_loss = float('inf')
    best_dice = 0.0

    # Initialize history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_dice': []
    }

    # Training loop
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 20)

        # Training phase
        model.train()
        running_loss = 0.0
        batch_count = 0

        train_bar = tqdm(train_loader, desc='Training')
        for inputs, targets in train_bar:
            try:
                inputs = inputs.to(device)
                targets = targets.to(device)

                # Check for NaN values
                if torch.isnan(inputs).any() or torch.isnan(targets).any():
                    print("Warning: NaN values detected in input or target, skipping batch")
                    continue

                # Zero the parameter gradients
                optimizer.zero_grad()

                # Forward pass
                with torch.set_grad_enabled(True):
                    outputs = model(inputs)

                    # Check for NaN outputs
                    if torch.isnan(outputs).any():
                        print("Warning: NaN values in model output, skipping batch")
                        continue

                    loss = criterion(outputs, targets)

                    # Backward pass and optimize
                    loss.backward()

                    # Gradient clipping to prevent explosion
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                    optimizer.step()

                # Update statistics
                running_loss += loss.item() * inputs.size(0)
                batch_count += 1
                train_bar.set_postfix({'loss': loss.item()})

            except Exception as e:
                print(f"Error in training batch: {e}")
                import traceback
                traceback.print_exc()
                continue

        if batch_count > 0:
            epoch_train_loss = running_loss / (batch_count * train_loader.batch_size)
            history['train_loss'].append(epoch_train_loss)
        else:
            print("Warning: No valid batches in this epoch")
            epoch_train_loss = float('inf')
            history['train_loss'].append(epoch_train_loss)

        # Validation phase
        model.eval()
        running_loss = 0.0
        running_dice = 0.0
        val_batch_count = 0

        val_bar = tqdm(val_loader, desc='Validation')
        for inputs, targets in val_bar:
            try:
                inputs = inputs.to(device)
                targets = targets.to(device)

                # Check for NaN values
                if torch.isnan(inputs).any() or torch.isnan(targets).any():
                    print("Warning: NaN values detected in input or target, skipping validation batch")
                    continue

                # Forward pass
                with torch.no_grad():
                    outputs = model(inputs)

                    # Check for NaN outputs
                    if torch.isnan(outputs).any():
                        print("Warning: NaN values in model output, skipping validation batch")
                        continue

                    loss = criterion(outputs, targets)

                    # Ensure targets have same dimension as predictions
                    if outputs.dim() == 4 and targets.dim() == 3:
                        targets = targets.unsqueeze(1)

                    # Calculate Dice score (for monitoring)
                    preds = (outputs > 0.5).float()
                    intersection = (preds * targets).sum()
                    dice = (2.0 * intersection + 1e-5) / (preds.sum() + targets.sum() + 1e-5)

                # Update statistics
                running_loss += loss.item() * inputs.size(0)
                running_dice += dice.item() * inputs.size(0)
                val_batch_count += 1
                val_bar.set_postfix({'loss': loss.item(), 'dice': dice.item()})

            except Exception as e:
                print(f"Error in validation batch: {e}")
                continue

        if val_batch_count > 0:
            epoch_val_loss = running_loss / (val_batch_count * val_loader.batch_size)
            epoch_val_dice = running_dice / (val_batch_count * val_loader.batch_size)
            history['val_loss'].append(epoch_val_loss)
            history['val_dice'].append(epoch_val_dice)
        else:
            print("Warning: No valid batches in validation")
            epoch_val_loss = float('inf')
            epoch_val_dice = 0.0
            history['val_loss'].append(epoch_val_loss)
            history['val_dice'].append(epoch_val_dice)

        # Update learning rate
        scheduler.step(epoch_val_loss)

        # Check if this is the best model
        if epoch_val_dice > best_dice:
            best_dice = epoch_val_dice
            best_loss = epoch_val_loss
            best_model_wts = copy.deepcopy(model.state_dict())

            # Save best model
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss,
                'dice': best_dice,
                'history': history
            }, os.path.join(save_dir, 'best_model.pth'))

        # Print epoch results
        print(f'Train Loss: {epoch_train_loss:.4f}')
        print(f'Validation Loss: {epoch_val_loss:.4f}, Dice: {epoch_val_dice:.4f}')
        print(f'Best Validation Dice: {best_dice:.4f}')

        # Save checkpoint every 5 epochs
        if (epoch + 1) % 5 == 0:
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': epoch_val_loss,
                'dice': epoch_val_dice,
                'history': history
            }, os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pth'))

    # Calculate total training time
    time_elapsed = time.time() - start_time
    print(f'\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'Best validation Dice: {best_dice:.4f}')

    # Load best model
    model.load_state_dict(best_model_wts)

    return model, history

# Model evaluation function
def evaluate_model(model, test_loader, device='cuda'):
    """Evaluate model on test set"""
    model.eval()

    running_dice = 0.0
    running_iou = 0.0
    running_precision = 0.0
    running_recall = 0.0
    running_f1 = 0.0

    test_bar = tqdm(test_loader, desc='Evaluation')

    with torch.no_grad():
        for inputs, targets in test_bar:
            inputs = inputs.to(device)
            targets = targets.to(device)

            # Forward pass
            outputs = model(inputs)
            preds = (outputs > 0.5).float()

            # Calculate metrics
            intersection = (preds * targets).sum().item()
            union = preds.sum().item() + targets.sum().item() - intersection

            # Avoid division by zero
            if union > 0:
                iou = intersection / union
            else:
                iou = 1.0

            if preds.sum().item() > 0:
                precision = intersection / preds.sum().item()
            else:
                precision = 0.0

            if targets.sum().item() > 0:
                recall = intersection / targets.sum().item()
            else:
                recall = 1.0

            # F1 score
            if precision + recall > 0:
                f1 = 2 * precision * recall / (precision + recall)
            else:
                f1 = 0.0

            # Dice score (it's same as F1 for binary segmentation)
            dice = 2 * intersection / (preds.sum().item() + targets.sum().item() + 1e-5)

            # Update statistics
            running_dice += dice * inputs.size(0)
            running_iou += iou * inputs.size(0)
            running_precision += precision * inputs.size(0)
            running_recall += recall * inputs.size(0)
            running_f1 += f1 * inputs.size(0)

            # Update progress bar
            test_bar.set_postfix({
                'Dice': dice,
                'IoU': iou,
                'Precision': precision,
                'Recall': recall
            })

    # Calculate average metrics
    avg_dice = running_dice / len(test_loader.dataset)
    avg_iou = running_iou / len(test_loader.dataset)
    avg_precision = running_precision / len(test_loader.dataset)
    avg_recall = running_recall / len(test_loader.dataset)
    avg_f1 = running_f1 / len(test_loader.dataset)

    # Print evaluation results
    print("\nTest metrics:")
    print(f"Dice score: {avg_dice:.4f}")
    print(f"IoU: {avg_iou:.4f}")
    print(f"Precision: {avg_precision:.4f}")
    print(f"Recall: {avg_recall:.4f}")
    print(f"F1 score: {avg_f1:.4f}")

    # Save metrics to file
    metrics = {
        'dice': avg_dice,
        'iou': avg_iou,
        'precision': avg_precision,
        'recall': avg_recall,
        'f1': avg_f1
    }

    return metrics

# Prediction function
def predict(model, image, device='cuda'):
    model.eval()
    with torch.no_grad():
        image = image.to(device)
        output = model(image.unsqueeze(0))
        pred = (output > 0.5).float()
    return pred.squeeze(0).cpu().numpy()



#20
if __name__ == "__main__":
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Define hyperparameters
    BATCH_SIZE = 8
    NUM_EPOCHS = 50
    LEARNING_RATE = 1e-4
    USE_MULTISPECTRAL = True  # Set to True for multispectral approach

    # Set model output directory
    model_dir = os.path.join(processed_dir, "models")
    os.makedirs(model_dir, exist_ok=True)

    try:
        # First, prepare the data and create datasets
        split_paths = preprocess_data_for_training()
        train_loader, val_loader, test_loader = prepare_data_loaders(
            split_paths,
            os.path.join(base_dir, "image"),
            mask_dir=mask_dir,
            batch_size=BATCH_SIZE,
            use_multispectral=USE_MULTISPECTRAL
        )




        # Now we can access the dataset through the data loader
        # Get a sample from the training dataset to determine input channels
        sample_img = None
        for sample_batch in train_loader:
            sample_img, _ = sample_batch
            in_channels = sample_img.shape[1]  # Shape is [batch_size, channels, height, width]
            print(f"Using {in_channels} input channels for the model")
            break

        if sample_img is None:
            raise ValueError("No valid samples found in the training dataset")

        # Initialize model with the correct number of input channels
        model = AttentionUNet(in_channels=in_channels, out_channels=1)

        # Initialize loss function
        criterion = CombinedLoss(dice_weight=0.7, focal_weight=0.3)

        # Initialize optimizer
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

        # Initialize learning rate scheduler
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

        # Move model to device safely
        model = model.to(device)
        print("Model successfully moved to device")



        # Train model
        print(f"\nStarting model training for {NUM_EPOCHS} epochs...")
        model, history = train_model(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            optimizer=optimizer,
            scheduler=scheduler,
            num_epochs=NUM_EPOCHS,
            device=device,
            save_dir=model_dir
        )

        # Evaluate model on test set
        print("\nEvaluating model on test set...")
        metrics = evaluate_model(model, test_loader, device=device)

        # Save metrics
        with open(os.path.join(model_dir, "test_metrics.json"), 'w') as f:
            import json
            json.dump(metrics, f, indent=4)

        print("\nTraining and evaluation completed!")

    except Exception as e:
        print(f"Error occurred: {e}")
        import traceback
        traceback.print_exc()



#21
# Save the trained model
torch.save(model.state_dict(), 'attention_unet_plastic_debris_pytorch.pth')
print("PyTorch model trained and saved.")



#22
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score
import torch

def evaluate_model(model, test_loader, device='cuda'):
    """Evaluate model on test set and generate confusion matrix"""
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc="Evaluating"):
            inputs = inputs.to(device)
            targets = targets.to(device)

            # Forward pass
            outputs = model(inputs)
            preds = (outputs > 0.5).float()  # Threshold predictions at 0.5

            # Flatten tensors for confusion matrix
            y_true.extend(targets.cpu().numpy().flatten())
            y_pred.extend(preds.cpu().numpy().flatten())

    # Convert lists to numpy arrays
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Generate confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)

    # Print accuracy
    print(f"Accuracy: {accuracy * 100:.2f}%")

    # Plot confusion matrix
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=['Non-Debris', 'Debris'],
                yticklabels=['Non-Debris', 'Debris'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    return cm, accuracy


def visualize_predictions(model, test_loader, device='cuda', num_samples=5):
    """Visualize model predictions on test data"""
    model.eval()
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 5))

    with torch.no_grad():
        for i, (inputs, targets) in enumerate(test_loader):
            if i >= num_samples:
                break

            inputs = inputs.to(device)
            targets = targets.to(device)

            # Forward pass
            outputs = model(inputs)
            preds = (outputs > 0.5).float()  # Threshold predictions at 0.5

            # Move tensors to CPU for visualization
            inputs = inputs.cpu().numpy()
            targets = targets.cpu().numpy()
            preds = preds.cpu().numpy()

            # Plot input image, ground truth, and prediction
            axes[i, 0].imshow(inputs[0].transpose(1, 2, 0)[:, :, :3])  # Show RGB bands
            axes[i, 0].set_title('Input Image')
            axes[i, 0].axis('off')

            axes[i, 1].imshow(targets[0].squeeze(), cmap='gray')
            axes[i, 1].set_title('Ground Truth')
            axes[i, 1].axis('off')

            axes[i, 2].imshow(preds[0].squeeze(), cmap='gray')
            axes[i, 2].set_title('Prediction')
            axes[i, 2].axis('off')

    plt.tight_layout()
    plt.show()


# Evaluate the model
cm, accuracy = evaluate_model(model, test_loader, device=device)

# Visualize predictions
visualize_predictions(model, test_loader, device=device, num_samples=5)