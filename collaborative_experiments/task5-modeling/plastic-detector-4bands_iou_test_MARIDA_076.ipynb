{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:00.642060Z",
     "iopub.status.busy": "2025-05-25T19:00:00.641899Z",
     "iopub.status.idle": "2025-05-25T19:00:00.646326Z",
     "shell.execute_reply": "2025-05-25T19:00:00.645782Z",
     "shell.execute_reply.started": "2025-05-25T19:00:00.642045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn')  # Set before any other imports or operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:00.648346Z",
     "iopub.status.busy": "2025-05-25T19:00:00.648030Z",
     "iopub.status.idle": "2025-05-25T19:00:00.670078Z",
     "shell.execute_reply": "2025-05-25T19:00:00.669372Z",
     "shell.execute_reply.started": "2025-05-25T19:00:00.648325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#bands 4.6.8.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:00.671078Z",
     "iopub.status.busy": "2025-05-25T19:00:00.670894Z",
     "iopub.status.idle": "2025-05-25T19:00:00.803763Z",
     "shell.execute_reply": "2025-05-25T19:00:00.802922Z",
     "shell.execute_reply.started": "2025-05-25T19:00:00.671063Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.pth\n",
      "global_stats.npz\n",
      "litter_rows_df_invalid_info.csv\n",
      "litter_rows_val_df_invalid_info.csv\n",
      "marida_df_invalid_info.csv\n",
      "marida_test_df_invalid_info.csv\n",
      "marida_val_df_invalid_info.csv\n",
      "model_30_epochs_ratio_1_20_bs16_iou_081.pth\n",
      "model_30_epochs_ratio_1_40_bs16_iou_0819.pth\n"
     ]
    }
   ],
   "source": [
    "! ls /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:00.804907Z",
     "iopub.status.busy": "2025-05-25T19:00:00.804688Z",
     "iopub.status.idle": "2025-05-25T19:00:05.980900Z",
     "shell.execute_reply": "2025-05-25T19:00:05.980216Z",
     "shell.execute_reply.started": "2025-05-25T19:00:00.804884Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rasterio\n",
      "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
      "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.26.4)\n",
      "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.0.9)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24->rasterio) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->rasterio) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24->rasterio) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->rasterio) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24->rasterio) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24->rasterio) (2024.2.0)\n",
      "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: affine, rasterio\n",
      "Successfully installed affine-2.4.0 rasterio-1.4.3\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "! pip install rasterio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:05.982062Z",
     "iopub.status.busy": "2025-05-25T19:00:05.981810Z",
     "iopub.status.idle": "2025-05-25T19:00:16.928767Z",
     "shell.execute_reply": "2025-05-25T19:00:16.928000Z",
     "shell.execute_reply.started": "2025-05-25T19:00:05.982025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import shutil\n",
    "import re\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array as da\n",
    "from scipy.ndimage import binary_dilation\n",
    "from skimage.morphology import disk  # For circular structuring elements\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as vF\n",
    "import torch.nn.functional as F\n",
    "import gdown\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, jaccard_score, hamming_loss, label_ranking_loss, coverage_error, classification_report\n",
    "import sklearn.metrics as metr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:16.930129Z",
     "iopub.status.busy": "2025-05-25T19:00:16.929606Z",
     "iopub.status.idle": "2025-05-25T19:00:16.934213Z",
     "shell.execute_reply": "2025-05-25T19:00:16.933640Z",
     "shell.execute_reply.started": "2025-05-25T19:00:16.930103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)  # Show full column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:16.936780Z",
     "iopub.status.busy": "2025-05-25T19:00:16.936236Z",
     "iopub.status.idle": "2025-05-25T19:00:16.952973Z",
     "shell.execute_reply": "2025-05-25T19:00:16.952440Z",
     "shell.execute_reply.started": "2025-05-25T19:00:16.936762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# seeding\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set random seeds for NumPy, PyTorch (CPU and GPU), and Python's random module.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): Seed value for RNGs\n",
    "    \"\"\"\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch CPU\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # PyTorch GPU (CUDA)\n",
    "    torch.cuda.manual_seed(seed)  # Current GPU\n",
    "    torch.cuda.manual_seed_all(seed)  # All GPUs\n",
    "    \n",
    "    # Ensure deterministic behavior\n",
    "    #torch.use_deterministic_algorithms(True)\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Initialize random seed for DataLoader workers.\n",
    "    Ensures each worker has a unique but reproducible RNG state.\n",
    "    \n",
    "    Args:\n",
    "        worker_id (int): Worker ID\n",
    "    \"\"\"\n",
    "    max_seed = 2**32 - 1  # NumPy seed limit\n",
    "    worker_seed = (torch.initial_seed() + worker_id) % max_seed\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:16.954035Z",
     "iopub.status.busy": "2025-05-25T19:00:16.953784Z",
     "iopub.status.idle": "2025-05-25T19:00:16.972573Z",
     "shell.execute_reply": "2025-05-25T19:00:16.972042Z",
     "shell.execute_reply.started": "2025-05-25T19:00:16.954011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_LR_dataframe(splits_path, mode='train'):\n",
    "    split_images_files = {'train' : 'train_X.txt', 'val' : 'val_X.txt', 'test' : 'test_X.txt'}\n",
    "    split_masks_files = {'train' : 'train_masks.txt', 'val' : 'val_masks.txt', 'test' : 'test_masks.txt'}  \n",
    "    with open(os.path.join(splits_path, split_images_files[mode]), \"r\") as file:\n",
    "        images = file.readlines()  # Reads all lines into a list\n",
    "        images = [image.strip() for image in images]  # Remove any trailing newline characters\n",
    "    with open(os.path.join(splits_path, split_masks_files[mode]), \"r\") as file:\n",
    "        masks = file.readlines()  # Reads all lines into a list\n",
    "        masks = [mask.strip() for mask in masks]  # Remove any trailing newline characters\n",
    "    df = pd.DataFrame({'image' : images, 'mask' : masks})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:16.973437Z",
     "iopub.status.busy": "2025-05-25T19:00:16.973235Z",
     "iopub.status.idle": "2025-05-25T19:00:16.987564Z",
     "shell.execute_reply": "2025-05-25T19:00:16.986712Z",
     "shell.execute_reply.started": "2025-05-25T19:00:16.973423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from Sagar and Navodita's code\n",
    "def compute_fdi_from_tiff(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        # Assuming band order follows your stacked TIFF (B1–B12, skipping B10 if needed)\n",
    "        # Band indices are 1-based in rasterio\n",
    "        R665 = src.read(4)    # B4\n",
    "        R859 = src.read(9)    # B8A\n",
    "        R1610 = src.read(10)  # B11\n",
    "        # Convert to float and mask invalid values\n",
    "        R665 = R665.astype(np.float32)\n",
    "        R859 = R859.astype(np.float32)\n",
    "        R1610 = R1610.astype(np.float32)\n",
    "        # Calculate FDI\n",
    "        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n",
    "        return FDI\n",
    "\n",
    "def cvt_to_fdi(images):\n",
    "    fdi_images = []\n",
    "    batch = images.copy()\n",
    "    if len(images.shape) == 3 : \n",
    "        batch = batch[None, :]\n",
    "    for i in range(batch.shape[0]):\n",
    "        im = batch[i]\n",
    "        R665 = im[3]   # B4\n",
    "        R859 = im[8]   # B8A\n",
    "        R1610 = im[0]  # B11\n",
    "        # Convert to float and mask invalid values\n",
    "        R665 = R665.astype(np.float32)\n",
    "        R859 = R859.astype(np.float32)\n",
    "        R1610 = R1610.astype(np.float32)\n",
    "        # Calculate FDI\n",
    "        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n",
    "        fdi_images.append(FDI)\n",
    "    return np.array(fdi_images)\n",
    "    \n",
    "def compute_ndwi(tiff_path):\n",
    "    with rasterio.open(tiff_path) as src:\n",
    "        Rgreen = src.read(3).astype(np.float32)  # Band 3 (Green)\n",
    "        Rnir = src.read(8).astype(np.float32)    # Band 8 (NIR)\n",
    "        ndwi = (Rgreen - Rnir) / (Rgreen + Rnir + 1e-6)  # avoid divide-by-zero\n",
    "    return ndwi\n",
    "def plot_fdi(fdi_array, ndwi, img_path, mask_path):\n",
    "    with rasterio.open(img_path) as src:\n",
    "        rgb = src.read([4, 3, 2])\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    # Normalization\n",
    "    rgb = rgb.astype(np.float32)\n",
    "    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        mask = src.read(1)\n",
    "    # Create binary mask\n",
    "    mask_binary = mask > 0\n",
    "    # Plot side-by-side\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    axs[0].imshow(rgb)\n",
    "    axs[0].set_title(\"RGB Patch\")\n",
    "    axs[1].imshow(mask_binary)  #, cmap='gray')\n",
    "    axs[1].set_title(\"Binary Mask (._cl.tif)\")\n",
    "    axs[2].imshow(fdi_array)\n",
    "    axs[2].set_title(\"FDI\")\n",
    "    axs[3].imshow(ndwi)\n",
    "    axs[3].set_title(\"NDWI\")\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # with rasterio.open(patch_path) as patch_src:\n",
    "    #     rgb = patch_src.read([4, 3, 2])  # Use bands B4, B3, B2 for RGB\n",
    "    #     rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    #     rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n",
    "    import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# List of image and mask file paths (replace with your file paths)\n",
    "image_mask_pairs = [\n",
    "    ('path_to_image1.jpg', 'path_to_mask1.png'),\n",
    "    ('path_to_image2.jpg', 'path_to_mask2.png'),\n",
    "    # Add more pairs as needed\n",
    "]\n",
    "\n",
    "\n",
    "def cvt_RGB(images):\n",
    "    rgb_images = []\n",
    "    for i in range(images.shape[0]):\n",
    "        rgb = images[i][[4-1, 3-1, 2-1]] # Use bands B4, B3, B2 for RGB\n",
    "        rgb = np.transpose(rgb, (1, 2, 0))\n",
    "        rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n",
    "        rgb_images.append(rgb)\n",
    "    return np.array(rgb_images)\n",
    "\n",
    "def display(images, masks):\n",
    "    # Determine the number of pairs\n",
    "    num_pairs = images.shape[0]\n",
    "\n",
    "    # Calculate layout: use 2 columns per pair (image + mask), adjust rows dynamically\n",
    "    cols = 2  # One column for image, one for mask\n",
    "    rows = num_pairs  # One row per pair\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "\n",
    "    # Handle case of single pair (axes is not a 2D array)\n",
    "    if num_pairs == 1:\n",
    "        axes = np.array([axes]).reshape(1, -1)\n",
    "\n",
    "    # Iterate through each pair and display image and mask\n",
    "    for idx, (image, mask) in enumerate(zip(images, masks)):\n",
    "\n",
    "        # Display the original image\n",
    "        axes[idx, 0].imshow(image)\n",
    "        axes[idx, 0].set_title(f'Image {idx + 1}')\n",
    "        axes[idx, 0].axis('off')  # Hide axes\n",
    "    \n",
    "        # Display the segmentation mask\n",
    "        axes[idx, 1].imshow(mask, cmap='gray')  # Adjust cmap if needed\n",
    "        axes[idx, 1].set_title(f'Mask {idx + 1}')\n",
    "        axes[idx, 1].axis('off')  # Hide axes\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:16.988622Z",
     "iopub.status.busy": "2025-05-25T19:00:16.988380Z",
     "iopub.status.idle": "2025-05-25T19:00:17.012058Z",
     "shell.execute_reply": "2025-05-25T19:00:17.011293Z",
     "shell.execute_reply.started": "2025-05-25T19:00:16.988607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_date_tile(filename):\n",
    "    \"\"\"Extract date and tile from filename using regex.\"\"\"\n",
    "    pattern = r'^(\\d{1,2}-\\d{1,2}-\\d{2})_([A-Z0-9]+)_\\d+$'\n",
    "    match = re.match(pattern, filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid filename format: {filename}\")\n",
    "    return match.groups()  # Returns tuple (date, tile)\n",
    "\n",
    "def create_marida_df(data_path, mode='train'):\n",
    "    \"\"\"Create DataFrame from MARIDA dataset files.\"\"\"\n",
    "    # Determine split file based on mode\n",
    "    split_files = {'train': 'train_X.txt', 'val': 'val_X.txt', 'test': 'test_X.txt'}\n",
    "    items_list_path = os.path.join(data_path, 'splits', split_files[mode])\n",
    "\n",
    "    # Read items list\n",
    "    with open(items_list_path, 'r') as file:\n",
    "        items = [item.strip() for item in file]\n",
    "\n",
    "    # Base path for patches\n",
    "    items_path = os.path.join(data_path, 'patches')\n",
    "\n",
    "    # Prepare data lists\n",
    "    data = {\n",
    "        'image': [],\n",
    "        'mask': [],\n",
    "        'confidence': [],\n",
    "        'date': [],\n",
    "        'tile': []\n",
    "    }\n",
    "\n",
    "    # Process each item\n",
    "    for item in items:\n",
    "        tile = \"_\".join(item.split(\"_\")[:-1])\n",
    "        tile_path = os.path.join(items_path, f\"S2_{tile}\")\n",
    "\n",
    "        # Define file paths\n",
    "        base_name = f'S2_{item}'\n",
    "        paths = {\n",
    "            'image': os.path.join(tile_path, f'{base_name}.tif'),\n",
    "            'mask': os.path.join(tile_path, f'{base_name}_cl.tif'),\n",
    "            'confidence': os.path.join(tile_path, f'{base_name}_conf.tif')\n",
    "        }\n",
    "\n",
    "        # Check if all files exist\n",
    "        if all(os.path.exists(p) for p in paths.values()):\n",
    "            data['image'].append(paths['image'])\n",
    "            data['mask'].append(paths['mask'])\n",
    "            data['confidence'].append(paths['confidence'])\n",
    "            date, tile = extract_date_tile(item)\n",
    "            data['date'].append(date)\n",
    "            data['tile'].append(tile)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# MARIDA labels dictionary\n",
    "MARIDA_LABELS = {\n",
    "    i: label for i, label in enumerate([\n",
    "        'Marine Debris', 'Dense Sargassum', 'Sparse Sargassum', 'Natural Organic Material',\n",
    "        'Ship', 'Clouds', 'Marine Water', 'Sediment-Laden Water', 'Foam', 'Turbid Water',\n",
    "        'Shallow Water', 'Waves', 'Cloud Shadows', 'Wakes', 'Mixed Water'\n",
    "    ], 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.013109Z",
     "iopub.status.busy": "2025-05-25T19:00:17.012863Z",
     "iopub.status.idle": "2025-05-25T19:00:17.031853Z",
     "shell.execute_reply": "2025-05-25T19:00:17.031302Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.013095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def compute_invalid_pixels(image_paths, mask_paths):\n",
    "    \"\"\"\n",
    "    Compute per-band statistics for Sentinel-2 L1C ACOLITE-processed images using segmentation masks.\n",
    "    Creates a mask to exclude invalid pixels (NaNs, negative values, specified no-data value).\n",
    "    \n",
    "    Parameters:\n",
    "    - image_paths: List of paths to image files (e.g., GeoTIFF with 11 bands).\n",
    "    - mask_paths: List of paths to segmentation mask files (single-band, integer class labels).\n",
    "    - class_labels: List of mask class labels to include (e.g., [1, 2] for vegetation and water).\n",
    "                   If None, include all non-zero labels (excluding background).\n",
    "    - invalid_value: Optional value to treat as invalid in images (e.g., -9999).\n",
    "    \n",
    "    Returns:\n",
    "    - mean_per_band: List of per-band means for each image.\n",
    "    - std_per_band: List of per-band standard deviations for each image.\n",
    "    \"\"\"\n",
    "    mean_per_band = []  # Initialize as list\n",
    "    std_per_band = []   # Initialize as list\n",
    "    positive_pixels = []\n",
    "    tot_pixels = [];\n",
    "    images_with_invalid_pixels = []\n",
    "    black_list = []\n",
    "    accumulator = None\n",
    "    no_data_pixels = []\n",
    "    neg_pixels = []\n",
    "    nan_pixels = []\n",
    "    gt1_pixels = []\n",
    "    imgs_with_invalid = []\n",
    "    positive_pixels = []\n",
    "    min_vals = []\n",
    "    max_vals = []\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        # Load image and mask\n",
    "        with rasterio.open(img_path) as src_img, rasterio.open(mask_path) as src_mask:\n",
    "            image = src_img.read()  # Shape: (bands, height, width)\n",
    "            mask = src_mask.read(1)  # Shape: (height, width)\n",
    "            \n",
    "            # Convert image to float for NaN handling\n",
    "            image = image.astype(float)\n",
    "\n",
    "            nan_mask = np.isnan(image)\n",
    "            neg_mask = (image < 0)\n",
    "            too_big_mask = (image > 1)\n",
    "            no_data_mask = (image == src_img.nodata)\n",
    "            nan_pixels.append(np.sum(nan_mask))\n",
    "            neg_pixels.append(np.sum(neg_mask))\n",
    "            gt1_pixels.append(np.sum(too_big_mask))\n",
    "            no_data_pixels.append(np.sum(no_data_mask))\n",
    "            imgs_with_invalid.append(img_path)\n",
    "            positive_pixels.append(np.sum(mask > 0))\n",
    "            min_vals.append(np.min(image))\n",
    "            max_vals.append(np.max(image))\n",
    "    df = pd.DataFrame({'image' : imgs_with_invalid, 'no data pixels' : no_data_pixels, 'negative pixels' : neg_pixels,\n",
    "                      'nan pixels' : nan_pixels, 'high value pixels' :  gt1_pixels, 'debris pixels' : positive_pixels,\n",
    "                      'min values' : min_vals, 'max values' : max_vals})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.032726Z",
     "iopub.status.busy": "2025-05-25T19:00:17.032506Z",
     "iopub.status.idle": "2025-05-25T19:00:17.111924Z",
     "shell.execute_reply": "2025-05-25T19:00:17.111295Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.032709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Setting batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.113000Z",
     "iopub.status.busy": "2025-05-25T19:00:17.112710Z",
     "iopub.status.idle": "2025-05-25T19:00:17.127808Z",
     "shell.execute_reply": "2025-05-25T19:00:17.127293Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.112984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_stats(image_files, discard_negatives = False, discard_gt_1 = False):\n",
    "    bands_std = []\n",
    "    bands_mean = []\n",
    "    valid_pixels = []\n",
    "\n",
    "    for band_idx in range(11):\n",
    "        arrays = [da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto')\n",
    "                  for f in image_files]\n",
    "        stack = da.stack(arrays)\n",
    "        #valid = (stack != rasterio.open(image_files[0]).nodata) & (stack >= 0)\n",
    "        if discard_negatives and  discard_gt_1: \n",
    "            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n",
    "                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) & \n",
    "                              (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1) \n",
    "                              for f in image_files])\n",
    "        elif discard_gt_1 :\n",
    "            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n",
    "                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1)  \n",
    "                              for f in image_files])\n",
    "        elif discard_negatives:\n",
    "            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n",
    "                  & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) \n",
    "                  for f in image_files])\n",
    "        else :\n",
    "            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n",
    "                  for f in image_files])\n",
    "                         \n",
    "        # Compute number of valid pixels\n",
    "        valid_count = da.sum(valid).compute()\n",
    "        valid_pixels.append(valid_count)\n",
    "        mean = da.nanmean(stack[valid]).compute()\n",
    "        std = da.nanstd(stack[valid]).compute()\n",
    "        bands_mean.append(mean)\n",
    "        bands_std.append(std)\n",
    "        print(f\"Band {band_idx} - Mean: {mean}, Std: {std}\")\n",
    "    return {'mean' : np.array(bands_mean), 'std': np.array(bands_std),'valid pixels' : np.array(valid_pixels) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.128736Z",
     "iopub.status.busy": "2025-05-25T19:00:17.128491Z",
     "iopub.status.idle": "2025-05-25T19:00:17.144499Z",
     "shell.execute_reply": "2025-05-25T19:00:17.143399Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.128719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def computing_labeled_pixels_stats(mask_paths):\n",
    "    arrays = [da.from_array(rasterio.open(f).read(1), chunks='auto')\n",
    "                  for f in mask_paths]\n",
    "    stack = da.stack(arrays)\n",
    "    valid = stack > 0\n",
    "    labeled_count = da.sum(valid).compute()\n",
    "    return labeled_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.145711Z",
     "iopub.status.busy": "2025-05-25T19:00:17.145450Z",
     "iopub.status.idle": "2025-05-25T19:00:17.161304Z",
     "shell.execute_reply": "2025-05-25T19:00:17.160784Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.145690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_invalid_mask(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        image = src.read()\n",
    "        \n",
    "        invalid_mask = image == src.nodata\n",
    "        invalid_mask |= np.isnan(image)\n",
    "        invalid_mask |= image < 0\n",
    "        invalid_mask |= image > 1\n",
    "        invalid_mask = np.any(invalid_mask, axis=0)\n",
    "        return invalid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.162359Z",
     "iopub.status.busy": "2025-05-25T19:00:17.162080Z",
     "iopub.status.idle": "2025-05-25T19:00:17.175981Z",
     "shell.execute_reply": "2025-05-25T19:00:17.174779Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.162343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_invalid_mask(image, no_data):\n",
    "    invalid_mask = image == no_data\n",
    "    invalid_mask |= np.isnan(image)\n",
    "    invalid_mask |= image < -1.5\n",
    "    invalid_mask |= image > 1.5\n",
    "    #invalid_mask = np.any(invalid_mask, axis=0)\n",
    "    return invalid_mask  #torch.fromnumpy(invalid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.177183Z",
     "iopub.status.busy": "2025-05-25T19:00:17.176932Z",
     "iopub.status.idle": "2025-05-25T19:00:17.195505Z",
     "shell.execute_reply": "2025-05-25T19:00:17.194649Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.177162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def select_bg_pixels(image, debris_mask, r1=5, r2=20, target_ratio=5):\n",
    "    H, W = debris_mask.shape\n",
    "    \n",
    "    #target_ratio = 5  # Debris-to-background ratio (1:5)\n",
    "\n",
    "    # Create structuring elements (circular or square)\n",
    "    se_r1 = disk(r1) if r1 > 0 else np.ones((1, 1))  # Inner dilation kernel\n",
    "    se_r2 = disk(r2)                         # Outer dilation kernel\n",
    "    #print('before binary dilation')\n",
    "    # Dilate debris mask with r1 and r2\n",
    "    dilated_r1 = binary_dilation(debris_mask, structure=se_r1)\n",
    "    dilated_r2 = binary_dilation(debris_mask, structure=se_r2)\n",
    "    #print('before anular mask')\n",
    "    # Compute annular region: pixels in dilated_r2 but not in dilated_r1\n",
    "    annular_mask = dilated_r2 & ~dilated_r1\n",
    "\n",
    "    # Sample background pixels from annular region\n",
    "    valid_background_coords = np.where(annular_mask)\n",
    "    num_debris = np.sum(debris_mask)\n",
    "    num_background = min(len(valid_background_coords[0]), num_debris * target_ratio)\n",
    "    if num_background > 0:\n",
    "        sample_idx = np.random.choice(len(valid_background_coords[0]), size=num_background, replace=False)\n",
    "        background_coords = [(valid_background_coords[0][i], valid_background_coords[1][i]) for i in sample_idx]\n",
    "    else:\n",
    "        print(\"Warning: No valid background pixels in annular region. Increase r2 or check mask.\")\n",
    "\n",
    "    # Create background mask (optional, for visualization or training)\n",
    "    background_mask = np.zeros_like(debris_mask)\n",
    "    for x, y in background_coords:\n",
    "        background_mask[x, y] = 1\n",
    "    return background_mask\n",
    "\n",
    "# Optional: Filter by features (e.g., RGB values for water-like pixels)\n",
    "# Example: If image is RGB, filter pixels with low green channel (common for water)\n",
    "# image = ...  # Your RGB or multispectral image\n",
    "# valid_background = [coord for coord in background_coords if image[coord[0], coord[1], 1] < threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.196452Z",
     "iopub.status.busy": "2025-05-25T19:00:17.196167Z",
     "iopub.status.idle": "2025-05-25T19:00:17.213887Z",
     "shell.execute_reply": "2025-05-25T19:00:17.213107Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.196430Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def batch_process_marida_masks(masks, dataset_ids, device='cpu'):\n",
    "    \"\"\"\n",
    "    Process masks for dataset_id == 0 (MARIDA) at the batch level.\n",
    "    - Set classes [1, 2, 3, 4, 9] to 2 (debris).\n",
    "    - Set class 0 to 0 (unlabeled), other classes to 1 (non-debris).\n",
    "    \n",
    "    Args:\n",
    "        masks: Tensor [batch_size, H, W] (integer-valued masks)\n",
    "        dataset_ids: Tensor [batch_size] (dataset IDs)\n",
    "        device: Device for PyTorch operations ('cpu' or 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "        marida_masks: Tensor [batch_size, H, W] with values 0, 1, 2\n",
    "    \"\"\"\n",
    "    batch_size, H, W = masks.shape\n",
    "    marida_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n",
    "    \n",
    "    # Identify masks with dataset_id == 0\n",
    "    marida_mask = (dataset_ids == 0)  # [batch_size], boolean\n",
    "    if not marida_mask.any():\n",
    "        return marida_masks\n",
    "    \n",
    "    # Select masks for dataset_id == 0\n",
    "    selected_masks = masks[marida_mask]  # [num_marida, H, W]\n",
    "    \n",
    "    # Set classes [1, 2, 3, 4, 9] to 2\n",
    "    debris_classes = torch.tensor([1, 2, 3, 4, 9], device=device)\n",
    "    is_debris = torch.isin(selected_masks, debris_classes)  # [num_marida, H, W]\n",
    "    marida_masks[marida_mask] = torch.where(\n",
    "        is_debris,\n",
    "        torch.tensor(2, dtype=torch.int64, device=device),\n",
    "        selected_masks  # Temporarily keep original values\n",
    "    )\n",
    "    # for idx in range( marida_masks[marida_mask].shape[0]):\n",
    "    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n",
    "    # Set non-zero, non-debris pixels to 1\n",
    "    marida_masks[marida_mask] = torch.where(\n",
    "        (marida_masks[marida_mask] != 0) & (marida_masks[marida_mask] != 2),\n",
    "        torch.tensor(1, dtype=torch.int64, device=device),\n",
    "        marida_masks[marida_mask]\n",
    "    )\n",
    "    # print('only 3 values : ')\n",
    "    # for idx in range( marida_masks[marida_mask].shape[0]):\n",
    "    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n",
    "    marida_masks[marida_mask] = marida_masks[marida_mask] - 1\n",
    "    #print('after subtr')\n",
    "    # for idx in range( marida_masks[marida_mask].shape[0]):\n",
    "    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n",
    "    return marida_masks\n",
    "\n",
    "\n",
    "\n",
    "# # Custom collate function\n",
    "# def custom_collate_fn(batch):\n",
    "#     images, masks, dataset_ids = zip(*batch)\n",
    "#     images = torch.stack(images)\n",
    "#     masks = torch.stack(masks)\n",
    "#     dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)\n",
    "    \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n",
    "    \n",
    "#     final_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n",
    "#                                          target_ratio=5, threshold=0.5, device=device)\n",
    "    \n",
    "#     return images, masks, final_masks, dataset_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.214717Z",
     "iopub.status.busy": "2025-05-25T19:00:17.214543Z",
     "iopub.status.idle": "2025-05-25T19:00:17.229008Z",
     "shell.execute_reply": "2025-05-25T19:00:17.228379Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.214702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def torch_dilate(mask, kernel_size, device='cpu'):\n",
    "    \"\"\"Apply dilation to a batch of masks using PyTorch convolution.\"\"\"\n",
    "    kernel = torch.ones(1, 1, kernel_size, kernel_size, device=device, dtype=torch.float32)\n",
    "    mask = mask.float().unsqueeze(1)  # [batch_size, 1, H, W]\n",
    "    dilated = torch.nn.functional.conv2d(mask, kernel, padding=kernel_size // 2) > 0\n",
    "    return dilated.squeeze(1).bool()  # [batch_size, H, W]\n",
    "\n",
    "def batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, target_ratio=5, threshold=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compute annular background masks for a batch of masks, only for dataset_id == 1.\n",
    "    - Set debris pixels (masks == 1) to 2 in bg_masks.\n",
    "    - Set randomly sampled annular pixels to 1 in bg_masks.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor [batch_size, C, H, W] \n",
    "        masks: Tensor [batch_size, H, W] (binary debris masks)\n",
    "        dataset_ids: Tensor [batch_size] (dataset IDs)\n",
    "        r1, r2: Radii for inner and outer dilation\n",
    "        target_ratio: Debris-to-background pixel ratio\n",
    "        threshold: Optional threshold for filtering (e.g., green channel)\n",
    "        device: Device for PyTorch operations ('cpu' or 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "        bg_masks: Tensor [batch_size, H, W] with values 0 (default), 1 (background), 2 (debris)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, H, W = masks.shape\n",
    "    # Initialize bg_masks with zeros (int64 to support values 0, 1, 2)\n",
    "    bg_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n",
    "    \n",
    "    # Identify masks to process (dataset_id == 1)\n",
    "    valid_mask = (dataset_ids == 1)  # [batch_size], boolean{\n",
    "    #print(f'LR indices {valid_mask}')\n",
    "    if not valid_mask.any():\n",
    "        return bg_masks  # Return zeros if no masks need processing\n",
    "    \n",
    "    # Select masks for dataset_id == 1\n",
    "    selected_masks = masks[valid_mask]  # [num_valid, H, W]\n",
    "    # for idx in range(selected_masks.shape[0]):\n",
    "    #     print(f'num debris pixels : {torch.sum(selected_masks[idx])}')\n",
    "    # Set debris pixels to 2 for selected masks\n",
    "    bg_masks[valid_mask] = selected_masks * 2  # Where selected_masks == 1, set bg_masks to 2\n",
    "    \n",
    "    # Perform dilation on selected masks\n",
    "    dilated_r1 = torch_dilate(selected_masks, 2 * r1 + 1, device=device)  # [num_valid, H, W]\n",
    "    dilated_r2 = torch_dilate(selected_masks, 2 * r2 + 1, device=device)  # [num_valid, H, W]\n",
    "    annular_masks = dilated_r2 & ~dilated_r1  # [num_valid, H, W]\n",
    "    \n",
    "    # Sample background pixels for each selected mask\n",
    "    for idx in range(annular_masks.shape[0]):\n",
    "        valid_coords = torch.where(annular_masks[idx])  # Tuple of (row, col) indices\n",
    "        #print(f'unique values in mask {idx} : {torch.unique(selected_masks[idx])}')\n",
    "        num_debris = torch.sum(selected_masks[idx] > 0).item()\n",
    "        #print(f'num debris for index {idx} : {num_debris}')\n",
    "        num_background = min(len(valid_coords[0]), int(num_debris * target_ratio))\n",
    "        \n",
    "        if num_background > 0:\n",
    "            # Randomly sample indices and set to 1\n",
    "            sample_indices = torch.randperm(len(valid_coords[0]), device=device)[:num_background]\n",
    "            bg_masks[valid_mask.nonzero(as_tuple=True)[0][idx], \n",
    "                     valid_coords[0][sample_indices], \n",
    "                     valid_coords[1][sample_indices]] = 1\n",
    "        else :\n",
    "            print(f'no background selected for index {idx}. Num debrid : {num_debris} Num background : {num_background}')\n",
    "            print(f'valid coords {len(valid_coords)}')\n",
    "            print(f'unique valus : {torch.unique(selected_masks[idx])}')\n",
    "    \n",
    "    # # Optional: Filter by image features (e.g., green channel) for dataset_id == 1\n",
    "    # if threshold is not None and images is not None:\n",
    "    #     valid_pixels = images[valid_mask, 1, :, :] < threshold  # Green channel\n",
    "    #     # Only apply filtering to background pixels (value 1), preserve debris pixels (value 2)\n",
    "    #     bg_masks[valid_mask] = torch.where(\n",
    "    #         bg_masks[valid_mask] == 1,\n",
    "    #         bg_masks[valid_mask] & valid_pixels,\n",
    "    #         bg_masks[valid_mask]\n",
    "    #     )\n",
    "    bg_masks[valid_mask] = bg_masks[valid_mask] - 1\n",
    "    return bg_masks\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    # print(f'custom collate function batch {len(batch)}')\n",
    "    # print(f'custom collate function batch type {type(batch)}')\n",
    "    # print(f'custom collate function batch[1] type {type(batch[1])}')\n",
    "    # print(f'custom collate function batch[1] len  {len(batch[1])}')\n",
    "    images, masks, dataset_ids = zip(*batch)\n",
    "    images = torch.stack(images)  # [batch_size, C, H, W]\n",
    "    masks = torch.stack(masks)    # [batch_size, H, W]\n",
    "    dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)  # [batch_size]\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n",
    "    \n",
    "    # Compute background masks\n",
    "    lr_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n",
    "                                      target_ratio=LR_ratio, device=device)\n",
    "    marida_masks = batch_process_marida_masks(masks, dataset_ids, device=device)\n",
    "    masks = lr_masks + marida_masks\n",
    "    \n",
    "    return images, masks, dataset_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.230514Z",
     "iopub.status.busy": "2025-05-25T19:00:17.229731Z",
     "iopub.status.idle": "2025-05-25T19:00:17.252382Z",
     "shell.execute_reply": "2025-05-25T19:00:17.251655Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.230489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Seeding for reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.253227Z",
     "iopub.status.busy": "2025-05-25T19:00:17.253007Z",
     "iopub.status.idle": "2025-05-25T19:00:17.256792Z",
     "shell.execute_reply": "2025-05-25T19:00:17.256203Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.253209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# # Download some pre-computed data \n",
    "\n",
    "#file_id = \"10bMAQaV2-EXCu52ON-6m0qh7u7__v-hH\"\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_df_invalid_info.csv', quiet=False)\n",
    "#file_id = '19VzQze4sBt76ylEcishVQNpGIFYSkseT'\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_val_df_invalid_info.csv', quiet=False)\n",
    "#file_id = '1CvUC8FAqj1aUV8fwrTljU3mC-geWz0it'\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_test_df_invalid_info.csv', quiet=False)\n",
    "#file_id = \"1YTJmy8X-xIo8dV7Qpq4h7wOi7kUAW4sw\"\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_df_invalid_info.csv', quiet=False)\n",
    "#file_id = '1CzvC9VLbzqyh9LbhyxWhtIHaz7o1hmak'\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_val_df_invalid_info.csv', quiet=False)\n",
    "#file_id = '1wrD41CDQud69AMOyHigw0-DR85Id4zDM'\n",
    "#gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/global_stats.npz', quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.260899Z",
     "iopub.status.busy": "2025-05-25T19:00:17.260506Z",
     "iopub.status.idle": "2025-05-25T19:00:17.274302Z",
     "shell.execute_reply": "2025-05-25T19:00:17.273591Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.260878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # check that the \n",
    "# ! ls /kaggle/input/litter-windrows-patches\n",
    "# # add the lr dataset to path to import code to prepare the dataset\n",
    "# sys.path.append('/kaggle/input/litter-windrows-patches')\n",
    "# # import functions to prepare dataset\n",
    "# from prepare_dataset import  get_image_and_mask_paths, split_and_save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.275274Z",
     "iopub.status.busy": "2025-05-25T19:00:17.275013Z",
     "iopub.status.idle": "2025-05-25T19:00:17.288677Z",
     "shell.execute_reply": "2025-05-25T19:00:17.288137Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.275232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#! git clone https://github.com/sheikhazhanmohammed/SADMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.289548Z",
     "iopub.status.busy": "2025-05-25T19:00:17.289311Z",
     "iopub.status.idle": "2025-05-25T19:00:17.302802Z",
     "shell.execute_reply": "2025-05-25T19:00:17.302222Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.289529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sys.path.append('/kaggle/working/SADMA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.304240Z",
     "iopub.status.busy": "2025-05-25T19:00:17.303622Z",
     "iopub.status.idle": "2025-05-25T19:00:17.316593Z",
     "shell.execute_reply": "2025-05-25T19:00:17.315994Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.304219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # define a variable for the lr dataset\n",
    "# LW_path = '/kaggle/input/litter-windrows-patches'\n",
    "# lr_images, lr_masks = get_image_and_mask_paths(LW_path)\n",
    "# ! mkdir ./LR_splits\n",
    "# split_and_save_data(lr_images, lr_masks, './LR_splits' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.317865Z",
     "iopub.status.busy": "2025-05-25T19:00:17.317216Z",
     "iopub.status.idle": "2025-05-25T19:00:17.330001Z",
     "shell.execute_reply": "2025-05-25T19:00:17.329445Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.317844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ! ls ./LR_splits/splits\n",
    "# LR_splits_path = '/kaggle/working/LR_splits/splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.330989Z",
     "iopub.status.busy": "2025-05-25T19:00:17.330677Z",
     "iopub.status.idle": "2025-05-25T19:00:17.343929Z",
     "shell.execute_reply": "2025-05-25T19:00:17.343376Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.330964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# with open(LR_splits_path+'/train_X.txt', \"r\") as file:\n",
    "#     display(file.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.344791Z",
     "iopub.status.busy": "2025-05-25T19:00:17.344588Z",
     "iopub.status.idle": "2025-05-25T19:00:17.626958Z",
     "shell.execute_reply": "2025-05-25T19:00:17.626298Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.344770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_mapping.txt  patches  shapefiles  splits\n",
      "annotation     multiclass_splits  prepare_dataset.ipynb\n",
      "binary_splits  patches\t\t  README.md\n"
     ]
    }
   ],
   "source": [
    "! ls /kaggle/input/marida-marine-debrish-dataset\n",
    "MARIDA_path = '/kaggle/input/marida-marine-debrish-dataset'\n",
    "! ls /kaggle/input/litter-windrows-patches\n",
    "LR_splits_path = '/kaggle/input/litter-windrows-patches/binary_splits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:17.628442Z",
     "iopub.status.busy": "2025-05-25T19:00:17.628140Z",
     "iopub.status.idle": "2025-05-25T19:00:27.884272Z",
     "shell.execute_reply": "2025-05-25T19:00:27.883709Z",
     "shell.execute_reply.started": "2025-05-25T19:00:17.628409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MARIDA dataframe\n",
    "marida_df = create_marida_df(MARIDA_path)\n",
    "#marida_df_invalid = compute_invalid_pixels(marida_df['image'].tolist(), marida_df['mask'].tolist())\n",
    "#marida_df_invalid.to_csv('/kaggle/working/marida_with_invalid.csv')\n",
    "marida_df_invalid = pd.read_csv('/kaggle/working/marida_df_invalid_info.csv')\n",
    "marida_df_F = marida_df.drop(marida_df_invalid[marida_df_invalid['nan pixels']>0].index)\n",
    "\n",
    "# MARIDA val dataframe\n",
    "marida_val_df = create_marida_df(MARIDA_path, 'val')\n",
    "#marida_val_df_invalid = compute_invalid_pixels(marida_val_df['image'].tolist(), marida_val_df['mask'].tolist())\n",
    "#marida_val_df_invalid.to_csv('/kaggle/working/marida_val_df_invalid.csv')\n",
    "marida_val_df_invalid =pd.read_csv('/kaggle/working/marida_val_df_invalid_info.csv')\n",
    "marida_val_df_F = marida_val_df.drop(marida_val_df_invalid[marida_val_df_invalid['nan pixels'] > 0].index)\n",
    "\n",
    "# MARIDA test dataframe\n",
    "marida_test_df = create_marida_df(MARIDA_path, 'test')\n",
    "#marida_test_df_invalid = compute_invalid_pixels(marida_test_df['image'].tolist(), marida_test_df['mask'].tolist())\n",
    "#marida_test_df_invalid.to_csv('/kaggle/working/marida_test_df_invalid.csv')\n",
    "marida_test_df_invalid =pd.read_csv('/kaggle/working/marida_test_df_invalid_info.csv')\n",
    "marida_test_df_F = marida_test_df.drop(marida_test_df_invalid[marida_test_df_invalid['nan pixels'] > 0].index)\n",
    "\n",
    "# LR dataframe\n",
    "\n",
    "lr_df = create_LR_dataframe(LR_splits_path)\n",
    "#lr_df_invalid = compute_invalid_pixels(lr_df['image'].tolist(), lr_df['mask'].tolist())\n",
    "#lr_df_invalid.to_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\n",
    "lr_df_invalid = pd.read_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\n",
    "lr_df_F = lr_df.drop(lr_df_invalid[lr_df_invalid['high value pixels'] > 0].index)\n",
    "\n",
    "#LR val dataset\n",
    "lr_val_df = create_LR_dataframe(LR_splits_path, 'val')\n",
    "#lr_val_df_invalid = compute_invalid_pixels(lr_val_df['image'].tolist(), lr_val_df['mask'].tolist())\n",
    "#lr_val_df_invalid.to_csv('/kaggle/working/litter_rows_val_invalid_info.csv')\n",
    "lr_val_df_invalid = pd.read_csv('/kaggle/working/litter_rows_val_df_invalid_info.csv')\n",
    "lr_val_df_F= lr_val_df.drop(lr_val_df_invalid[lr_val_df_invalid['high value pixels']>0].index)\n",
    "\n",
    "\n",
    "#lr_test_df_invalid = compute_invalid_pixels(lr_test_df['image'].tolist(), lr_test_df['mask'].tolist())\n",
    "#lr_test_df_invalid.to_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\n",
    "#lr_test_df_invalid = pd.read_csv('/kaggle/working/litter_rows_df_invalid_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.885230Z",
     "iopub.status.busy": "2025-05-25T19:00:27.884996Z",
     "iopub.status.idle": "2025-05-25T19:00:27.888612Z",
     "shell.execute_reply": "2025-05-25T19:00:27.887997Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.885207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lr valid = 79495168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.889872Z",
     "iopub.status.busy": "2025-05-25T19:00:27.889358Z",
     "iopub.status.idle": "2025-05-25T19:00:27.905509Z",
     "shell.execute_reply": "2025-05-25T19:00:27.905040Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.889854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#lr_stats = compute_stats(lr_df_filt['image'].tolist())\n",
    "#np.savez(\"/kaggle/working/lr_stats.npz\", first=lr_stats['mean'], second=lr_stats['std'])\n",
    "#marida_stats = compute_stats(marida_df['image'].tolist())\n",
    "#np.savez(\"/kaggle/working/my_marida_stats.npz\", first=marida_stats['mean'], second=marida_stats['std'])\n",
    "#global_stats = compute_stats(marida_df['image'].tolist() + lr_df_filt['image'].to_list())\n",
    "#np.savez(\"/kaggle/working/global_stats.npz\", first=global_stats['mean'], second=global_stats['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.906658Z",
     "iopub.status.busy": "2025-05-25T19:00:27.906409Z",
     "iopub.status.idle": "2025-05-25T19:00:27.922925Z",
     "shell.execute_reply": "2025-05-25T19:00:27.922274Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.906636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "global_stats = np.load('/kaggle/working/global_stats.npz')\n",
    "global_bands_mean = global_stats['first']\n",
    "global_bands_std = global_stats['second']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.923806Z",
     "iopub.status.busy": "2025-05-25T19:00:27.923619Z",
     "iopub.status.idle": "2025-05-25T19:00:27.935073Z",
     "shell.execute_reply": "2025-05-25T19:00:27.934405Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.923791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# global_bands_mean =np.array([0.03721786, 0.03547978, 0.03033651, 0.01722546, 0.01574046,\n",
    "#         0.01738895, 0.01939084, 0.01724032, 0.01895351, 0.0109694 ,\n",
    "#         0.00784716])\n",
    "# global_bands_std = np.array([0.03185222, 0.03198375, 0.03251331, 0.03379553, 0.03407218,\n",
    "#         0.04551132, 0.05334419, 0.05064404, 0.0578197 , 0.03721222,\n",
    "#         0.02560836])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.936385Z",
     "iopub.status.busy": "2025-05-25T19:00:27.935834Z",
     "iopub.status.idle": "2025-05-25T19:00:27.947454Z",
     "shell.execute_reply": "2025-05-25T19:00:27.946807Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.936368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#computing_labeled_pixels_stats(lr_df_filt['mask'].tolist())\n",
    "#computing_labeled_pixels_stats(marida_df['mask'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.948756Z",
     "iopub.status.busy": "2025-05-25T19:00:27.948193Z",
     "iopub.status.idle": "2025-05-25T19:00:27.961735Z",
     "shell.execute_reply": "2025-05-25T19:00:27.961149Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.948736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marida debris pixels 5092.826320000001\n",
      "marida_debris_fraction : 0.011860000000000002\n"
     ]
    }
   ],
   "source": [
    "marida_classes_distr = np.array([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n",
    " 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\n",
    "lr_debris_pixels = 92090\n",
    "marida_pixels = 429412\n",
    "marida_debris_pixels = np.sum(marida_classes_distr[[0,1,2,3,8]]) * marida_pixels\n",
    "print(f'marida debris pixels {marida_debris_pixels}')\n",
    "tot_glob_pixels = (len(lr_df_F) + len(marida_df_F))*256**2\n",
    "marida_debris_fraction = np.sum(marida_classes_distr[[0,1,2,3,8]])\n",
    "#debris_fraction = (lr_debris_pixels + marida_debris_pixels)/tot_glob_pixels\n",
    "print(f'marida_debris_fraction : {marida_debris_fraction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.962801Z",
     "iopub.status.busy": "2025-05-25T19:00:27.962488Z",
     "iopub.status.idle": "2025-05-25T19:00:27.979809Z",
     "shell.execute_reply": "2025-05-25T19:00:27.979230Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.962777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class distribution [0.96384548 0.03615452]\n"
     ]
    }
   ],
   "source": [
    "# Computing here the percentage of debris pixels across the two datasets\n",
    "# This will be used as class distribution to generate weights for the loss function\n",
    "LR_ratio = 20 # \n",
    "\n",
    "# For MARIDA the loss function uses only pixels in the 15 classes \n",
    "# The fraction of classes assimilated to marine debris is \n",
    "marida_debrix_pixels_distr = np.sum(marida_classes_distr[[0,1,2,3,8]])\n",
    "# For LR the DataSet will sample backgroung pixels with a given ratio, stored in the variable LR_ratio\n",
    "# Then the effective ratio \n",
    "effective_ratio = (1/LR_ratio * len(lr_df_F) + 0.011860000000000002 * len(marida_df_F))/(len(lr_df_F) + len(marida_df_F))\n",
    "#print(f'effective global ratio {effective_ratio}')\n",
    "class_distribution = np.array([1 - effective_ratio, effective_ratio])\n",
    "print(f'class distribution {class_distribution}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.980859Z",
     "iopub.status.busy": "2025-05-25T19:00:27.980611Z",
     "iopub.status.idle": "2025-05-25T19:00:27.994657Z",
     "shell.execute_reply": "2025-05-25T19:00:27.994029Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.980838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:27.995461Z",
     "iopub.status.busy": "2025-05-25T19:00:27.995241Z",
     "iopub.status.idle": "2025-05-25T19:00:28.009583Z",
     "shell.execute_reply": "2025-05-25T19:00:28.008979Z",
     "shell.execute_reply.started": "2025-05-25T19:00:27.995446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Other code references  \n",
    "# https://github.com/MarcCoru/marinedebrisdetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:28.010358Z",
     "iopub.status.busy": "2025-05-25T19:00:28.010129Z",
     "iopub.status.idle": "2025-05-25T19:00:28.027819Z",
     "shell.execute_reply": "2025-05-25T19:00:28.027280Z",
     "shell.execute_reply.started": "2025-05-25T19:00:28.010343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MARIDA CLASSES\n",
    "# {\n",
    "#  1: \"Marine Debris\",\n",
    "#  2: \"Dense Sargassum\", \n",
    "#  3: \"Sparse Sargassum\", \n",
    "#  4: \"Natural Organic Material\", \n",
    "#  5: \"Ship\", \n",
    "#  6: \"Clouds\", \n",
    "#  7: \"Marine Water\", \n",
    "#  8: \"Sediment-Laden Water\", \n",
    "#  9: \"Foam\", \n",
    "#  10: \"Turbid Water\", \n",
    "#  11: \"Shallow Water\", \n",
    "#  12: \"Waves\", \n",
    "#  13: \"Cloud Shadows\", \n",
    "#  14: \"Wakes\", \n",
    "#  15: \"Mixed Water\"\n",
    "# }\n",
    "\n",
    "\n",
    "# From marinedebrisdetector \n",
    "# DEBRIS_CLASSES = [1,2,3,4,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:28.028767Z",
     "iopub.status.busy": "2025-05-25T19:00:28.028528Z",
     "iopub.status.idle": "2025-05-25T19:00:28.041907Z",
     "shell.execute_reply": "2025-05-25T19:00:28.041187Z",
     "shell.execute_reply.started": "2025-05-25T19:00:28.028747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/drive/folders/1rntiw5BvOs80eIbpOu7dk9g1BfOVw61-?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:28.042835Z",
     "iopub.status.busy": "2025-05-25T19:00:28.042628Z",
     "iopub.status.idle": "2025-05-25T19:00:28.057395Z",
     "shell.execute_reply": "2025-05-25T19:00:28.056817Z",
     "shell.execute_reply.started": "2025-05-25T19:00:28.042813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class RandomRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return vF.rotate(x, angle)\n",
    "    \n",
    "def gen_weights(class_distribution, c = 1.02):\n",
    "    return 1/torch.log(c + class_distribution)\n",
    "    \n",
    "transformTrain = transforms.Compose([transforms.ToTensor(),\n",
    "                                    RandomRotationTransform([-90, 0, 90, 180]),\n",
    "                                    transforms.RandomHorizontalFlip()])\n",
    "    \n",
    "transformTest = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "standardization = transforms.Normalize(global_bands_mean, global_bands_std) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:28.058369Z",
     "iopub.status.busy": "2025-05-25T19:00:28.058046Z",
     "iopub.status.idle": "2025-05-25T19:00:28.071848Z",
     "shell.execute_reply": "2025-05-25T19:00:28.071160Z",
     "shell.execute_reply.started": "2025-05-25T19:00:28.058353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gen_weights(class_distribution, c = 1.02):\n",
    "    return 1/torch.log(c + class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:28.072804Z",
     "iopub.status.busy": "2025-05-25T19:00:28.072599Z",
     "iopub.status.idle": "2025-05-25T19:00:32.878162Z",
     "shell.execute_reply": "2025-05-25T19:00:32.877171Z",
     "shell.execute_reply.started": "2025-05-25T19:00:28.072781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchvision import transforms\n",
    "# model import UNet, AttentionUNet, ResidualAttentionUNet  # From original script\n",
    "#f#rom dataloader import bands_mean, bands_std, RandomRotationTransform, class_distr, gen_weights\n",
    "#from metrics import Evaluation\n",
    "#from customLosses import FocalLoss\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MergedSegmentationDataset_B(Dataset):\n",
    "    \"\"\"\n",
    "    df_dataset1 : MARIDA dataset\n",
    "    df_dataset2 : LR dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, df_dataset1, df_dataset2, bands_mean, bands_std, selected_bands, transform=None, standardization=None):\n",
    "        \"\"\"\n",
    "        df_dataset1 : MARIDA\n",
    "        df_dataset2 : Litter Windrows\n",
    "        \"\"\"\n",
    "        self.bands_mean = bands_mean[selected_bands]\n",
    "        self.bands_std = bands_std[selected_bands]\n",
    "        self.transform = transform\n",
    "        self.standardization = standardization\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "        self.dataset_ids = []\n",
    "        self.image_paths = df_dataset1['image'].tolist() + df_dataset2['image'].tolist() \n",
    "        self.mask_paths =  df_dataset1['mask'].tolist() + df_dataset2['mask'].tolist() \n",
    "        self.dataset_ids = [0] * len(df_dataset1['image']) + [1] * len(df_dataset2['image'])\n",
    "        # Generate shuffled indices\n",
    "        indices = np.random.permutation(len(self.image_paths))\n",
    "        self.image_paths = np.array(self.image_paths)[indices]\n",
    "        self.mask_paths = np.array(self.mask_paths)[indices]\n",
    "        self.dataset_ids = np.array(self.dataset_ids)[indices]        \n",
    "        #print(self.dataset_ids)\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        ## preloading images in memory \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(f'idx {idx}') \n",
    "        max_seed = 2**32 - 1  # NumPy seed limit\n",
    "        #index_seed_seed = (42 + idx) % max_seed\n",
    "        #np.random.seed(index_seed_seed)\n",
    "        # Load Classsification Mask np.random.seed(self.seed + index)  # Deterministic per item\n",
    "        dataset_id = self.dataset_ids[idx]\n",
    "        # Open t#he GeoTIFF image file\n",
    "        #print(f'image path {self.image_paths[idx]}')\n",
    "        #print(f'mask path {self.mask_paths[idx]}')\n",
    "        with rasterio.open(self.image_paths[idx]) as src:\n",
    "            #print(f#\"Number of bands: {dataset.count}\")  # Check the number of bands\n",
    "            # Read all bands as a NumPy array\n",
    "            image = src.read()\n",
    "            # Keep the bands in selecred_bands\n",
    "            image = image[selected_bands, :, :]\n",
    "            invalid_mask = get_invalid_mask(image, src.nodata)\n",
    "            with rasterio.open(self.mask_paths[idx]) as src_mask:\n",
    "                mask = src_mask.read().astype(int)\n",
    "            debris_before_invalid = np.sum(mask)\n",
    "            invalid_pixels = np.sum(np.any(invalid_mask, axis=0))\n",
    "            mask[np.any(invalid_mask.astype(bool), axis=0, keepdims=True)] = 0 #I guess it makes sense not to feed invalid pixels to the loss function\n",
    "            #print(f'before inputing 2')\n",
    "            image[invalid_mask.astype(bool)] = np.tile(self.bands_mean[:, np.newaxis, np.newaxis], (1, 256, 256))[invalid_mask.astype(bool)]\n",
    "            #print(f'after inputing')\n",
    "            ## Since the model sees unvalid pixels anyway, it's better (?) to replace those with mean values ? \n",
    "            #print(f'mask type before transh {type(mask)} - {mask.dtype}')\n",
    "            #print(f'image type before transh {type(image)} - {image.dtype}')\n",
    "            #############\n",
    "            debris_after_invalid = np.sum(mask)\n",
    "            #############\n",
    "            if self.transform is not None:\n",
    "                # applying the same rotation on the image-mask pair\n",
    "                #print(f'transform - image shape {image.shape}')\n",
    "                #print(f'transform - mask shape {mask.shape}')\n",
    "                stack = np.concatenate([image, mask], axis=0).astype(np.float32) \n",
    "                stack = np.transpose(stack,(1, 2, 0)) #to channel last\n",
    "                #print(f'stack shape before transfrom {stack.shape}')\n",
    "                stack = self.transform(stack) #expects channel last, returns channel first\n",
    "               \n",
    "                #print(f'stack shape after transfrom {stack.shape}')\n",
    "                image = stack[:-1,:,:]\n",
    "                mask = stack[-1,:,:].long()\n",
    "                #print(f'image type {image.dtype}')\n",
    "                #print(f'image shape after transform {image.shape}')\n",
    "                #print(f'mask shape after transform {mask.shape}')\n",
    "\n",
    "                   \n",
    "            \n",
    "            if self.standardization is not None:\n",
    "                image = self.standardization(image)\n",
    "                \n",
    "            #mask = mask - 1 Moved to collate function\n",
    "            if isinstance(mask, np.ndarray):\n",
    "                mask = torch.from_numpy(mask).to(torch.long)\n",
    "            else:\n",
    "                mask = mask.to(torch.long)\n",
    "            if isinstance(image, np.ndarray):\n",
    "                image = torch.from_numpy(image).to(torch.float32)\n",
    "            else:\n",
    "                im = image.to(torch.float32)\n",
    "            if torch.sum(mask) == 0 :\n",
    "                print(f'{self.mask_paths[idx]} has no debris pixels')\n",
    "                print(f'debris pixels before invalid mask : {debris_before_invalid}')\n",
    "                print(f'debris pixels after invalid mask : {debris_after_invalid}')\n",
    "                print(f'invalid pixels : {invalid_pixels}')\n",
    "           \n",
    "        ## Add logic for transform\n",
    "\n",
    "            return image, mask, dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:32.879980Z",
     "iopub.status.busy": "2025-05-25T19:00:32.879120Z",
     "iopub.status.idle": "2025-05-25T19:00:32.900345Z",
     "shell.execute_reply": "2025-05-25T19:00:32.899317Z",
     "shell.execute_reply.started": "2025-05-25T19:00:32.879951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Conv2d(channels, channels // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(channels // 16, channels, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(outputChannel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(outputChannel, outputChannel)\n",
    "        self.bn2 = nn.BatchNorm2d(outputChannel)\n",
    "        self.downsample = downsample\n",
    "        self.ca = ChannelAttention(outputChannel)\n",
    "        self.sa = SpatialAttention()\n",
    "        \n",
    "    # def forward(self, x):\n",
    "    \n",
    "    #     residual = x\n",
    "    #     out = self.conv1(x)\n",
    "    #     out = self.bn1(out)\n",
    "    #     out = self.relu(out)\n",
    "    #     out = self.conv2(out)\n",
    "    #     out = self.bn2(out)\n",
    "    #     if self.downsample:\n",
    "    #         residual = self.downsample(x)\n",
    "    #     out += residual\n",
    "    #     out = self.relu(out)\n",
    "    #     caOutput = self.ca(out)\n",
    "    #     out = caOutput * out\n",
    "    #     saOutput = self.sa(out)\n",
    "    #     out = saOutput * out\n",
    "    #     return out, saOutput\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        caOutput = self.ca(out)\n",
    "        out = caOutput * out\n",
    "        saOutput = self.sa(out)\n",
    "        out = saOutput * out\n",
    "        return out, saOutput\n",
    "\n",
    "\n",
    "class DownSampleWithAttention(nn.Module):\n",
    "    def __init__(self, inputChannel, outputChannel):\n",
    "        super().__init__()\n",
    "        self.convolution = nn.Sequential(\n",
    "            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outputChannel),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outputChannel),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "        self.ca = ChannelAttention(outputChannel)\n",
    "        self.sa = SpatialAttention()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.convolution(x)\n",
    "        caOutput = self.ca(x)\n",
    "        x = caOutput * x\n",
    "        saOutput = self.sa(x)\n",
    "        x = saOutput * x\n",
    "        return x, saOutput\n",
    "\n",
    "    \n",
    "class UpSampleWithAttention(nn.Module):\n",
    "    def __init__(self, inputChannel, outputChannel):\n",
    "        super().__init__()\n",
    "        self.convolution = nn.Sequential(\n",
    "            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outputChannel),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(outputChannel),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.ca = ChannelAttention(outputChannel)\n",
    "        self.sa = SpatialAttention()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.convolution(x)\n",
    "        caOutput = self.ca(x)\n",
    "        x = caOutput * x\n",
    "        saOutput = self.sa(x)\n",
    "        x = saOutput * x\n",
    "        return x, saOutput\n",
    "\n",
    "class ResidualAttentionUNet(nn.Module):\n",
    "  def __init__(self, inputChannel, outputChannel):\n",
    "    super().__init__()\n",
    "    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n",
    "    self.downsample2 = DownSampleWithAttention(32, 64)\n",
    "    self.downsample3 = DownSampleWithAttention(64, 128)\n",
    "    self.downsample4 = DownSampleWithAttention(128, 256)\n",
    "    self.downsample5 = DownSampleWithAttention(256, 512)\n",
    "\n",
    "    self.residualBlock1 = ResidualBlock(512, 512)\n",
    "    self.residualBlock2 = ResidualBlock(512, 512)\n",
    "    self.residualBlock3 = ResidualBlock(512, 512)\n",
    "\n",
    "    self.upsample1 = UpSampleWithAttention(512, 256)\n",
    "    self.upsample2 = UpSampleWithAttention(512, 128)\n",
    "    self.upsample3 = UpSampleWithAttention(256, 64)\n",
    "    self.upsample4 = UpSampleWithAttention(128, 32)\n",
    "    self.upsample5 = UpSampleWithAttention(64, 32)\n",
    "    self.classification = nn.Sequential(\n",
    "            nn.Conv2d(32, outputChannel, kernel_size=1),\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    scale128, sa128down = self.downsample1(x)\n",
    "    scale64, sa64down = self.downsample2(scale128)\n",
    "    scale32, sa32down = self.downsample3(scale64)\n",
    "    scale16, sa64down = self.downsample4(scale32)\n",
    "    scale8, sa8down = self.downsample5(scale16)\n",
    "    \n",
    "    scale8, sa8down = self.residualBlock1(scale8)\n",
    "    scale8, sa8down = self.residualBlock2(scale8)\n",
    "    scale8, sa8down = self.residualBlock3(scale8)\n",
    "\n",
    "    upscale16, sa16up = self.upsample1(scale8)\n",
    "    \n",
    "    upscale16 = torch.cat([upscale16, scale16], dim=1)\n",
    "    upscale32, sa32up = self.upsample2(upscale16)\n",
    "    \n",
    "    upscale32 = torch.cat([upscale32, scale32], dim=1)\n",
    "    upscale64, sa64up = self.upsample3(upscale32)\n",
    "    \n",
    "    upscale64 = torch.cat([upscale64, scale64], dim=1)\n",
    "    upscale128, sa128up = self.upsample4(upscale64)\n",
    "    \n",
    "    upscale128 = torch.cat([upscale128, scale128], dim=1)\n",
    "    upscale256, sa256up = self.upsample5(upscale128)\n",
    "    finaloutput = self.classification(upscale256)\n",
    "    return finaloutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:32.901671Z",
     "iopub.status.busy": "2025-05-25T19:00:32.901296Z",
     "iopub.status.idle": "2025-05-25T19:00:32.938643Z",
     "shell.execute_reply": "2025-05-25T19:00:32.938034Z",
     "shell.execute_reply.started": "2025-05-25T19:00:32.901643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Evaluation(y_predicted, y_true):\n",
    "\n",
    "    micro_prec = precision_score(y_true, y_predicted, average='micro')\n",
    "    macro_prec = precision_score(y_true, y_predicted, average='macro')\n",
    "    weight_prec = precision_score(y_true, y_predicted, average='weighted')\n",
    "    \n",
    "    micro_rec = recall_score(y_true, y_predicted, average='micro')\n",
    "    macro_rec = recall_score(y_true, y_predicted, average='macro')\n",
    "    weight_rec = recall_score(y_true, y_predicted, average='weighted')\n",
    "        \n",
    "    macro_f1 = f1_score(y_true, y_predicted, average=\"macro\")\n",
    "    micro_f1 = f1_score(y_true, y_predicted, average=\"micro\")\n",
    "    weight_f1 = f1_score(y_true, y_predicted, average=\"weighted\")\n",
    "        \n",
    "    subset_acc = accuracy_score(y_true, y_predicted)\n",
    "    \n",
    "    iou_acc = jaccard_score(y_true, y_predicted, average='macro')\n",
    "\n",
    "    # Debris-specific metrics\n",
    "    debris_class = 1\n",
    "    debris_prec = precision_score(y_true, y_predicted, labels=[debris_class], average='macro')\n",
    "    debris_rec = recall_score(y_true, y_predicted, labels=[debris_class], average='macro')\n",
    "    debris_f1 = f1_score(y_true, y_predicted, labels=[debris_class], average='macro')\n",
    "    debris_iou = jaccard_score(y_true, y_predicted, labels=[debris_class], average='macro')\n",
    "\n",
    "    info = {\n",
    "            \"macroPrec\" : macro_prec,\n",
    "            \"microPrec\" : micro_prec,\n",
    "            \"weightPrec\" : weight_prec,\n",
    "            \"macroRec\" : macro_rec,\n",
    "            \"microRec\" : micro_rec,\n",
    "            \"weightRec\" : weight_rec,\n",
    "            \"macroF1\" : macro_f1,\n",
    "            \"microF1\" : micro_f1,\n",
    "            \"weightF1\" : weight_f1,\n",
    "            \"subsetAcc\" : subset_acc,\n",
    "            \"IoU\": iou_acc,\n",
    "            \"debris Prec\" : debris_prec,\n",
    "            \"debris Rec\" : debris_rec,\n",
    "            \"debris F1\" : debris_f1,\n",
    "            \"debris IoU\" : debris_iou\n",
    "            }\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:32.939635Z",
     "iopub.status.busy": "2025-05-25T19:00:32.939385Z",
     "iopub.status.idle": "2025-05-25T19:00:32.960161Z",
     "shell.execute_reply": "2025-05-25T19:00:32.959513Z",
     "shell.execute_reply.started": "2025-05-25T19:00:32.939617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "selected_bands = np.array([ 4, 6, 8, 11]) - 1 #bands conted from 0\n",
    "\n",
    "transformTrain = transforms.Compose([transforms.ToTensor(),\n",
    "                                    RandomRotationTransform([-90, 0, 90, 180]),\n",
    "                                    transforms.RandomHorizontalFlip()])\n",
    "    \n",
    "transformTest = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "standardization = transforms.Normalize(global_bands_mean[selected_bands].tolist(), global_bands_std[selected_bands].tolist())\n",
    "merged_ds = MergedSegmentationDataset_B(marida_df_F, lr_df_F, global_bands_mean, global_bands_std, selected_bands, transform=transformTrain, standardization= standardization)\n",
    "val_ds = MergedSegmentationDataset_B(marida_val_df_F, lr_val_df_F, global_bands_mean, global_bands_std,selected_bands, transform=transformTest, standardization= standardization )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:32.961160Z",
     "iopub.status.busy": "2025-05-25T19:00:32.960912Z",
     "iopub.status.idle": "2025-05-25T19:00:32.975650Z",
     "shell.execute_reply": "2025-05-25T19:00:32.975007Z",
     "shell.execute_reply.started": "2025-05-25T19:00:32.961138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainLoader = DataLoader(merged_ds,\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True,  \n",
    "                        #num_workers=2, \n",
    "                        #pin_memory=True,\n",
    "                        #prefetch_factor=2,\n",
    "                        collate_fn=custom_collate_fn\n",
    "                        # worker_init_fn=worker_init_fn,\n",
    "                        # generator=torch.Generator().manual_seed(seed) \n",
    "                        )\n",
    "\n",
    "\n",
    "testLoader = DataLoader(val_ds, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False,\n",
    "                        collate_fn=custom_collate_fn\n",
    "                        # worker_init_fn=worker_init_fn,\n",
    "                        # generator=torch.Generator().manual_seed(seed) \n",
    "                        )\n",
    "                        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:32.976494Z",
     "iopub.status.busy": "2025-05-25T19:00:32.976290Z",
     "iopub.status.idle": "2025-05-25T19:00:33.377572Z",
     "shell.execute_reply": "2025-05-25T19:00:33.376962Z",
     "shell.execute_reply.started": "2025-05-25T19:00:32.976479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = ResidualAttentionUNet(len(selected_bands), 2).to(device)\n",
    "weight = gen_weights(torch.from_numpy(class_distribution), c = 1.03).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean', weight=weight.to(torch.float32))\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=1e-2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# assuming about 40 reductions => .9 ** 40 = 1e-2, starting from 8e-4 ending with 8e-6\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=5)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T19:00:33.378564Z",
     "iopub.status.busy": "2025-05-25T19:00:33.378320Z",
     "iopub.status.idle": "2025-05-25T20:31:52.467986Z",
     "shell.execute_reply": "2025-05-25T20:31:52.467298Z",
     "shell.execute_reply.started": "2025-05-25T19:00:33.378540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/50: 100%|██████████| 120/120 [02:47<00:00,  1.40s/it, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.11067941821079896\n",
      "{'macroPrec': 0.554694903717085, 'microPrec': 0.7500274633552682, 'weightPrec': 0.9694497617677444, 'macroRec': 0.8514600190305359, 'microRec': 0.7500274633552682, 'weightRec': 0.7500274633552682, 'macroF1': 0.525597743357803, 'microF1': 0.7500274633552682, 'weightF1': 0.8307452155430837, 'subsetAcc': 0.7500274633552682, 'IoU': 0.4263402604468488, 'debris Prec': 0.11119323124773849, 'debris Rec': 0.9599229030971687, 'debris F1': 0.19930038568481478, 'debris IoU': 0.11067941821079896}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/50: 100%|██████████| 120/120 [01:24<00:00,  1.41it/s, loss=0.276] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.32212740086915403\n",
      "{'macroPrec': 0.6629131769089602, 'microPrec': 0.9351541717375149, 'weightPrec': 0.9765053507998842, 'macroRec': 0.9427235153599445, 'microRec': 0.9351541717375149, 'weightRec': 0.9351541717375149, 'macroF1': 0.7263373961690862, 'microF1': 0.9351541717375149, 'weightF1': 0.9498935554713359, 'subsetAcc': 0.9351541717375149, 'IoU': 0.6276099090048817, 'debris Prec': 0.32758581209498294, 'debris Rec': 0.9508174930214011, 'debris F1': 0.4872864758076877, 'debris IoU': 0.32212740086915403}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/50: 100%|██████████| 120/120 [01:25<00:00,  1.41it/s, loss=0.124] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.579630007958074, 'microPrec': 0.8476096110973496, 'weightPrec': 0.9689803204767132, 'macroRec': 0.8718225409608276, 'microRec': 0.8476096110973496, 'weightRec': 0.8476096110973496, 'macroF1': 0.5955811640673743, 'microF1': 0.8476096110973496, 'weightF1': 0.8941446472658943, 'subsetAcc': 0.8476096110973496, 'IoU': 0.501677040105433, 'debris Prec': 0.16329367957831617, 'debris Rec': 0.8977136780539678, 'debris F1': 0.27632413412164236, 'debris IoU': 0.16031096077384133}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/50: 100%|██████████| 120/120 [01:25<00:00,  1.41it/s, loss=0.208] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.3732914960378663\n",
      "{'macroPrec': 0.6906079218600636, 'microPrec': 0.9492347955326276, 'weightPrec': 0.9777416263760098, 'macroRec': 0.9413919190935077, 'microRec': 0.9492347955326276, 'weightRec': 0.9492347955326276, 'macroF1': 0.7583837075219382, 'microF1': 0.9492347955326276, 'weightF1': 0.9592035618738215, 'subsetAcc': 0.9492347955326276, 'IoU': 0.6604717030583919, 'debris Prec': 0.38357287283458114, 'debris Rec': 0.933005449953476, 'debris F1': 0.543644953915266, 'debris IoU': 0.3732914960378663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.6835551785425663, 'microPrec': 0.9472509719873776, 'weightPrec': 0.9761743463088215, 'macroRec': 0.9240506589533961, 'microRec': 0.9472509719873776, 'weightRec': 0.9472509719873776, 'macroF1': 0.7485062965805596, 'microF1': 0.9472509719873776, 'weightF1': 0.957583907540057, 'subsetAcc': 0.9472509719873776, 'IoU': 0.6507711149344303, 'debris Prec': 0.37065446675615704, 'debris Rec': 0.8992423235411405, 'debris F1': 0.5249374381656289, 'debris IoU': 0.35587469423183143}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.219] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.5522335698136743\n",
      "{'macroPrec': 0.7849131734624086, 'microPrec': 0.9752571323949123, 'weightPrec': 0.9841880899500626, 'macroRec': 0.9589821571357741, 'microRec': 0.9752571323949123, 'weightRec': 0.9752571323949123, 'macroF1': 0.8493041667401042, 'microF1': 0.9752571323949123, 'weightF1': 0.9781442630273677, 'subsetAcc': 0.9752571323949123, 'IoU': 0.763355947026245, 'debris Prec': 0.5718264379414733, 'debris Rec': 0.9415791572510966, 'debris F1': 0.711534115165365, 'debris IoU': 0.5522335698136743}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.5887421997707688\n",
      "{'macroPrec': 0.8085207223058575, 'microPrec': 0.9791321579735275, 'weightPrec': 0.9850977321481263, 'macroRec': 0.9514132957268787, 'microRec': 0.9791321579735275, 'weightRec': 0.9791321579735275, 'macroF1': 0.8651351850096808, 'microF1': 0.9791321579735275, 'weightF1': 0.981090913821225, 'subsetAcc': 0.9791321579735275, 'IoU': 0.7836158810286704, 'debris Prec': 0.6197050938337801, 'debris Rec': 0.9217732287651202, 'debris F1': 0.7411425212419174, 'debris IoU': 0.5887421997707688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.297] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.6420919361121932\n",
      "{'macroPrec': 0.8509327466403733, 'microPrec': 0.9841681834336733, 'weightPrec': 0.9864340126059193, 'macroRec': 0.9320788107151021, 'microRec': 0.9841681834336733, 'weightRec': 0.9841681834336733, 'macroF1': 0.8869135805430393, 'microF1': 0.9841681834336733, 'weightF1': 0.9849881768104712, 'subsetAcc': 0.9841681834336733, 'IoU': 0.8128986557823646, 'debris Prec': 0.706039837224245, 'debris Rec': 0.8763791040808189, 'debris F1': 0.7820413973073959, 'debris IoU': 0.6420919361121932}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.6549355019629838\n",
      "{'macroPrec': 0.8429163776078257, 'microPrec': 0.9840971018082735, 'weightPrec': 0.9876418787211828, 'macroRec': 0.9586039598930414, 'microRec': 0.9840971018082735, 'weightRec': 0.9840971018082735, 'macroF1': 0.8916134929235583, 'microF1': 0.9840971018082735, 'weightF1': 0.9852437496689042, 'subsetAcc': 0.9840971018082735, 'IoU': 0.8192688268473596, 'debris Prec': 0.6881598978539508, 'debris Rec': 0.9313438787717666, 'debris F1': 0.7914936880454122, 'debris IoU': 0.6549355019629838}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macroPrec': 0.8456717778667626, 'microPrec': 0.9809723494268596, 'weightPrec': 0.9859181284230046, 'macroRec': 0.9735245224752371, 'microRec': 0.9809723494268596, 'weightRec': 0.9809723494268596, 'macroF1': 0.8983628040544622, 'microF1': 0.9809723494268596, 'weightF1': 0.9824551043465876, 'subsetAcc': 0.9809723494268596, 'IoU': 0.8281274494593381, 'debris Prec': 0.6928529761421283, 'debris Rec': 0.9654090315146152, 'debris F1': 0.80673200928553, 'debris IoU': 0.6760694291334328}\n",
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.6864666864666865\n",
      "{'macroPrec': 0.8631640550529729, 'microPrec': 0.9863566359005288, 'weightPrec': 0.988656270053002, 'macroRec': 0.9551144062242657, 'microRec': 0.9863566359005288, 'weightRec': 0.9863566359005288, 'macroF1': 0.9035035575811154, 'microF1': 0.9863566359005288, 'weightF1': 0.9871227935897895, 'subsetAcc': 0.9863566359005288, 'IoU': 0.8362016127437866, 'debris Prec': 0.7289739276703112, 'debris Rec': 0.921706765917852, 'debris F1': 0.814088641033167, 'debris IoU': 0.6864666864666865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8662601877736464, 'microPrec': 0.9861390830470323, 'weightPrec': 0.987924125258783, 'macroRec': 0.9406450812620706, 'microRec': 0.9861390830470323, 'weightRec': 0.9861390830470323, 'macroF1': 0.8997175601853843, 'microF1': 0.9861390830470323, 'weightF1': 0.986777752655027, 'subsetAcc': 0.9861390830470323, 'IoU': 0.8308215265778871, 'debris Prec': 0.7361636772530306, 'debris Rec': 0.8919978731888875, 'debris F1': 0.8066231931965022, 'debris IoU': 0.675916599516519}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 12/50: 100%|██████████| 120/120 [01:24<00:00,  1.41it/s, loss=0.117] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.6983850064799123\n",
      "{'macroPrec': 0.8670249869464892, 'microPrec': 0.9869662146880486, 'weightPrec': 0.9892068759142735, 'macroRec': 0.9600223292730882, 'microRec': 0.9869662146880486, 'weightRec': 0.9869662146880486, 'macroF1': 0.9078227784752635, 'microF1': 0.9869662146880486, 'weightF1': 0.987698625666572, 'subsetAcc': 0.9869662146880486, 'IoU': 0.8424728134717416, 'debris Prec': 0.7363746255321385, 'debris Rec': 0.9312109530772298, 'debris F1': 0.8224107064244416, 'debris IoU': 0.6983850064799123}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 13/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.858648520956027, 'microPrec': 0.9853291833152039, 'weightPrec': 0.9874612120654951, 'macroRec': 0.9407404615266437, 'microRec': 0.9853291833152039, 'weightRec': 0.9853291833152039, 'macroF1': 0.8950958161656861, 'microF1': 0.9853291833152039, 'weightF1': 0.9860821584137203, 'subsetAcc': 0.9853291833152039, 'IoU': 0.8242565029895812, 'debris Prec': 0.7209077740222115, 'debris Rec': 0.8930612787451815, 'debris F1': 0.797803176488051, 'debris IoU': 0.6636210983800869}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 14/50: 100%|██████████| 120/120 [01:26<00:00,  1.38it/s, loss=0.0775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.7889065163356681, 'microPrec': 0.976032568308365, 'weightPrec': 0.9846709242406861, 'macroRec': 0.9627552888647142, 'microRec': 0.976032568308365, 'weightRec': 0.976032568308365, 'macroF1': 0.8534997291198279, 'microF1': 0.976032568308365, 'weightF1': 0.9787970202580074, 'subsetAcc': 0.976032568308365, 'IoU': 0.7685921427371303, 'debris Prec': 0.5795736040609137, 'debris Rec': 0.9485577562142762, 'debris F1': 0.7195180358448238, 'debris IoU': 0.561911886294736}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 15/50: 100%|██████████| 120/120 [01:26<00:00,  1.38it/s, loss=0.152] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.8587657708789941, 'microPrec': 0.9860981572627112, 'weightPrec': 0.9887972440956013, 'macroRec': 0.9615329827313597, 'microRec': 0.9860981572627112, 'weightRec': 0.9860981572627112, 'macroF1': 0.9031187376112499, 'microF1': 0.9860981572627112, 'weightF1': 0.9869683985597213, 'subsetAcc': 0.9860981572627112, 'IoU': 0.8356165647464721, 'debris Prec': 0.719721767594108, 'debris Rec': 0.9352651867606008, 'debris F1': 0.8134574252846986, 'debris IoU': 0.6855695215823833}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7049122453096631\n",
      "{'macroPrec': 0.8713413939278869, 'microPrec': 0.9873970124177446, 'weightPrec': 0.9894165912752275, 'macroRec': 0.9591529186965061, 'microRec': 0.9873970124177446, 'weightRec': 0.9873970124177446, 'macroF1': 0.9101897545674851, 'microF1': 0.9873970124177446, 'weightF1': 0.9880629911571985, 'subsetAcc': 0.9873970124177446, 'IoU': 0.8459590253060826, 'debris Prec': 0.7450823604669758, 'debris Rec': 0.928951216270105, 'debris F1': 0.8269190948084603, 'debris IoU': 0.7049122453096631}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 17/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8771182814089121, 'microPrec': 0.9872031534393814, 'weightPrec': 0.9885381501041189, 'macroRec': 0.9397496106546962, 'microRec': 0.9872031534393814, 'weightRec': 0.9872031534393814, 'macroF1': 0.9058236037267092, 'microF1': 0.9872031534393814, 'weightF1': 0.987693623427014, 'subsetAcc': 0.9872031534393814, 'IoU': 0.8396352915967498, 'debris Prec': 0.7579758599195331, 'debris Rec': 0.8890070450618105, 'debris F1': 0.8182791423240449, 'debris IoU': 0.6924470673500026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 18/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.8602820990959804, 'microPrec': 0.986447103423765, 'weightPrec': 0.9891978573678613, 'macroRec': 0.9663383429521428, 'microRec': 0.986447103423765, 'weightRec': 0.986447103423765, 'macroF1': 0.9058799805071617, 'microF1': 0.986447103423765, 'weightF1': 0.9873159275909372, 'subsetAcc': 0.986447103423765, 'IoU': 0.8396059448984676, 'debris Prec': 0.7224311413761562, 'debris Rec': 0.9448358367672471, 'debris F1': 0.8187996774565143, 'debris IoU': 0.6931929003315779}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 19/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7084440277126635\n",
      "{'macroPrec': 0.8814577633338037, 'microPrec': 0.9879441255344584, 'weightPrec': 0.9892877691229711, 'macroRec': 0.9473270176651545, 'microRec': 0.9879441255344584, 'weightRec': 0.9879441255344584, 'macroF1': 0.9115477470115358, 'microF1': 0.9879441255344584, 'weightF1': 0.9884230880381193, 'subsetAcc': 0.9879441255344584, 'IoU': 0.8480121637492654, 'debris Prec': 0.7661540194918596, 'debris Rec': 0.9038947228499269, 'debris F1': 0.8293441473305485, 'debris IoU': 0.7084440277126635}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 20/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.154] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macroPrec': 0.8632387991914148, 'microPrec': 0.9838460926403394, 'weightPrec': 0.9875585081391406, 'macroRec': 0.9773174841775037, 'microRec': 0.9838460926403394, 'weightRec': 0.9838460926403394, 'macroF1': 0.9115992414541709, 'microF1': 0.9838460926403394, 'weightF1': 0.984941154954656, 'subsetAcc': 0.9838460926403394, 'IoU': 0.8475190553960688, 'debris Prec': 0.7277743904792041, 'debris Rec': 0.970203615486712, 'debris F1': 0.8316825570421852, 'debris IoU': 0.7118635111161439}\n",
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8651393036583505, 'microPrec': 0.986811127505358, 'weightPrec': 0.9891784136757695, 'macroRec': 0.9613875144231365, 'microRec': 0.986811127505358, 'weightRec': 0.986811127505358, 'macroF1': 0.9071505304316048, 'microF1': 0.986811127505358, 'weightF1': 0.9875786123983317, 'subsetAcc': 0.986811127505358, 'IoU': 0.8414825422504371, 'debris Prec': 0.7325029965084163, 'debris Rec': 0.9342017812043067, 'debris F1': 0.8211479480064262, 'debris IoU': 0.6965657366569206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21/50: 100%|██████████| 120/120 [01:26<00:00,  1.40it/s, loss=0.0625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.7727574617557335, 'microPrec': 0.972876974938342, 'weightPrec': 0.983231921006032, 'macroRec': 0.9552791039854469, 'microRec': 0.972876974938342, 'weightRec': 0.972876974938342, 'macroF1': 0.838488117602598, 'microF1': 0.972876974938342, 'weightF1': 0.976266185358467, 'subsetAcc': 0.972876974938342, 'IoU': 0.7500496965669218, 'debris Prec': 0.547694938972246, 'debris Rec': 0.9364615180114316, 'debris F1': 0.6911606004120475, 'debris IoU': 0.528071358968593}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 22/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.8694732374254053, 'microPrec': 0.9866064985837525, 'weightPrec': 0.9883400985558318, 'macroRec': 0.944066334182982, 'microRec': 0.9866064985837525, 'weightRec': 0.9866064985837525, 'macroF1': 0.9030459671248349, 'microF1': 0.9866064985837525, 'weightF1': 0.9872203140919786, 'subsetAcc': 0.9866064985837525, 'IoU': 0.8355889513492898, 'debris Prec': 0.7423676696683505, 'debris Rec': 0.8985776950684567, 'debris F1': 0.8130374646701546, 'debris IoU': 0.6849731482419698}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 23/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 4/10 epochs\n",
      "{'macroPrec': 0.8420654798945764, 'microPrec': 0.9836339942488503, 'weightPrec': 0.9868650815144471, 'macroRec': 0.9473159358880108, 'microRec': 0.9836339942488503, 'weightRec': 0.9836339942488503, 'macroF1': 0.8870068343105513, 'microF1': 0.9836339942488503, 'weightF1': 0.9847242370816649, 'subsetAcc': 0.9836339942488503, 'IoU': 0.8129351933923591, 'debris Prec': 0.6872297637003519, 'debris Rec': 0.908480659311445, 'debris F1': 0.7825166017861231, 'debris IoU': 0.642732872525509}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 24/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 5/10 epochs\n",
      "{'macroPrec': 0.881181325680209, 'microPrec': 0.9878321181247375, 'weightPrec': 0.9891425840828733, 'macroRec': 0.9453099184321991, 'microRec': 0.9878321181247375, 'weightRec': 0.9878321181247375, 'macroF1': 0.9105420067327543, 'microF1': 0.9878321181247375, 'weightF1': 0.9883040676703472, 'subsetAcc': 0.9878321181247375, 'IoU': 0.8465319284825117, 'debris Prec': 0.7657372320570104, 'debris Rec': 0.8998404891665559, 'debris F1': 0.8273902282519021, 'debris IoU': 0.7055972482801751}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 25/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 6/10 epochs\n",
      "{'macroPrec': 0.8757017852349891, 'microPrec': 0.9876684149874531, 'weightPrec': 0.9893930452260995, 'macroRec': 0.9545717668870908, 'microRec': 0.9876684149874531, 'weightRec': 0.9876684149874531, 'macroF1': 0.9110600228631797, 'microF1': 0.9876684149874531, 'weightF1': 0.988253908141456, 'subsetAcc': 0.9876684149874531, 'IoU': 0.847262891447716, 'debris Prec': 0.7541305414690005, 'debris Rec': 0.9191811777216536, 'debris F1': 0.8285158005092108, 'debris IoU': 0.7072360010227563}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 7/10 epochs\n",
      "{'macroPrec': 0.8653815051000795, 'microPrec': 0.9871622276550602, 'weightPrec': 0.989741294841419, 'macroRec': 0.9699518353632662, 'microRec': 0.9871622276550602, 'weightRec': 0.9871622276550602, 'macroF1': 0.9105242478706912, 'microF1': 0.9871622276550602, 'weightF1': 0.9879652425551542, 'subsetAcc': 0.9871622276550602, 'IoU': 0.8464123241513047, 'debris Prec': 0.732402291794557, 'debris Rec': 0.9515485843413531, 'debris F1': 0.8277157888651211, 'debris IoU': 0.7060709177886275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 27/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7214065708418891\n",
      "{'macroPrec': 0.8789640844936204, 'microPrec': 0.9883103036047, 'weightPrec': 0.9900714881177545, 'macroRec': 0.9620658544302649, 'microRec': 0.9883103036047, 'weightRec': 0.9883103036047, 'macroF1': 0.9160477910312734, 'microF1': 0.9883103036047, 'weightF1': 0.9888876024326334, 'subsetAcc': 0.9883103036047, 'IoU': 0.8546759910064372, 'debris Prec': 0.760155785146319, 'debris Rec': 0.9340023926625016, 'debris F1': 0.8381594250439866, 'debris IoU': 0.7214065708418891}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 28/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7250040251167283\n",
      "{'macroPrec': 0.8933737912298332, 'microPrec': 0.9889629621651894, 'weightPrec': 0.989874260279268, 'macroRec': 0.9449307282615241, 'microRec': 0.9889629621651894, 'weightRec': 0.9889629621651894, 'macroF1': 0.917433006669562, 'microF1': 0.9889629621651894, 'weightF1': 0.9893023095321188, 'subsetAcc': 0.9889629621651894, 'IoU': 0.8568181018858346, 'debris Prec': 0.7901848385587272, 'debris Rec': 0.8978466037485046, 'debris F1': 0.8405824155310808, 'debris IoU': 0.7250040251167283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 29/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7335310965630114\n",
      "{'macroPrec': 0.8796744823651361, 'microPrec': 0.9887777191414201, 'weightPrec': 0.990722634176496, 'macroRec': 0.9715895949907053, 'microRec': 0.9887777191414201, 'weightRec': 0.9887777191414201, 'macroF1': 0.9202308752667265, 'microF1': 0.9887777191414201, 'weightF1': 0.9893832923704339, 'subsetAcc': 0.9887777191414201, 'IoU': 0.860975539991014, 'debris Prec': 0.7609295415959253, 'debris Rec': 0.9532101555230627, 'debris F1': 0.8462854782557385, 'debris IoU': 0.7335310965630114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 30/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macroPrec': 0.8724564934411042, 'microPrec': 0.9853401005259923, 'weightPrec': 0.9885496549342913, 'macroRec': 0.9808735065045471, 'microRec': 0.9853401005259923, 'weightRec': 0.9853401005259923, 'macroF1': 0.9189594414329239, 'microF1': 0.9853401005259923, 'weightF1': 0.9862705544682896, 'subsetAcc': 0.9853401005259923, 'IoU': 0.8586249724763546, 'debris Prec': 0.7459561049643383, 'debris Rec': 0.976006502525902, 'debris F1': 0.8456141914779818, 'debris IoU': 0.7325230310658769}\n",
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8607383491147653, 'microPrec': 0.986806819528061, 'weightPrec': 0.9897649971115021, 'macroRec': 0.9747465243828317, 'microRec': 0.986806819528061, 'weightRec': 0.986806819528061, 'macroF1': 0.9092444020775303, 'microF1': 0.986806819528061, 'weightF1': 0.9877062620812928, 'subsetAcc': 0.986806819528061, 'IoU': 0.8445044591241199, 'debris Prec': 0.722768815861759, 'debris Rec': 0.9618503256679516, 'debris F1': 0.8253443211953577, 'debris IoU': 0.702626596106229}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.8743773154644945, 'microPrec': 0.9881164446263369, 'weightPrec': 0.9902379802830077, 'macroRec': 0.9690959533744143, 'microRec': 0.9881164446263369, 'weightRec': 0.9881164446263369, 'macroF1': 0.9159431688365186, 'microF1': 0.9881164446263369, 'weightF1': 0.9887833362441051, 'subsetAcc': 0.9881164446263369, 'IoU': 0.8544952327980947, 'debris Prec': 0.7504863046106934, 'debris Rec': 0.9487571447560813, 'debris F1': 0.8380544221680805, 'debris IoU': 0.7212510105092966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 32/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.115] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.8704316497316332, 'microPrec': 0.9877567285220407, 'weightPrec': 0.9901051981815776, 'macroRec': 0.9708050540829445, 'microRec': 0.9877567285220407, 'weightRec': 0.9877567285220407, 'macroF1': 0.9140898721378221, 'microF1': 0.9877567285220407, 'weightF1': 0.9884867090250768, 'subsetAcc': 0.9877567285220407, 'IoU': 0.8517111105213999, 'debris Prec': 0.7424634828550709, 'debris Rec': 0.9526784527449156, 'debris F1': 0.8345365626455519, 'debris IoU': 0.7160555500049955}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 33/50: 100%|██████████| 120/120 [01:24<00:00,  1.43it/s, loss=0.0689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7377049180327869\n",
      "{'macroPrec': 0.8825735555274734, 'microPrec': 0.9890405057565347, 'weightPrec': 0.9908440032398671, 'macroRec': 0.9706976018504581, 'microRec': 0.9890405057565347, 'weightRec': 0.9890405057565347, 'macroF1': 0.9216852148127805, 'microF1': 0.9890405057565347, 'weightF1': 0.9896061971492343, 'subsetAcc': 0.9890405057565347, 'IoU': 0.8631984346356358, 'debris Prec': 0.7667988425677849, 'debris Rec': 0.9510833444104746, 'debris F1': 0.8490566037735849, 'debris IoU': 0.7377049180327869}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 34/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7531601685423223\n",
      "{'macroPrec': 0.8950872103036729, 'microPrec': 0.9900313405348354, 'weightPrec': 0.9912620940866074, 'macroRec': 0.965139243679794, 'microRec': 0.9900313405348354, 'weightRec': 0.9900313405348354, 'macroF1': 0.9270178329478694, 'microF1': 0.9900313405348354, 'weightF1': 0.9904371300826328, 'subsetAcc': 0.9900313405348354, 'IoU': 0.8714393928326117, 'debris Prec': 0.7922464093357271, 'debris Rec': 0.9385218662767513, 'debris F1': 0.8592029205962883, 'debris IoU': 0.7531601685423223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 35/50: 100%|██████████| 120/120 [01:26<00:00,  1.39it/s, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8833610933007141, 'microPrec': 0.9891589751322011, 'weightPrec': 0.9909496269007625, 'macroRec': 0.9715938978264104, 'microRec': 0.9891589751322011, 'weightRec': 0.9891589751322011, 'macroF1': 0.9225251424306855, 'microF1': 0.9891589751322011, 'weightF1': 0.9897181390795953, 'subsetAcc': 0.9891589751322011, 'IoU': 0.864482665467103, 'debris Prec': 0.7683155581756793, 'debris Rec': 0.9528113784394523, 'debris F1': 0.85067497403946, 'debris IoU': 0.7401517889410915}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 36/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7640192013964652\n",
      "{'macroPrec': 0.9038215568833708, 'microPrec': 0.9906818451066763, 'weightPrec': 0.9915899269057716, 'macroRec': 0.9617817791663529, 'microRec': 0.9906818451066763, 'weightRec': 0.9906818451066763, 'macroF1': 0.9306991474387325, 'microF1': 0.9906818451066763, 'weightF1': 0.9909937595230159, 'subsetAcc': 0.9906818451066763, 'IoU': 0.8772055922495742, 'debris Prec': 0.8099699282905389, 'debris Rec': 0.930878638840888, 'debris F1': 0.8662254932277815, 'debris IoU': 0.7640192013964652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 37/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8957309499751664, 'microPrec': 0.9899301030683568, 'weightPrec': 0.9910742321165639, 'macroRec': 0.9615539103411433, 'microRec': 0.9899301030683568, 'weightRec': 0.9899301030683568, 'macroF1': 0.9259008993281828, 'microF1': 0.9899301030683568, 'weightF1': 0.9903166005397122, 'subsetAcc': 0.9899301030683568, 'IoU': 0.8697147175701531, 'debris Prec': 0.7937793892697298, 'debris Rec': 0.9312109530772298, 'debris F1': 0.8570205217604061, 'debris IoU': 0.7498126939955047}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 38/50: 100%|██████████| 120/120 [01:26<00:00,  1.40it/s, loss=0.0169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.8799498677074367, 'microPrec': 0.9888057209938503, 'weightPrec': 0.9907384342911725, 'macroRec': 0.9715719465139845, 'microRec': 0.9888057209938503, 'weightRec': 0.9888057209938503, 'macroF1': 0.9203960202009596, 'microF1': 0.9888057209938503, 'weightF1': 0.9894076772760377, 'subsetAcc': 0.9888057209938503, 'IoU': 0.861227215628934, 'debris Prec': 0.7614825041151171, 'debris Rec': 0.9531436926757942, 'debris F1': 0.8466011393488592, 'debris IoU': 0.7340055276896305}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 39/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.8881983248865344, 'microPrec': 0.9895617710094667, 'weightPrec': 0.9911216353527379, 'macroRec': 0.9696179932666359, 'microRec': 0.9895617710094667, 'weightRec': 0.9895617710094667, 'macroF1': 0.9247090972386096, 'microF1': 0.9895617710094667, 'weightF1': 0.9900569640169333, 'subsetAcc': 0.9895617710094667, 'IoU': 0.867849678539735, 'debris Prec': 0.7781413612565445, 'debris Rec': 0.9482919048252028, 'debris F1': 0.8548319453597747, 'debris IoU': 0.7464685570785812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 40/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macroPrec': 0.8780982019016205, 'microPrec': 0.9861827548320363, 'weightPrec': 0.9890797376034597, 'macroRec': 0.9820957783029062, 'microRec': 0.9861827548320363, 'weightRec': 0.9861827548320363, 'macroF1': 0.9230724191674662, 'microF1': 0.9861827548320364, 'weightF1': 0.9870173411263389, 'subsetAcc': 0.9861827548320363, 'IoU': 0.8649419230229423, 'debris Prec': 0.7571676733549014, 'debris Rec': 0.9776424228082269, 'debris F1': 0.853395124097949, 'debris IoU': 0.7442800410442793}\n",
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 4/10 epochs\n",
      "{'macroPrec': 0.9075192948555281, 'microPrec': 0.9907529267320762, 'weightPrec': 0.991478104920321, 'macroRec': 0.9563905082347819, 'microRec': 0.9907529267320762, 'weightRec': 0.9907529267320762, 'macroF1': 0.9304584328996433, 'microF1': 0.9907529267320762, 'weightF1': 0.9910144472367761, 'subsetAcc': 0.9907529267320762, 'IoU': 0.8768395011503158, 'debris Prec': 0.8177412682465576, 'debris Rec': 0.9196464176525322, 'debris F1': 0.8657052585478775, 'debris IoU': 0.7632101489244346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41/50: 100%|██████████| 120/120 [01:24<00:00,  1.43it/s, loss=0.0875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 5/10 epochs\n",
      "{'macroPrec': 0.8952839604947613, 'microPrec': 0.9902251995131985, 'weightPrec': 0.9915214799744941, 'macroRec': 0.9690293857748016, 'microRec': 0.9902251995131985, 'weightRec': 0.9902251995131985, 'macroF1': 0.928741948105257, 'microF1': 0.9902251995131985, 'weightF1': 0.9906420844125626, 'subsetAcc': 0.9902251995131985, 'IoU': 0.874118632344659, 'debris Prec': 0.7923761825264329, 'debris Rec': 0.9463644822544198, 'debris F1': 0.8625514901865763, 'debris IoU': 0.7583213505884859}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 42/50: 100%|██████████| 120/120 [01:23<00:00,  1.43it/s, loss=0.0939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7724853207485046\n",
      "{'macroPrec': 0.9068505018323982, 'microPrec': 0.9910695630634027, 'weightPrec': 0.991934982425018, 'macroRec': 0.964262534616777, 'microRec': 0.9910695630634027, 'weightRec': 0.9910695630634027, 'macroF1': 0.9335073594918097, 'microF1': 0.9910695630634027, 'weightF1': 0.9913638005313936, 'subsetAcc': 0.9910695630634027, 'IoU': 0.8816378151796849, 'debris Prec': 0.8158687840500753, 'debris Rec': 0.9355975009969427, 'debris F1': 0.871640866873065, 'debris IoU': 0.7724853207485046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 43/50: 100%|██████████| 120/120 [01:24<00:00,  1.43it/s, loss=0.041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.9004004328728443, 'microPrec': 0.9908627801531485, 'weightPrec': 0.9920525436669336, 'macroRec': 0.9724100976150197, 'microRec': 0.9908627801531485, 'weightRec': 0.9908627801531485, 'macroF1': 0.9331832593499751, 'microF1': 0.9908627801531485, 'weightF1': 0.9912396035534972, 'subsetAcc': 0.9908627801531485, 'IoU': 0.8811062525062181, 'debris Prec': 0.8023958799820868, 'debris Rec': 0.9526784527449156, 'debris F1': 0.8711030082041933, 'debris IoU': 0.771640826873385}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 44/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0625] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.8620085478142754, 'microPrec': 0.987026526370206, 'weightPrec': 0.9899672937005716, 'macroRec': 0.9766908035736306, 'microRec': 0.987026526370206, 'weightRec': 0.987026526370206, 'macroF1': 0.9107850897172387, 'microF1': 0.987026526370206, 'weightF1': 0.9879128068239652, 'subsetAcc': 0.987026526370206, 'IoU': 0.8467729492103169, 'debris Prec': 0.7251809333666084, 'debris Rec': 0.9656387079622492, 'debris F1': 0.828311621675551, 'debris IoU': 0.7069384974698326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 45/50: 100%|██████████| 120/120 [01:24<00:00,  1.42it/s, loss=0.0642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.8969738770283824, 'microPrec': 0.9903716707412952, 'weightPrec': 0.9916062246226109, 'macroRec': 0.9687196541021247, 'microRec': 0.9903716707412952, 'weightRec': 0.9903716707412952, 'macroF1': 0.9296208165289639, 'microF1': 0.9903716707412952, 'weightF1': 0.9907705440407552, 'subsetAcc': 0.9903716707412952, 'IoU': 0.8754957485713919, 'debris Prec': 0.7957825260096207, 'debris Rec': 0.9455669280871992, 'debris F1': 0.8642327785202283, 'debris IoU': 0.7609242124404985}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 46/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "Saved best model with validation metric: 0.7822445561139029\n",
      "{'macroPrec': 0.913977861948952, 'microPrec': 0.9915994442709287, 'weightPrec': 0.9922586310088195, 'macroRec': 0.9623844194035636, 'microRec': 0.9915994442709287, 'weightRec': 0.9915994442709287, 'macroF1': 0.9367348672878955, 'microF1': 0.9915994442709287, 'weightF1': 0.9918314228216473, 'subsetAcc': 0.9915994442709287, 'IoU': 0.8867913026990908, 'debris Prec': 0.8302714234917625, 'debris Rec': 0.9311444902299615, 'debris F1': 0.8778195488721805, 'debris IoU': 0.7822445561139029}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 47/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 1/10 epochs\n",
      "{'macroPrec': 0.8920227921590127, 'microPrec': 0.9900916522169928, 'weightPrec': 0.9915793958635847, 'macroRec': 0.9731036435050082, 'microRec': 0.9900916522169928, 'weightRec': 0.9900916522169928, 'macroF1': 0.9284361866689637, 'microF1': 0.9900916522169928, 'weightF1': 0.9905558005377626, 'subsetAcc': 0.9900916522169928, 'IoU': 0.8736307514614778, 'debris Prec': 0.7855658829961728, 'debris Rec': 0.9549381895520405, 'debris F1': 0.8620110391168707, 'debris IoU': 0.7574862927035007}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 48/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 2/10 epochs\n",
      "{'macroPrec': 0.9114801763291949, 'microPrec': 0.9912978858601416, 'weightPrec': 0.9919899356706094, 'macroRec': 0.9605905537785565, 'microRec': 0.9912978858601416, 'weightRec': 0.9912978858601416, 'macroF1': 0.9345389633220893, 'microF1': 0.9912978858601416, 'weightF1': 0.9915428844362503, 'subsetAcc': 0.9912978858601416, 'IoU': 0.8832856047250235, 'debris Prec': 0.8253902554399243, 'debris Rec': 0.9277548850192743, 'debris F1': 0.8735840791038239, 'debris IoU': 0.7755430857269848}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 49/50: 100%|██████████| 120/120 [01:25<00:00,  1.40it/s, loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 3/10 epochs\n",
      "{'macroPrec': 0.903135654382996, 'microPrec': 0.9910587931201602, 'weightPrec': 0.9921424644556822, 'macroRec': 0.9711624158180606, 'microRec': 0.9910587931201602, 'weightRec': 0.9910587931201602, 'macroF1': 0.934279926534437, 'microF1': 0.9910587931201602, 'weightF1': 0.9914065557832722, 'subsetAcc': 0.9910587931201602, 'IoU': 0.8828514770316065, 'debris Prec': 0.8079597489965515, 'debris Rec': 0.9498870131596437, 'debris F1': 0.873193829234764, 'debris IoU': 0.7749281570243453}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 50/50: 100%|██████████| 120/120 [01:26<00:00,  1.38it/s, loss=0.00916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'macroPrec': 0.8857288565050285, 'microPrec': 0.9872454896198194, 'weightPrec': 0.9897289613961839, 'macroRec': 0.982896126741839, 'microRec': 0.9872454896198194, 'weightRec': 0.9872454896198194, 'macroF1': 0.9282503790903758, 'microF1': 0.9872454896198194, 'weightF1': 0.9879583886924932, 'subsetAcc': 0.9872454896198194, 'IoU': 0.8730097176301508, 'debris Prec': 0.7724056124728842, 'debris Rec': 0.9781568631485806, 'debris F1': 0.8631898164121375, 'debris IoU': 0.7593086593294252}\n",
      "########### Validation Set Evaluation : #############\n",
      "No improvement for 4/10 epochs\n",
      "{'macroPrec': 0.8782841985632883, 'microPrec': 0.9888789566078987, 'weightPrec': 0.991022252788304, 'macroRec': 0.9776159233851675, 'microRec': 0.9888789566078987, 'weightRec': 0.9888789566078987, 'macroF1': 0.9216728109931687, 'microF1': 0.9888789566078987, 'weightF1': 0.9895239253150581, 'subsetAcc': 0.9888789566078987, 'IoU': 0.8631593482084969, 'debris Prec': 0.7577322276117456, 'debris Rec': 0.9655722451149807, 'debris F1': 0.8491189105467722, 'debris IoU': 0.7377989944644762}\n"
     ]
    }
   ],
   "source": [
    "best_metric = -float('inf')  # Initialize to negative infinity (for maximization, e.g., accuracy)\n",
    "best_model_path = '/kaggle/working/best_model.pth'\n",
    "output_classes = 2\n",
    "metrics_history = []\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "epochs_no_improve = 0  # Counter for epochs without improvement\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    pb = tqdm(trainLoader, desc=f\"epoch {epoch}/{epochs}: \")\n",
    "    yTrue = []\n",
    "    yPredicted = []\n",
    "\n",
    "    bg_yTrue = []\n",
    "    bg_yPredicted = []\n",
    "    for image, target, _ in pb:\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(image)\n",
    "        # print(f'logits shape : {logits.shape}')\n",
    "        # print(f'target shape : {target.shape}')\n",
    "        # print(f'image dtype {image.dtype}')\n",
    "        # print(f'logits dtype {logits.dtype}')\n",
    "        # print(f'target dtype {target.dtype}')\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pb.set_postfix(loss=loss.item())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                logits = logits.detach()\n",
    "                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "                logits = logits.reshape((-1,output_classes))\n",
    "                target = target.reshape(-1)\n",
    "                ###################################################################################\n",
    "                mask = target != -1\n",
    "                ###################################################################################\n",
    "                \n",
    "                # bg_logits = logits[~mask]\n",
    "                # bg_target = target[~mask]\n",
    "    \n",
    "                # only considering annotated pixels\n",
    "                logits = logits[mask]\n",
    "                target = target[mask]\n",
    "    \n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "                target = target.cpu().numpy()\n",
    "                yPredicted += probs.argmax(1).tolist()\n",
    "                yTrue += target.tolist()\n",
    "        \n",
    "                \n",
    "                # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n",
    "                # bg_target = bg_target.cpu().numpy()\n",
    "                \n",
    "                # bg_yPredicted += bg_probs.argmax(1).tolist()\n",
    "                # bg_yTrue += bg_target.tolist()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        yPredicted = np.asarray(yPredicted)\n",
    "        yTrue = np.asarray(yTrue)\n",
    "        acc = Evaluation(yPredicted, yTrue)\n",
    "        print(acc)\n",
    "    \n",
    "        # bg_yPredicted = np.asarray(bg_yPredicted)\n",
    "        # bg_yTrue = np.asarray(bg_yTrue)\n",
    "        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n",
    "        # print(\"background:\", bg_acc)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    yTrue = []\n",
    "    yPredicted = []\n",
    "    testLossF = []\n",
    "    valPrecHistory = []\n",
    "    # bg_yTrue = []\n",
    "    # bg_yPredicted = []\n",
    "    iters = len(testLoader)\n",
    "    with torch.no_grad():\n",
    "        for i, (image, target, _) in enumerate(testLoader):\n",
    "\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            logits = model(image)\n",
    "            # print(f'image dtype {image.dtype}')\n",
    "            # print(f'logits dtype {logits.dtype}')\n",
    "            # print(f'target dtype {target.dtype}')\n",
    "            # print(f'test - target shape {target.shape}')\n",
    "            # print(f'test - logit shape {logits.shape}')\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "            logits = logits.reshape((-1,output_classes))\n",
    "            target = target.reshape(-1)\n",
    "            ###################################################################################\n",
    "            mask = target != -1\n",
    "            ###################################################################################\n",
    "            \n",
    "            # bg_logits = logits[~mask]\n",
    "            # bg_target = target[~mask]\n",
    "            \n",
    "            logits = logits[mask]\n",
    "            target = target[mask]\n",
    "            \n",
    "\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            # testBatches += target.shape[0]\n",
    "            testLossF.append((loss.data*target.shape[0]).tolist())\n",
    "            yPredicted += probs.argmax(1).tolist()\n",
    "            yTrue += target.tolist()\n",
    "\n",
    "            #scheduler.step(epoch + i/iters)\n",
    "            # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n",
    "            # bg_target = bg_target.cpu().numpy()\n",
    "\n",
    "            # bg_yPredicted += bg_probs.argmax(1).tolist()\n",
    "            # bg_yTrue += bg_target.tolist()\n",
    "        \n",
    "        yPredicted = np.asarray(yPredicted)\n",
    "        yTrue = np.asarray(yTrue)\n",
    "        print('########### Validation Set Evaluation : #############')\n",
    "        acc = Evaluation(yPredicted, yTrue)\n",
    "        metrics_history.append(acc)\n",
    "        if acc['debris IoU'] > best_metric:\n",
    "            best_metric = acc['debris IoU']\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved best model with validation metric: {best_metric}\")\n",
    "            epochs_no_improve = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve}/{patience} epochs\")\n",
    "        # bg_yPredicted = np.asarray(bg_yPredicted)\n",
    "        # bg_yTrue = np.asarray(bg_yTrue)\n",
    "        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n",
    "        print(acc)\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "        # print(\"background:\", bg_acc)\n",
    "    #scheduler.step(sum(testLossF) / len(testLoader.dataset))\n",
    "    scheduler.step(acc['debris Prec'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:31:52.469092Z",
     "iopub.status.busy": "2025-05-25T20:31:52.468860Z",
     "iopub.status.idle": "2025-05-25T20:31:52.473024Z",
     "shell.execute_reply": "2025-05-25T20:31:52.472416Z",
     "shell.execute_reply.started": "2025-05-25T20:31:52.469075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Save everything in a checkpoint\n",
    "# checkpoint = {\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict(),\n",
    "#     'epoch': 10  # Optional: Save the epoch number\n",
    "# }\n",
    "\n",
    "# torch.save(checkpoint, 'model_checkpoint_8_epochs_bs16_iou075.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:31:52.474070Z",
     "iopub.status.busy": "2025-05-25T20:31:52.473873Z",
     "iopub.status.idle": "2025-05-25T20:31:52.487859Z",
     "shell.execute_reply": "2025-05-25T20:31:52.487098Z",
     "shell.execute_reply.started": "2025-05-25T20:31:52.474055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:44:05.689893Z",
     "iopub.status.busy": "2025-05-25T20:44:05.689420Z",
     "iopub.status.idle": "2025-05-25T20:44:19.227238Z",
     "shell.execute_reply": "2025-05-25T20:44:19.226426Z",
     "shell.execute_reply.started": "2025-05-25T20:44:05.689869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test - probs shape (13343, 2)\n",
      "test - probs shape (3144, 2)\n",
      "test - probs shape (11192, 2)\n",
      "test - probs shape (4456, 2)\n",
      "test - probs shape (4984, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1683949948.py:4: RuntimeWarning: invalid value encountered in less\n",
      "  invalid_mask |= image < -1.5\n",
      "/tmp/ipykernel_35/1683949948.py:5: RuntimeWarning: invalid value encountered in greater\n",
      "  invalid_mask |= image > 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test - probs shape (589, 2)\n",
      "test - probs shape (6638, 2)\n",
      "test - probs shape (1737, 2)\n",
      "test - probs shape (19586, 2)\n",
      "test - probs shape (12855, 2)\n",
      "test - probs shape (2904, 2)\n",
      "test - probs shape (5307, 2)\n",
      "test - probs shape (3079, 2)\n",
      "test - probs shape (14738, 2)\n",
      "test - probs shape (5915, 2)\n",
      "test - probs shape (19936, 2)\n",
      "test - probs shape (2145, 2)\n",
      "test - probs shape (18336, 2)\n",
      "test - probs shape (24205, 2)\n",
      "test - probs shape (5307, 2)\n",
      "test - probs shape (12354, 2)\n",
      "test - probs shape (1777, 2)\n",
      "test - probs shape (336, 2)\n",
      "{'macroPrec': 0.9432830944027024, 'microPrec': 0.9966591913292929, 'weightPrec': 0.996584548472167, 'macroRec': 0.9195873148561116, 'microRec': 0.9966591913292929, 'weightRec': 0.9966591913292929, 'macroF1': 0.9311008973712369, 'microF1': 0.9966591913292929, 'weightF1': 0.9966133198378344, 'subsetAcc': 0.9966591913292929, 'IoU': 0.8785104443412077, 'debris Prec': 0.8886021505376344, 'debris Rec': 0.8405207485760781, 'debris F1': 0.8638929542128371, 'debris IoU': 0.7603974972396025}\n"
     ]
    }
   ],
   "source": [
    "# Load the saved state_dict\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "marida_test_df = create_marida_df(MARIDA_path, 'test')\n",
    "empty_df =  pd.DataFrame(columns=marida_test_df.columns)\n",
    "marida_test_ds = MergedSegmentationDataset_B(marida_test_df, empty_df, global_bands_mean, global_bands_std, selected_bands, transform=transformTest, standardization= standardization )\n",
    "\n",
    "marida_testLoader = DataLoader(marida_test_ds, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False,\n",
    "                        collate_fn=custom_collate_fn,\n",
    "                        #worker_init_fn=worker_init_fn,\n",
    "                        #generator=torch.Generator().manual_seed(seed) \n",
    "                        )\n",
    "\n",
    "test_metrics_history = []\n",
    "model.eval()\n",
    "yTrue = []\n",
    "yPredicted = []\n",
    "testLossF = []\n",
    "with torch.no_grad():\n",
    "    for image, target, _ in marida_testLoader:\n",
    "\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        logits = model(image)\n",
    "        # print(f'image dtype {image.dtype}')\n",
    "        # print(f'logits dtype {logits.dtype}')\n",
    "        # print(f'target dtype {target.dtype}')\n",
    "        # print(f'test - target shape {target.shape}')\n",
    "        #print(f'test - logit shape {logits.shape}')\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "        logits = logits.reshape((-1,output_classes))\n",
    "        target = target.reshape(-1)\n",
    "        ###################################################################################\n",
    "        mask = target != -1\n",
    "        ###################################################################################\n",
    "        \n",
    "        # bg_logits = logits[~mask]\n",
    "        # bg_target = target[~mask]\n",
    "        \n",
    "        logits = logits[mask]\n",
    "        target = target[mask]\n",
    "        \n",
    "\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        ########### threshold #########\n",
    "        probs[probs[:, 1] < 0.8] = 0.\n",
    "        ###############################\n",
    "        print(f'test - probs shape {probs.shape}')\n",
    "        target = target.cpu().numpy()\n",
    "        # testBatches += target.shape[0]\n",
    "        testLossF.append((loss.data*target.shape[0]).tolist())\n",
    "        yPredicted += probs.argmax(1).tolist()\n",
    "        yTrue += target.tolist()\n",
    "\n",
    "\n",
    "        # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n",
    "        # bg_target = bg_target.cpu().numpy()\n",
    "\n",
    "        # bg_yPredicted += bg_probs.argmax(1).tolist()\n",
    "        # bg_yTrue += bg_target.tolist()\n",
    "    \n",
    "    yPredicted = np.asarray(yPredicted)\n",
    "    yTrue = np.asarray(yTrue)\n",
    "    acc = Evaluation(yPredicted, yTrue)\n",
    "    test_metrics_history.append(acc)\n",
    "\n",
    "\n",
    "    # bg_yPredicted = np.asarray(bg_yPredicted)\n",
    "    # bg_yTrue = np.asarray(bg_yTrue)\n",
    "    # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n",
    "    print(acc)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:45:02.085205Z",
     "iopub.status.busy": "2025-05-25T20:45:02.084691Z",
     "iopub.status.idle": "2025-05-25T20:45:02.315907Z",
     "shell.execute_reply": "2025-05-25T20:45:02.314855Z",
     "shell.execute_reply.started": "2025-05-25T20:45:02.085182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! cp best_model.pth model_50_epochs_ratio_1_20_bs16_test_iou_debris_076_thr0.8.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:32:23.233343Z",
     "iopub.status.busy": "2025-05-25T20:32:23.233145Z",
     "iopub.status.idle": "2025-05-25T20:32:23.247170Z",
     "shell.execute_reply": "2025-05-25T20:32:23.246550Z",
     "shell.execute_reply.started": "2025-05-25T20:32:23.233328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# All black\n",
    "# /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180916T101021_R022_T33TUL/S2A_MSIL1C_20180916T101021_R022_T33TUL_366560_5053920.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T20:32:23.247993Z",
     "iopub.status.busy": "2025-05-25T20:32:23.247827Z",
     "iopub.status.idle": "2025-05-25T20:32:23.261313Z",
     "shell.execute_reply": "2025-05-25T20:32:23.260636Z",
     "shell.execute_reply.started": "2025-05-25T20:32:23.247980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Lightning implementation. To be used later.\n",
    "\n",
    "# class BinaryClassificationModel(pl.LightningModule):\n",
    "#     def __init__(self, hparams):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(hparams)\n",
    "\n",
    "#         # Model selection\n",
    "#         if hparams.model_name == \"resattunet\":\n",
    "#             self.model = ResidualAttentionUNet(11, 11)\n",
    "#             # Modify for binary classification\n",
    "#             self.model.decoder = nn.Sequential(\n",
    "#                 self.model.decoder,\n",
    "#                 nn.AdaptiveAvgPool2d(1),\n",
    "#                 nn.Flatten(),\n",
    "#                 nn.Linear(11, 2)  # Binary output\n",
    "#             )\n",
    "#         elif hparams.model_name == \"attunet\":\n",
    "#             self.model = AttentionUNet(11, 11)\n",
    "#             self.model.decoder = nn.Sequential(\n",
    "#                 self.model.decoder,\n",
    "#                 nn.AdaptiveAvgPool2d(1),\n",
    "#                 nn.Flatten(),\n",
    "#                 nn.Linear(11, 2)\n",
    "#             )\n",
    "#         elif hparams.model_name == \"unet\":\n",
    "#             self.model = UNet(11, 11)\n",
    "#             self.model.decoder = nn.Sequential(\n",
    "#                 self.model.decoder,\n",
    "#                 nn.AdaptiveAvgPool2d(1),\n",
    "#                 nn.Flatten(),\n",
    "#                 nn.Linear(11, 2)\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid model name\")\n",
    "\n",
    "#         # Loss function\n",
    "#         if hparams.focal_loss:\n",
    "#             self.criterion = FocalLoss()\n",
    "#         else:\n",
    "#             weight = gen_weights(class_distr, c=1.03)[:2]  # Binary classes\n",
    "#             self.criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=-1)\n",
    "\n",
    "#         # Track best metrics\n",
    "#         self.best_macro_f1 = 0.0\n",
    "#         self.best_micro_f1 = 0.0\n",
    "#         self.best_weight_f1 = 0.0\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         images, labels, _ = batch\n",
    "#         logits = self(images)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         images, labels, _ = batch\n",
    "#         logits = self(images)\n",
    "#         loss = self.criterion(logits, labels)\n",
    "#         probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "#         labels = labels.cpu().numpy()\n",
    "#         preds = probs.argmax(1)\n",
    "#         return {\"loss\": loss, \"preds\": preds.tolist(), \"labels\": labels.tolist()}\n",
    "\n",
    "#     def validation_epoch_end(self, outputs):\n",
    "#         preds = np.concatenate([o[\"preds\"] for o in outputs])\n",
    "#         labels = np.concatenate([o[\"labels\"] for o in outputs])\n",
    "#         loss = torch.stack([o[\"loss\"] for o in outputs]).mean()\n",
    "#         acc = Evaluation(preds, labels)\n",
    "\n",
    "#         self.log(\"val_loss\", loss, prog_bar=True)\n",
    "#         self.log(\"val_macro_precision\", acc[\"macroPrec\"], prog_bar=True)\n",
    "#         self.log(\"val_macro_recall\", acc[\"macroRec\"])\n",
    "#         self.log(\"val_macro_f1\", acc[\"macroF1\"])\n",
    "#         self.log(\"val_micro_precision\", acc[\"microPrec\"])\n",
    "#         self.log(\"val_micro_recall\", acc[\"microRec\"])\n",
    "#         self.log(\"val_micro_f1\", acc[\"microF1\"])\n",
    "#         self.log(\"val_weight_precision\", acc[\"weightPrec\"])\n",
    "#         self.log(\"val_weight_recall\", acc[\"weightRec\"])\n",
    "#         self.log(\"val_weight_f1\", acc[\"weightF1\"])\n",
    "#         self.log(\"val_iou\", acc[\"IoU\"])\n",
    "\n",
    "#         # Update best metrics\n",
    "#         if acc[\"macroF1\"] > self.best_macro_f1:\n",
    "#             self.best_macro_f1 = acc[\"macroF1\"]\n",
    "#         if acc[\"microF1\"] > self.best_micro_f1:\n",
    "#             self.best_micro_f1 = acc[\"microF1\"]\n",
    "#         if acc[\"weightF1\"] > self.best_weight_f1:\n",
    "#             self.best_weight_f1 = acc[\"weightF1\"]\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(\n",
    "#             self.parameters(),\n",
    "#             lr=self.hparams.initial_lr,\n",
    "#             weight_decay=self.hparams.decay_lr\n",
    "#         )\n",
    "#         if self.hparams.scheduler_lr == \"rop\":\n",
    "#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#                 optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n",
    "#             )\n",
    "#             return {\n",
    "#                 \"optimizer\": optimizer,\n",
    "#                 \"lr_scheduler\": scheduler,\n",
    "#                 \"monitor\": \"val_loss\"\n",
    "#             }\n",
    "#         else:\n",
    "#             scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "#                 optimizer, milestones=[40, 80, 120, 160], gamma=0.5, verbose=True\n",
    "#             )\n",
    "#             return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             RandomRotationTransform([-90, 0, 90, 180]),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.Normalize(bands_mean, bands_std)\n",
    "#         ])\n",
    "#         dataset = MergedSegmentationDataset(\n",
    "#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n",
    "#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n",
    "#             transform=transform\n",
    "#         )\n",
    "#         return DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=self.hparams.train_batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=4,\n",
    "#             worker_init_fn=seed_worker,\n",
    "#             generator=torch.Generator().manual_seed(0)\n",
    "#         )\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(bands_mean, bands_std)\n",
    "#         ])\n",
    "#         dataset = MergedSegmentationDataset(\n",
    "#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n",
    "#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n",
    "#             transform=transform\n",
    "#         )\n",
    "#         return DataLoader(\n",
    "#             dataset,\n",
    "#             batch_size=self.hparams.test_batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=4,\n",
    "#             worker_init_fn=seed_worker,\n",
    "#             generator=torch.Generator().manual_seed(0)\n",
    "#         )\n",
    "\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--train_batch_size', type=int, default=8)\n",
    "#     parser.add_argument('--test_batch_size', type=int, default=4)\n",
    "#     parser.add_argument('--total_epochs', type=int, default=50)\n",
    "#     parser.add_argument('--experiment_name', type=str, required=True)\n",
    "#     parser.add_argument('--initial_lr', type=float, default=1e-3)\n",
    "#     parser.add_argument('--decay_lr', type=float, default=0)\n",
    "#     parser.add_argument('--scheduler_lr', type=str, default=\"ms\")\n",
    "#     parser.add_argument('--focal_loss', type=bool, default=False)\n",
    "#     parser.add_argument('--model_name', type=str, default=\"resattunet\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Set seeds for reproducibility\n",
    "#     pl.seed_everything(0, workers=True)\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = BinaryClassificationModel(args)\n",
    "\n",
    "#     # Logger\n",
    "#     logger = TensorBoardLogger(save_dir=args.experiment_name, name=\"logs\")\n",
    "\n",
    "#     # Callbacks for saving best models\n",
    "#     checkpoint_macro = ModelCheckpoint(\n",
    "#         dirpath=args.experiment_name,\n",
    "#         filename=\"bestMacroF1Model\",\n",
    "#         monitor=\"val_macro_f1\",\n",
    "#         mode=\"max\",\n",
    "#         save_top_k=1\n",
    "#     )\n",
    "#     checkpoint_micro = ModelCheckpoint(\n",
    "#         dirpath=args.experiment_name,\n",
    "#         filename=\"bestMicroF1Model\",\n",
    "#         monitor=\"val_micro_f1\",\n",
    "#         mode=\"max\",\n",
    "#         save_top_k=1\n",
    "#     )\n",
    "#     checkpoint_weight = ModelCheckpoint(\n",
    "#         dirpath=args.experiment_name,\n",
    "#         filename=\"bestWeightF1Model\",\n",
    "#         monitor=\"val_weight_f1\",\n",
    "#         mode=\"max\",\n",
    "#         save_top_k=1\n",
    "#     )\n",
    "\n",
    "#     # Trainer\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=args.total_epochs,\n",
    "#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "#         devices=1,\n",
    "#         logger=logger,\n",
    "#         callbacks=[checkpoint_macro, checkpoint_micro, checkpoint_weight],\n",
    "#         deterministic=True\n",
    "#     )\n",
    "\n",
    "#     # Train\n",
    "#     trainer.fit(model)\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5735509,
     "sourceId": 9438808,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7401788,
     "sourceId": 11885707,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
