{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9438808,"sourceType":"datasetVersion","datasetId":5735509},{"sourceId":11885707,"sourceType":"datasetVersion","datasetId":7401788}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import multiprocessing as mp\n# mp.set_start_method('spawn')  # Set before any other imports or operations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:03:46.658524Z","iopub.execute_input":"2025-05-22T16:03:46.658759Z","iopub.status.idle":"2025-05-22T16:03:46.663831Z","shell.execute_reply.started":"2025-05-22T16:03:46.658735Z","shell.execute_reply":"2025-05-22T16:03:46.663020Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! ls /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:03:46.664793Z","iopub.execute_input":"2025-05-22T16:03:46.665295Z","iopub.status.idle":"2025-05-22T16:03:46.826848Z","shell.execute_reply.started":"2025-05-22T16:03:46.665248Z","shell.execute_reply":"2025-05-22T16:03:46.826029Z"}},"outputs":[{"name":"stdout","text":"best_model.pth\t\t\t  marida_df_invalid_info.csv\nglobal_stats.npz\t\t  marida_stats.npz\nlitter_rows_df_invalid_info.csv   marida_test_df_invalid.csv\nlitter_rows_stats.npz\t\t  marida_val_df_invalid.csv\nlitter_rows_val_invalid_info.csv  model_checkpoint_30_epochs.pth\nLR_splits\t\t\t  model_checkpoint_8_epochs_bs16_iou0713.pth\nlr_with_invalid.csv\t\t  model_checkpoint_8_epochs_bs16_iou075.pth\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%capture\n! pip install rasterio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:03:46.829107Z","iopub.execute_input":"2025-05-22T16:03:46.829669Z","iopub.status.idle":"2025-05-22T16:03:52.088892Z","shell.execute_reply.started":"2025-05-22T16:03:46.829640Z","shell.execute_reply":"2025-05-22T16:03:52.088127Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport shutil\nimport re\nfrom PIL import Image\nimport rasterio\nimport matplotlib.pyplot as plt\nimport dask.array as da\nfrom scipy.ndimage import binary_dilation\nfrom skimage.morphology import disk  # For circular structuring elements\nimport torch\nfrom torchvision import transforms\nimport torchvision.transforms.functional as vF\nimport torch.nn.functional as F\nimport gdown\nfrom tqdm import tqdm\nimport random\n\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, jaccard_score, hamming_loss, label_ranking_loss, coverage_error, classification_report\nimport sklearn.metrics as metr\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:03:52.089786Z","iopub.execute_input":"2025-05-22T16:03:52.090027Z","iopub.status.idle":"2025-05-22T16:04:02.599695Z","shell.execute_reply.started":"2025-05-22T16:03:52.089994Z","shell.execute_reply":"2025-05-22T16:04:02.599111Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", None)  # Show full column values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.600421Z","iopub.execute_input":"2025-05-22T16:04:02.600887Z","iopub.status.idle":"2025-05-22T16:04:02.604491Z","shell.execute_reply.started":"2025-05-22T16:04:02.600867Z","shell.execute_reply":"2025-05-22T16:04:02.603832Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nfrom torch.utils.data import DataLoader, Dataset\n\ndef set_seed(seed):\n    \"\"\"\n    Set random seeds for NumPy, PyTorch (CPU and GPU), and Python's random module.\n    \n    Args:\n        seed (int): Seed value for RNGs\n    \"\"\"\n    # Python random\n    random.seed(seed)\n    \n    # NumPy\n    np.random.seed(seed)\n    \n    # PyTorch CPU\n    torch.manual_seed(seed)\n    \n    # PyTorch GPU (CUDA)\n    torch.cuda.manual_seed(seed)  # Current GPU\n    torch.cuda.manual_seed_all(seed)  # All GPUs\n    \n    # Ensure deterministic behavior\n    #torch.use_deterministic_algorithms(True)\n    #torch.backends.cudnn.deterministic = True\n    #torch.backends.cudnn.benchmark = False\n\ndef worker_init_fn(worker_id):\n    \"\"\"\n    Initialize random seed for DataLoader workers.\n    Ensures each worker has a unique but reproducible RNG state.\n    \n    Args:\n        worker_id (int): Worker ID\n    \"\"\"\n    max_seed = 2**32 - 1  # NumPy seed limit\n    worker_seed = (torch.initial_seed() + worker_id) % max_seed\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.605189Z","iopub.execute_input":"2025-05-22T16:04:02.605433Z","iopub.status.idle":"2025-05-22T16:04:02.627950Z","shell.execute_reply.started":"2025-05-22T16:04:02.605410Z","shell.execute_reply":"2025-05-22T16:04:02.627235Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def create_LR_dataframe(splits_path, mode='train'):\n    split_images_files = {'train' : 'train_X.txt', 'val' : 'val_X.txt', 'test' : 'test_X.txt'}\n    split_masks_files = {'train' : 'train_masks.txt', 'val' : 'val_masks.txt', 'test' : 'test_masks.txt'}  \n    with open(os.path.join(splits_path, split_images_files[mode]), \"r\") as file:\n        images = file.readlines()  # Reads all lines into a list\n        images = [image.strip() for image in images]  # Remove any trailing newline characters\n    with open(os.path.join(splits_path, split_masks_files[mode]), \"r\") as file:\n        masks = file.readlines()  # Reads all lines into a list\n        masks = [mask.strip() for mask in masks]  # Remove any trailing newline characters\n    df = pd.DataFrame({'image' : images, 'mask' : masks})\n    return df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.628768Z","iopub.execute_input":"2025-05-22T16:04:02.629009Z","iopub.status.idle":"2025-05-22T16:04:02.644054Z","shell.execute_reply.started":"2025-05-22T16:04:02.628994Z","shell.execute_reply":"2025-05-22T16:04:02.643515Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# from Sagar and Navodita's code\ndef compute_fdi_from_tiff(tiff_path):\n    with rasterio.open(tiff_path) as src:\n        # Assuming band order follows your stacked TIFF (B1â€“B12, skipping B10 if needed)\n        # Band indices are 1-based in rasterio\n        R665 = src.read(4)    # B4\n        R859 = src.read(9)    # B8A\n        R1610 = src.read(10)  # B11\n        # Convert to float and mask invalid values\n        R665 = R665.astype(np.float32)\n        R859 = R859.astype(np.float32)\n        R1610 = R1610.astype(np.float32)\n        # Calculate FDI\n        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n        return FDI\n\ndef cvt_to_fdi(images):\n    fdi_images = []\n    batch = images.copy()\n    if len(images.shape) == 3 : \n        batch = batch[None, :]\n    for i in range(batch.shape[0]):\n        im = batch[i]\n        R665 = im[3]   # B4\n        R859 = im[8]   # B8A\n        R1610 = im[0]  # B11\n        # Convert to float and mask invalid values\n        R665 = R665.astype(np.float32)\n        R859 = R859.astype(np.float32)\n        R1610 = R1610.astype(np.float32)\n        # Calculate FDI\n        FDI = R859 - (R665 + ((R1610 - R665) * (859 - 665) / (1610 - 665)))\n        fdi_images.append(FDI)\n    return np.array(fdi_images)\n    \ndef compute_ndwi(tiff_path):\n    with rasterio.open(tiff_path) as src:\n        Rgreen = src.read(3).astype(np.float32)  # Band 3 (Green)\n        Rnir = src.read(8).astype(np.float32)    # Band 8 (NIR)\n        ndwi = (Rgreen - Rnir) / (Rgreen + Rnir + 1e-6)  # avoid divide-by-zero\n    return ndwi\ndef plot_fdi(fdi_array, ndwi, img_path, mask_path):\n    with rasterio.open(img_path) as src:\n        rgb = src.read([4, 3, 2])\n        rgb = np.transpose(rgb, (1, 2, 0))\n    # Normalization\n    rgb = rgb.astype(np.float32)\n    rgb = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n    with rasterio.open(mask_path) as src:\n        mask = src.read(1)\n    # Create binary mask\n    mask_binary = mask > 0\n    # Plot side-by-side\n    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n    axs[0].imshow(rgb)\n    axs[0].set_title(\"RGB Patch\")\n    axs[1].imshow(mask_binary)  #, cmap='gray')\n    axs[1].set_title(\"Binary Mask (._cl.tif)\")\n    axs[2].imshow(fdi_array)\n    axs[2].set_title(\"FDI\")\n    axs[3].imshow(ndwi)\n    axs[3].set_title(\"NDWI\")\n    for ax in axs:\n        ax.axis('off')\n\n    # with rasterio.open(patch_path) as patch_src:\n    #     rgb = patch_src.read([4, 3, 2])  # Use bands B4, B3, B2 for RGB\n    #     rgb = np.transpose(rgb, (1, 2, 0))\n    #     rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n    import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\n# List of image and mask file paths (replace with your file paths)\nimage_mask_pairs = [\n    ('path_to_image1.jpg', 'path_to_mask1.png'),\n    ('path_to_image2.jpg', 'path_to_mask2.png'),\n    # Add more pairs as needed\n]\n\n\ndef cvt_RGB(images):\n    rgb_images = []\n    for i in range(images.shape[0]):\n        rgb = images[i][[4-1, 3-1, 2-1]] # Use bands B4, B3, B2 for RGB\n        rgb = np.transpose(rgb, (1, 2, 0))\n        rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n        rgb_images.append(rgb)\n    return np.array(rgb_images)\n\ndef display(images, masks):\n    # Determine the number of pairs\n    num_pairs = images.shape[0]\n\n    # Calculate layout: use 2 columns per pair (image + mask), adjust rows dynamically\n    cols = 2  # One column for image, one for mask\n    rows = num_pairs  # One row per pair\n\n    # Create a figure with subplots\n    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n\n    # Handle case of single pair (axes is not a 2D array)\n    if num_pairs == 1:\n        axes = np.array([axes]).reshape(1, -1)\n\n    # Iterate through each pair and display image and mask\n    for idx, (image, mask) in enumerate(zip(images, masks)):\n\n        # Display the original image\n        axes[idx, 0].imshow(image)\n        axes[idx, 0].set_title(f'Image {idx + 1}')\n        axes[idx, 0].axis('off')  # Hide axes\n    \n        # Display the segmentation mask\n        axes[idx, 1].imshow(mask, cmap='gray')  # Adjust cmap if needed\n        axes[idx, 1].set_title(f'Mask {idx + 1}')\n        axes[idx, 1].axis('off')  # Hide axes\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    \n    # Show the plot\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.644724Z","iopub.execute_input":"2025-05-22T16:04:02.644945Z","iopub.status.idle":"2025-05-22T16:04:02.660862Z","shell.execute_reply.started":"2025-05-22T16:04:02.644918Z","shell.execute_reply":"2025-05-22T16:04:02.660288Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\ndef extract_date_tile(filename):\n    \"\"\"Extract date and tile from filename using regex.\"\"\"\n    pattern = r'^(\\d{1,2}-\\d{1,2}-\\d{2})_([A-Z0-9]+)_\\d+$'\n    match = re.match(pattern, filename)\n    if not match:\n        raise ValueError(f\"Invalid filename format: {filename}\")\n    return match.groups()  # Returns tuple (date, tile)\n\ndef create_marida_df(data_path, mode='train'):\n    \"\"\"Create DataFrame from MARIDA dataset files.\"\"\"\n    # Determine split file based on mode\n    split_files = {'train': 'train_X.txt', 'val': 'val_X.txt', 'test': 'test_X.txt'}\n    items_list_path = os.path.join(data_path, 'splits', split_files[mode])\n\n    # Read items list\n    with open(items_list_path, 'r') as file:\n        items = [item.strip() for item in file]\n\n    # Base path for patches\n    items_path = os.path.join(data_path, 'patches')\n\n    # Prepare data lists\n    data = {\n        'image': [],\n        'mask': [],\n        'confidence': [],\n        'date': [],\n        'tile': []\n    }\n\n    # Process each item\n    for item in items:\n        tile = \"_\".join(item.split(\"_\")[:-1])\n        tile_path = os.path.join(items_path, f\"S2_{tile}\")\n\n        # Define file paths\n        base_name = f'S2_{item}'\n        paths = {\n            'image': os.path.join(tile_path, f'{base_name}.tif'),\n            'mask': os.path.join(tile_path, f'{base_name}_cl.tif'),\n            'confidence': os.path.join(tile_path, f'{base_name}_conf.tif')\n        }\n\n        # Check if all files exist\n        if all(os.path.exists(p) for p in paths.values()):\n            data['image'].append(paths['image'])\n            data['mask'].append(paths['mask'])\n            data['confidence'].append(paths['confidence'])\n            date, tile = extract_date_tile(item)\n            data['date'].append(date)\n            data['tile'].append(tile)\n\n    return pd.DataFrame(data)\n\n# MARIDA labels dictionary\nMARIDA_LABELS = {\n    i: label for i, label in enumerate([\n        'Marine Debris', 'Dense Sargassum', 'Sparse Sargassum', 'Natural Organic Material',\n        'Ship', 'Clouds', 'Marine Water', 'Sediment-Laden Water', 'Foam', 'Turbid Water',\n        'Shallow Water', 'Waves', 'Cloud Shadows', 'Wakes', 'Mixed Water'\n    ], 1)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.663030Z","iopub.execute_input":"2025-05-22T16:04:02.663206Z","iopub.status.idle":"2025-05-22T16:04:02.679832Z","shell.execute_reply.started":"2025-05-22T16:04:02.663194Z","shell.execute_reply":"2025-05-22T16:04:02.679293Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import rasterio\nimport numpy as np\n\ndef compute_invalid_pixels(image_paths, mask_paths):\n    \"\"\"\n    Compute per-band statistics for Sentinel-2 L1C ACOLITE-processed images using segmentation masks.\n    Creates a mask to exclude invalid pixels (NaNs, negative values, specified no-data value).\n    \n    Parameters:\n    - image_paths: List of paths to image files (e.g., GeoTIFF with 11 bands).\n    - mask_paths: List of paths to segmentation mask files (single-band, integer class labels).\n    - class_labels: List of mask class labels to include (e.g., [1, 2] for vegetation and water).\n                   If None, include all non-zero labels (excluding background).\n    - invalid_value: Optional value to treat as invalid in images (e.g., -9999).\n    \n    Returns:\n    - mean_per_band: List of per-band means for each image.\n    - std_per_band: List of per-band standard deviations for each image.\n    \"\"\"\n    mean_per_band = []  # Initialize as list\n    std_per_band = []   # Initialize as list\n    positive_pixels = []\n    tot_pixels = [];\n    images_with_invalid_pixels = []\n    black_list = []\n    accumulator = None\n    no_data_pixels = []\n    neg_pixels = []\n    nan_pixels = []\n    gt1_pixels = []\n    imgs_with_invalid = []\n    positive_pixels = []\n    min_vals = []\n    max_vals = []\n    for img_path, mask_path in zip(image_paths, mask_paths):\n        # Load image and mask\n        with rasterio.open(img_path) as src_img, rasterio.open(mask_path) as src_mask:\n            image = src_img.read()  # Shape: (bands, height, width)\n            mask = src_mask.read(1)  # Shape: (height, width)\n            \n            # Convert image to float for NaN handling\n            image = image.astype(float)\n\n            nan_mask = np.isnan(image)\n            neg_mask = (image < 0)\n            too_big_mask = (image > 1)\n            no_data_mask = (image == src_img.nodata)\n            nan_pixels.append(np.sum(nan_mask))\n            neg_pixels.append(np.sum(neg_mask))\n            gt1_pixels.append(np.sum(too_big_mask))\n            no_data_pixels.append(np.sum(no_data_mask))\n            imgs_with_invalid.append(img_path)\n            positive_pixels.append(np.sum(mask > 0))\n            min_vals.append(np.min(image))\n            max_vals.append(np.max(image))\n    df = pd.DataFrame({'image' : imgs_with_invalid, 'no data pixels' : no_data_pixels, 'negative pixels' : neg_pixels,\n                      'nan pixels' : nan_pixels, 'high value pixels' :  gt1_pixels, 'debris pixels' : positive_pixels,\n                      'min values' : min_vals, 'max values' : max_vals})\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.680479Z","iopub.execute_input":"2025-05-22T16:04:02.680710Z","iopub.status.idle":"2025-05-22T16:04:02.697450Z","shell.execute_reply.started":"2025-05-22T16:04:02.680691Z","shell.execute_reply":"2025-05-22T16:04:02.696833Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#Setting batch size\nbatch_size = 16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.697993Z","iopub.execute_input":"2025-05-22T16:04:02.698169Z","iopub.status.idle":"2025-05-22T16:04:02.771718Z","shell.execute_reply.started":"2025-05-22T16:04:02.698157Z","shell.execute_reply":"2025-05-22T16:04:02.771033Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def compute_stats(image_files, discard_negatives = False, discard_gt_1 = False):\n    bands_std = []\n    bands_mean = []\n    valid_pixels = []\n\n    for band_idx in range(11):\n        arrays = [da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto')\n                  for f in image_files]\n        stack = da.stack(arrays)\n        #valid = (stack != rasterio.open(image_files[0]).nodata) & (stack >= 0)\n        if discard_negatives and  discard_gt_1: \n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) & \n                              (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1) \n                              for f in image_files])\n        elif discard_gt_1 :\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                              & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') <= 1)  \n                              for f in image_files])\n        elif discard_negatives:\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                  & (da.from_array(rasterio.open(f).read(band_idx + 1), chunks='auto') >= 0) \n                  for f in image_files])\n        else :\n            valid = da.stack([da.from_array(rasterio.open(f).read(band_idx + 1) != rasterio.open(f).nodata, chunks='auto')\n                  for f in image_files])\n                         \n        # Compute number of valid pixels\n        valid_count = da.sum(valid).compute()\n        valid_pixels.append(valid_count)\n        mean = da.nanmean(stack[valid]).compute()\n        std = da.nanstd(stack[valid]).compute()\n        bands_mean.append(mean)\n        bands_std.append(std)\n        print(f\"Band {band_idx} - Mean: {mean}, Std: {std}\")\n    return {'mean' : np.array(bands_mean), 'std': np.array(bands_std),'valid pixels' : np.array(valid_pixels) }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.772409Z","iopub.execute_input":"2025-05-22T16:04:02.772693Z","iopub.status.idle":"2025-05-22T16:04:02.786821Z","shell.execute_reply.started":"2025-05-22T16:04:02.772677Z","shell.execute_reply":"2025-05-22T16:04:02.786309Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def computing_labeled_pixels_stats(mask_paths):\n    arrays = [da.from_array(rasterio.open(f).read(1), chunks='auto')\n                  for f in mask_paths]\n    stack = da.stack(arrays)\n    valid = stack > 0\n    labeled_count = da.sum(valid).compute()\n    return labeled_count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.787651Z","iopub.execute_input":"2025-05-22T16:04:02.787984Z","iopub.status.idle":"2025-05-22T16:04:02.803856Z","shell.execute_reply.started":"2025-05-22T16:04:02.787961Z","shell.execute_reply":"2025-05-22T16:04:02.803153Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def compute_invalid_mask(path):\n    with rasterio.open(path) as src:\n        image = src.read()\n        \n        invalid_mask = image == src.nodata\n        invalid_mask |= np.isnan(image)\n        invalid_mask |= image < 0\n        invalid_mask |= image > 1\n        invalid_mask = np.any(invalid_mask, axis=0)\n        return invalid_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.804697Z","iopub.execute_input":"2025-05-22T16:04:02.804902Z","iopub.status.idle":"2025-05-22T16:04:02.816644Z","shell.execute_reply.started":"2025-05-22T16:04:02.804880Z","shell.execute_reply":"2025-05-22T16:04:02.816075Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_invalid_mask(image, no_data):\n    invalid_mask = image == no_data\n    invalid_mask |= np.isnan(image)\n    invalid_mask |= image < -1.5\n    invalid_mask |= image > 1.5\n    #invalid_mask = np.any(invalid_mask, axis=0)\n    return invalid_mask  #torch.fromnumpy(invalid_mask)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.817419Z","iopub.execute_input":"2025-05-22T16:04:02.817674Z","iopub.status.idle":"2025-05-22T16:04:02.830397Z","shell.execute_reply.started":"2025-05-22T16:04:02.817653Z","shell.execute_reply":"2025-05-22T16:04:02.829776Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def select_bg_pixels(image, debris_mask, r1=5, r2=20, target_ratio=5):\n    H, W = debris_mask.shape\n    \n    #target_ratio = 5  # Debris-to-background ratio (1:5)\n\n    # Create structuring elements (circular or square)\n    se_r1 = disk(r1) if r1 > 0 else np.ones((1, 1))  # Inner dilation kernel\n    se_r2 = disk(r2)                         # Outer dilation kernel\n    #print('before binary dilation')\n    # Dilate debris mask with r1 and r2\n    dilated_r1 = binary_dilation(debris_mask, structure=se_r1)\n    dilated_r2 = binary_dilation(debris_mask, structure=se_r2)\n    #print('before anular mask')\n    # Compute annular region: pixels in dilated_r2 but not in dilated_r1\n    annular_mask = dilated_r2 & ~dilated_r1\n\n    # Sample background pixels from annular region\n    valid_background_coords = np.where(annular_mask)\n    num_debris = np.sum(debris_mask)\n    num_background = min(len(valid_background_coords[0]), num_debris * target_ratio)\n    if num_background > 0:\n        sample_idx = np.random.choice(len(valid_background_coords[0]), size=num_background, replace=False)\n        background_coords = [(valid_background_coords[0][i], valid_background_coords[1][i]) for i in sample_idx]\n    else:\n        print(\"Warning: No valid background pixels in annular region. Increase r2 or check mask.\")\n\n    # Create background mask (optional, for visualization or training)\n    background_mask = np.zeros_like(debris_mask)\n    for x, y in background_coords:\n        background_mask[x, y] = 1\n    return background_mask\n\n# Optional: Filter by features (e.g., RGB values for water-like pixels)\n# Example: If image is RGB, filter pixels with low green channel (common for water)\n# image = ...  # Your RGB or multispectral image\n# valid_background = [coord for coord in background_coords if image[coord[0], coord[1], 1] < threshold]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.830998Z","iopub.execute_input":"2025-05-22T16:04:02.831229Z","iopub.status.idle":"2025-05-22T16:04:02.848876Z","shell.execute_reply.started":"2025-05-22T16:04:02.831215Z","shell.execute_reply":"2025-05-22T16:04:02.848295Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def batch_process_marida_masks(masks, dataset_ids, device='cpu'):\n    \"\"\"\n    Process masks for dataset_id == 0 (MARIDA) at the batch level.\n    - Set classes [1, 2, 3, 4, 9] to 2 (debris).\n    - Set class 0 to 0 (unlabeled), other classes to 1 (non-debris).\n    \n    Args:\n        masks: Tensor [batch_size, H, W] (integer-valued masks)\n        dataset_ids: Tensor [batch_size] (dataset IDs)\n        device: Device for PyTorch operations ('cpu' or 'cuda')\n    \n    Returns:\n        marida_masks: Tensor [batch_size, H, W] with values 0, 1, 2\n    \"\"\"\n    batch_size, H, W = masks.shape\n    marida_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n    \n    # Identify masks with dataset_id == 0\n    marida_mask = (dataset_ids == 0)  # [batch_size], boolean\n    if not marida_mask.any():\n        return marida_masks\n    \n    # Select masks for dataset_id == 0\n    selected_masks = masks[marida_mask]  # [num_marida, H, W]\n    \n    # Set classes [1, 2, 3, 4, 9] to 2\n    debris_classes = torch.tensor([1, 2, 3, 4, 9], device=device)\n    is_debris = torch.isin(selected_masks, debris_classes)  # [num_marida, H, W]\n    marida_masks[marida_mask] = torch.where(\n        is_debris,\n        torch.tensor(2, dtype=torch.int64, device=device),\n        selected_masks  # Temporarily keep original values\n    )\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    # Set non-zero, non-debris pixels to 1\n    marida_masks[marida_mask] = torch.where(\n        (marida_masks[marida_mask] != 0) & (marida_masks[marida_mask] != 2),\n        torch.tensor(1, dtype=torch.int64, device=device),\n        marida_masks[marida_mask]\n    )\n    # print('only 3 values : ')\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    marida_masks[marida_mask] = marida_masks[marida_mask] - 1\n    #print('after subtr')\n    # for idx in range( marida_masks[marida_mask].shape[0]):\n    #     print(f' {idx} has {torch.sum(is_debris[idx])} : {torch.unique(marida_masks[marida_mask][idx])}')\n    return marida_masks\n\n\n\n# # Custom collate function\n# def custom_collate_fn(batch):\n#     images, masks, dataset_ids = zip(*batch)\n#     images = torch.stack(images)\n#     masks = torch.stack(masks)\n#     dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)\n    \n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n    \n#     final_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n#                                          target_ratio=5, threshold=0.5, device=device)\n    \n#     return images, masks, final_masks, dataset_ids\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.849567Z","iopub.execute_input":"2025-05-22T16:04:02.849801Z","iopub.status.idle":"2025-05-22T16:04:02.867733Z","shell.execute_reply.started":"2025-05-22T16:04:02.849786Z","shell.execute_reply":"2025-05-22T16:04:02.867128Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"\ndef torch_dilate(mask, kernel_size, device='cpu'):\n    \"\"\"Apply dilation to a batch of masks using PyTorch convolution.\"\"\"\n    kernel = torch.ones(1, 1, kernel_size, kernel_size, device=device, dtype=torch.float32)\n    mask = mask.float().unsqueeze(1)  # [batch_size, 1, H, W]\n    dilated = torch.nn.functional.conv2d(mask, kernel, padding=kernel_size // 2) > 0\n    return dilated.squeeze(1).bool()  # [batch_size, H, W]\n\ndef batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, target_ratio=5, threshold=None, device='cpu'):\n    \"\"\"\n    Compute annular background masks for a batch of masks, only for dataset_id == 1.\n    - Set debris pixels (masks == 1) to 2 in bg_masks.\n    - Set randomly sampled annular pixels to 1 in bg_masks.\n    \n    Args:\n        images: Tensor [batch_size, C, H, W] \n        masks: Tensor [batch_size, H, W] (binary debris masks)\n        dataset_ids: Tensor [batch_size] (dataset IDs)\n        r1, r2: Radii for inner and outer dilation\n        target_ratio: Debris-to-background pixel ratio\n        threshold: Optional threshold for filtering (e.g., green channel)\n        device: Device for PyTorch operations ('cpu' or 'cuda')\n    \n    Returns:\n        bg_masks: Tensor [batch_size, H, W] with values 0 (default), 1 (background), 2 (debris)\n    \"\"\"\n\n    batch_size, H, W = masks.shape\n    # Initialize bg_masks with zeros (int64 to support values 0, 1, 2)\n    bg_masks = torch.zeros_like(masks, dtype=torch.int64, device=device)\n    \n    # Identify masks to process (dataset_id == 1)\n    valid_mask = (dataset_ids == 1)  # [batch_size], boolean{\n    #print(f'LR indices {valid_mask}')\n    if not valid_mask.any():\n        return bg_masks  # Return zeros if no masks need processing\n    \n    # Select masks for dataset_id == 1\n    selected_masks = masks[valid_mask]  # [num_valid, H, W]\n    # for idx in range(selected_masks.shape[0]):\n    #     print(f'num debris pixels : {torch.sum(selected_masks[idx])}')\n    # Set debris pixels to 2 for selected masks\n    bg_masks[valid_mask] = selected_masks * 2  # Where selected_masks == 1, set bg_masks to 2\n    \n    # Perform dilation on selected masks\n    dilated_r1 = torch_dilate(selected_masks, 2 * r1 + 1, device=device)  # [num_valid, H, W]\n    dilated_r2 = torch_dilate(selected_masks, 2 * r2 + 1, device=device)  # [num_valid, H, W]\n    annular_masks = dilated_r2 & ~dilated_r1  # [num_valid, H, W]\n    \n    # Sample background pixels for each selected mask\n    for idx in range(annular_masks.shape[0]):\n        valid_coords = torch.where(annular_masks[idx])  # Tuple of (row, col) indices\n        #print(f'unique values in mask {idx} : {torch.unique(selected_masks[idx])}')\n        num_debris = torch.sum(selected_masks[idx] > 0).item()\n        #print(f'num debris for index {idx} : {num_debris}')\n        num_background = min(len(valid_coords[0]), int(num_debris * target_ratio))\n        \n        if num_background > 0:\n            # Randomly sample indices and set to 1\n            sample_indices = torch.randperm(len(valid_coords[0]), device=device)[:num_background]\n            bg_masks[valid_mask.nonzero(as_tuple=True)[0][idx], \n                     valid_coords[0][sample_indices], \n                     valid_coords[1][sample_indices]] = 1\n        else :\n            print(f'no background selected for index {idx}. Num debrid : {num_debris} Num background : {num_background}')\n            print(f'valid coords {len(valid_coords)}')\n            print(f'unique valus : {torch.unique(selected_masks[idx])}')\n    \n    # # Optional: Filter by image features (e.g., green channel) for dataset_id == 1\n    # if threshold is not None and images is not None:\n    #     valid_pixels = images[valid_mask, 1, :, :] < threshold  # Green channel\n    #     # Only apply filtering to background pixels (value 1), preserve debris pixels (value 2)\n    #     bg_masks[valid_mask] = torch.where(\n    #         bg_masks[valid_mask] == 1,\n    #         bg_masks[valid_mask] & valid_pixels,\n    #         bg_masks[valid_mask]\n    #     )\n    bg_masks[valid_mask] = bg_masks[valid_mask] - 1\n    return bg_masks\n\n# Custom collate function\ndef custom_collate_fn(batch):\n    # print(f'custom collate function batch {len(batch)}')\n    # print(f'custom collate function batch type {type(batch)}')\n    # print(f'custom collate function batch[1] type {type(batch[1])}')\n    # print(f'custom collate function batch[1] len  {len(batch[1])}')\n    images, masks, dataset_ids = zip(*batch)\n    images = torch.stack(images)  # [batch_size, C, H, W]\n    masks = torch.stack(masks)    # [batch_size, H, W]\n    dataset_ids = torch.tensor(dataset_ids, dtype=torch.long)  # [batch_size]\n    \n    # Move to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    images, masks, dataset_ids = images.to(device), masks.to(device), dataset_ids.to(device)\n    \n    # Compute background masks\n    lr_masks = batch_select_bg_pixels(images, masks, dataset_ids, r1=5, r2=20, \n                                      target_ratio=LR_ratio, device=device)\n    marida_masks = batch_process_marida_masks(masks, dataset_ids, device=device)\n    masks = lr_masks + marida_masks\n    \n    return images, masks, dataset_ids\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.868366Z","iopub.execute_input":"2025-05-22T16:04:02.868524Z","iopub.status.idle":"2025-05-22T16:04:02.887553Z","shell.execute_reply.started":"2025-05-22T16:04:02.868511Z","shell.execute_reply":"2025-05-22T16:04:02.887024Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Seeding for reproducibility\nseed = 42\nset_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.888381Z","iopub.execute_input":"2025-05-22T16:04:02.888678Z","iopub.status.idle":"2025-05-22T16:04:02.909200Z","shell.execute_reply.started":"2025-05-22T16:04:02.888657Z","shell.execute_reply":"2025-05-22T16:04:02.908571Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# %%capture\n# # Download some pre-computed data \n\n\n# file_id = \"1NvgyeN-k-pRXF114IFhB-AcW86M9e7km\"\n# gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_stats.npz', quiet=False)\n# file_id = \"160mw3xELYG_44yemtf2vrBg6n-Q_e8LO\"\n# gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/marida_df_invalid_info.csv', quiet=False)\n# file_id = \"1a-4sJZ4NUZsNHuaSBRKoAG7Dy2O4kHWj\"\n# gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_df_invalid_info.csv', quiet=False)\n# file_id = \"1sEiP73c4I3S4KK-58c6J02wgmayu3EYa\"\n# gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/litter_rows_stats.npz', quiet=False)\n# file_id = \"1wrD41CDQud69AMOyHigw0-DR85Id4zDM\"\n# gdown.download(f'https://drive.google.com/uc?id={file_id}', '/kaggle/working/global_stats.npz', quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.909893Z","iopub.execute_input":"2025-05-22T16:04:02.910106Z","iopub.status.idle":"2025-05-22T16:04:02.924244Z","shell.execute_reply.started":"2025-05-22T16:04:02.910083Z","shell.execute_reply":"2025-05-22T16:04:02.923602Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# # check that the \n# ! ls /kaggle/input/litter-windrows-patches\n# # add the lr dataset to path to import code to prepare the dataset\n# sys.path.append('/kaggle/input/litter-windrows-patches')\n# # import functions to prepare dataset\n# from prepare_dataset import  get_image_and_mask_paths, split_and_save_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.924870Z","iopub.execute_input":"2025-05-22T16:04:02.925058Z","iopub.status.idle":"2025-05-22T16:04:02.940335Z","shell.execute_reply.started":"2025-05-22T16:04:02.925045Z","shell.execute_reply":"2025-05-22T16:04:02.939676Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#! git clone https://github.com/sheikhazhanmohammed/SADMA.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.941017Z","iopub.execute_input":"2025-05-22T16:04:02.941270Z","iopub.status.idle":"2025-05-22T16:04:02.954390Z","shell.execute_reply.started":"2025-05-22T16:04:02.941236Z","shell.execute_reply":"2025-05-22T16:04:02.953886Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#sys.path.append('/kaggle/working/SADMA')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.955109Z","iopub.execute_input":"2025-05-22T16:04:02.955294Z","iopub.status.idle":"2025-05-22T16:04:02.968541Z","shell.execute_reply.started":"2025-05-22T16:04:02.955273Z","shell.execute_reply":"2025-05-22T16:04:02.968020Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# # define a variable for the lr dataset\n# LW_path = '/kaggle/input/litter-windrows-patches'\n# lr_images, lr_masks = get_image_and_mask_paths(LW_path)\n# ! mkdir ./LR_splits\n# split_and_save_data(lr_images, lr_masks, './LR_splits' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.969166Z","iopub.execute_input":"2025-05-22T16:04:02.969455Z","iopub.status.idle":"2025-05-22T16:04:02.981707Z","shell.execute_reply.started":"2025-05-22T16:04:02.969440Z","shell.execute_reply":"2025-05-22T16:04:02.981174Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# ! ls ./LR_splits/splits\n# LR_splits_path = '/kaggle/working/LR_splits/splits'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.982414Z","iopub.execute_input":"2025-05-22T16:04:02.982618Z","iopub.status.idle":"2025-05-22T16:04:02.996605Z","shell.execute_reply.started":"2025-05-22T16:04:02.982605Z","shell.execute_reply":"2025-05-22T16:04:02.995951Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# from IPython.display import display\n\n# with open(LR_splits_path+'/train_X.txt', \"r\") as file:\n#     display(file.read())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:02.997289Z","iopub.execute_input":"2025-05-22T16:04:02.997753Z","iopub.status.idle":"2025-05-22T16:04:03.009872Z","shell.execute_reply.started":"2025-05-22T16:04:02.997737Z","shell.execute_reply":"2025-05-22T16:04:03.009374Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"! ls /kaggle/input/marida-marine-debrish-dataset\nMARIDA_path = '/kaggle/input/marida-marine-debrish-dataset'\n! ls /kaggle/input/litter-windrows-patches\nLR_splits_path = '/kaggle/input/litter-windrows-patches/binary_splits'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:03.013640Z","iopub.execute_input":"2025-05-22T16:04:03.013870Z","iopub.status.idle":"2025-05-22T16:04:03.294500Z","shell.execute_reply.started":"2025-05-22T16:04:03.013849Z","shell.execute_reply":"2025-05-22T16:04:03.293294Z"}},"outputs":[{"name":"stdout","text":"labels_mapping.txt  patches  shapefiles  splits\nannotation     multiclass_splits  prepare_dataset.ipynb\nbinary_splits  patches\t\t  README.md\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# MARIDA dataframe\nmarida_df = create_marida_df(MARIDA_path)\n#marida_df_invalid = compute_invalid_pixels(marida_df['image'].tolist(), marida_df['mask'].tolist())\n#marida_df_invalid.to_csv('/kaggle/working/marida_with_invalid.csv')\nmarida_df_invalid = pd.read_csv('/kaggle/working/marida_df_invalid_info.csv')\nmarida_df_F = marida_df.drop(marida_df_invalid[marida_df_invalid['nan pixels']>0].index)\n\n# MARIDA val dataframe\nmarida_val_df = create_marida_df(MARIDA_path, 'val')\n#marida_val_df_invalid = compute_invalid_pixels(marida_val_df['image'].tolist(), marida_val_df['mask'].tolist())\n#marida_val_df_invalid.to_csv('/kaggle/working/marida_val_df_invalid.csv')\nmarida_val_df_invalid =pd.read_csv('/kaggle/working/marida_val_df_invalid.csv')\nmarida_val_df_F = marida_val_df.drop(marida_val_df_invalid[marida_val_df_invalid['nan pixels'] > 0].index)\n\n# MARIDA test dataframe\nmarida_test_df = create_marida_df(MARIDA_path, 'test')\n#marida_test_df_invalid = compute_invalid_pixels(marida_test_df['image'].tolist(), marida_test_df['mask'].tolist())\n#marida_test_df_invalid.to_csv('/kaggle/working/marida_test_df_invalid.csv')\nmarida_test_df_invalid =pd.read_csv('/kaggle/working/marida_test_df_invalid.csv')\nmarida_test_df_F = marida_test_df.drop(marida_test_df_invalid[marida_test_df_invalid['nan pixels'] > 0].index)\n\n# LR dataframe\n\nlr_df = create_LR_dataframe(LR_splits_path)\n#lr_df_invalid = compute_invalid_pixels(lr_df['image'].tolist(), lr_df['mask'].tolist())\n#lr_df_invalid.to_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\nlr_df_invalid = pd.read_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\nlr_df_F = lr_df.drop(lr_df_invalid[lr_df_invalid['high value pixels'] > 0].index)\n\n#LR val dataset\nlr_val_df = create_LR_dataframe(LR_splits_path, 'val')\n#lr_val_df_invalid = compute_invalid_pixels(lr_val_df['image'].tolist(), lr_val_df['mask'].tolist())\n#lr_val_df_invalid.to_csv('/kaggle/working/litter_rows_val_invalid_info.csv')\nlr_val_df_invalid = pd.read_csv('/kaggle/working/litter_rows_val_invalid_info.csv')\nlr_val_df_F= lr_val_df.drop(lr_val_df_invalid[lr_val_df_invalid['high value pixels']>0].index)\n\n\n#lr_test_df_invalid = compute_invalid_pixels(lr_test_df['image'].tolist(), lr_test_df['mask'].tolist())\n#lr_test_df_invalid.to_csv('/kaggle/working/litter_rows_df_invalid_info.csv')\n#lr_test_df_invalid = pd.read_csv('/kaggle/working/litter_rows_df_invalid_info.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:03.295819Z","iopub.execute_input":"2025-05-22T16:04:03.296081Z","iopub.status.idle":"2025-05-22T16:04:13.801515Z","shell.execute_reply.started":"2025-05-22T16:04:03.296059Z","shell.execute_reply":"2025-05-22T16:04:13.800951Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# lr valid = 79495168","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.802169Z","iopub.execute_input":"2025-05-22T16:04:13.802425Z","iopub.status.idle":"2025-05-22T16:04:13.805875Z","shell.execute_reply.started":"2025-05-22T16:04:13.802400Z","shell.execute_reply":"2025-05-22T16:04:13.805211Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#lr_stats = compute_stats(lr_df_filt['image'].tolist())\n#np.savez(\"/kaggle/working/lr_stats.npz\", first=lr_stats['mean'], second=lr_stats['std'])\n#marida_stats = compute_stats(marida_df['image'].tolist())\n#np.savez(\"/kaggle/working/my_marida_stats.npz\", first=marida_stats['mean'], second=marida_stats['std'])\n#global_stats = compute_stats(marida_df['image'].tolist() + lr_df_filt['image'].to_list())\n#np.savez(\"/kaggle/working/global_stats.npz\", first=global_stats['mean'], second=global_stats['std'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.806654Z","iopub.execute_input":"2025-05-22T16:04:13.807432Z","iopub.status.idle":"2025-05-22T16:04:13.822367Z","shell.execute_reply.started":"2025-05-22T16:04:13.807404Z","shell.execute_reply":"2025-05-22T16:04:13.821597Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"global_stats = np.load('/kaggle/working/global_stats.npz')\nglobal_bands_mean = global_stats['first']\nglobal_bands_std = global_stats['second']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.822962Z","iopub.execute_input":"2025-05-22T16:04:13.823198Z","iopub.status.idle":"2025-05-22T16:04:13.838694Z","shell.execute_reply.started":"2025-05-22T16:04:13.823172Z","shell.execute_reply":"2025-05-22T16:04:13.838206Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# global_bands_mean =np.array([0.03721786, 0.03547978, 0.03033651, 0.01722546, 0.01574046,\n#         0.01738895, 0.01939084, 0.01724032, 0.01895351, 0.0109694 ,\n#         0.00784716])\n# global_bands_std = np.array([0.03185222, 0.03198375, 0.03251331, 0.03379553, 0.03407218,\n#         0.04551132, 0.05334419, 0.05064404, 0.0578197 , 0.03721222,\n#         0.02560836])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.839290Z","iopub.execute_input":"2025-05-22T16:04:13.839497Z","iopub.status.idle":"2025-05-22T16:04:13.853027Z","shell.execute_reply.started":"2025-05-22T16:04:13.839475Z","shell.execute_reply":"2025-05-22T16:04:13.852286Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"#computing_labeled_pixels_stats(lr_df_filt['mask'].tolist())\n#computing_labeled_pixels_stats(marida_df['mask'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.853638Z","iopub.execute_input":"2025-05-22T16:04:13.853822Z","iopub.status.idle":"2025-05-22T16:04:13.867263Z","shell.execute_reply.started":"2025-05-22T16:04:13.853802Z","shell.execute_reply":"2025-05-22T16:04:13.866587Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"marida_classes_distr = np.array([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\nlr_debris_pixels = 92090\nmarida_pixels = 429412\nmarida_debris_pixels = np.sum(marida_classes_distr[[0,1,2,3,8]]) * marida_pixels\nprint(f'marida debris pixels {marida_debris_pixels}')\ntot_glob_pixels = (len(lr_df_F) + len(marida_df_F))*256**2\nmarida_debris_fraction = np.sum(marida_classes_distr[[0,1,2,3,8]])\n#debris_fraction = (lr_debris_pixels + marida_debris_pixels)/tot_glob_pixels\nprint(f'marida_debris_fraction : {marida_debris_fraction}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.868107Z","iopub.execute_input":"2025-05-22T16:04:13.868690Z","iopub.status.idle":"2025-05-22T16:04:13.881996Z","shell.execute_reply.started":"2025-05-22T16:04:13.868673Z","shell.execute_reply":"2025-05-22T16:04:13.881462Z"}},"outputs":[{"name":"stdout","text":"marida debris pixels 5092.826320000001\nmarida_debris_fraction : 0.011860000000000002\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Computing here the percentage of debris pixels across the two datasets\n# This will be used as class distribution to generate weights for the loss function\nLR_ratio = 20 # \n\n# For MARIDA the loss function uses only pixels in the 15 classes \n# The fraction of classes assimilated to marine debris is \nmarida_debrix_pixels_distr = np.sum(marida_classes_distr[[0,1,2,3,8]])\n# For LR the DataSet will sample backgroung pixels with a given ratio, stored in the variable LR_ratio\n# Then the effective ratio \neffective_ratio = (1/LR_ratio * len(lr_df_F) + 0.011860000000000002 * len(marida_df_F))/(len(lr_df_F) + len(marida_df_F))\n#print(f'effective global ratio {effective_ratio}')\nclass_distribution = np.array([1 - effective_ratio, effective_ratio])\nprint(f'class distribution {class_distribution}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:28.533093Z","iopub.execute_input":"2025-05-22T17:51:28.533910Z","iopub.status.idle":"2025-05-22T17:51:28.540115Z","shell.execute_reply.started":"2025-05-22T17:51:28.533876Z","shell.execute_reply":"2025-05-22T17:51:28.539462Z"}},"outputs":[{"name":"stdout","text":"class distribution [0.97977005 0.02022995]\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# MARIDA statistics\n\nclass_distr = np.array([0.00452, 0.00203, 0.00254, 0.00168, 0.00766, 0.15206, 0.20232,\n 0.35941, 0.00109, 0.20218, 0.03226, 0.00693, 0.01322, 0.01158, 0.00052])\n\nbands_mean = np.array([0.05197577, 0.04783991, 0.04056812, 0.03163572, 0.02972606, 0.03457443,\n 0.03875053, 0.03436435, 0.0392113,  0.02358126, 0.01588816]).astype(np.float32)\n\nbands_std = np.array([0.04725893, 0.04743808, 0.04699043, 0.04967381, 0.04946782, 0.06458357,\n 0.07594915, 0.07120246, 0.08251058, 0.05111466, 0.03524419]).astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:29.610792Z","iopub.execute_input":"2025-05-22T17:51:29.611391Z","iopub.status.idle":"2025-05-22T17:51:29.616091Z","shell.execute_reply.started":"2025-05-22T17:51:29.611369Z","shell.execute_reply":"2025-05-22T17:51:29.615454Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Other code references  \n# https://github.com/MarcCoru/marinedebrisdetector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:29.864563Z","iopub.execute_input":"2025-05-22T17:51:29.864971Z","iopub.status.idle":"2025-05-22T17:51:29.868328Z","shell.execute_reply.started":"2025-05-22T17:51:29.864944Z","shell.execute_reply":"2025-05-22T17:51:29.867429Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# MARIDA CLASSES\n# {\n#  1: \"Marine Debris\",\n#  2: \"Dense Sargassum\", \n#  3: \"Sparse Sargassum\", \n#  4: \"Natural Organic Material\", \n#  5: \"Ship\", \n#  6: \"Clouds\", \n#  7: \"Marine Water\", \n#  8: \"Sediment-Laden Water\", \n#  9: \"Foam\", \n#  10: \"Turbid Water\", \n#  11: \"Shallow Water\", \n#  12: \"Waves\", \n#  13: \"Cloud Shadows\", \n#  14: \"Wakes\", \n#  15: \"Mixed Water\"\n# }\n\n\n# From marinedebrisdetector \n# DEBRIS_CLASSES = [1,2,3,4,9]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:30.034444Z","iopub.execute_input":"2025-05-22T17:51:30.034797Z","iopub.status.idle":"2025-05-22T17:51:30.037927Z","shell.execute_reply.started":"2025-05-22T17:51:30.034780Z","shell.execute_reply":"2025-05-22T17:51:30.037263Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# https://drive.google.com/drive/folders/1rntiw5BvOs80eIbpOu7dk9g1BfOVw61-?usp=drive_link","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:30.266747Z","iopub.execute_input":"2025-05-22T17:51:30.267370Z","iopub.status.idle":"2025-05-22T17:51:30.270310Z","shell.execute_reply.started":"2025-05-22T17:51:30.267352Z","shell.execute_reply":"2025-05-22T17:51:30.269634Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"\nclass RandomRotationTransform:\n    \"\"\"Rotate by one of the given angles.\"\"\"\n\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return vF.rotate(x, angle)\n    \ndef gen_weights(class_distribution, c = 1.02):\n    return 1/torch.log(c + class_distribution)\n    \ntransformTrain = transforms.Compose([transforms.ToTensor(),\n                                    RandomRotationTransform([-90, 0, 90, 180]),\n                                    transforms.RandomHorizontalFlip()])\n    \ntransformTest = transforms.Compose([transforms.ToTensor()])\n    \nstandardization = transforms.Normalize(global_bands_mean, global_bands_std) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:30.468791Z","iopub.execute_input":"2025-05-22T17:51:30.468984Z","iopub.status.idle":"2025-05-22T17:51:30.474358Z","shell.execute_reply.started":"2025-05-22T17:51:30.468970Z","shell.execute_reply":"2025-05-22T17:51:30.473649Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"def gen_weights(class_distribution, c = 1.02):\n    return 1/torch.log(c + class_distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.973107Z","iopub.execute_input":"2025-05-22T16:04:13.973356Z","iopub.status.idle":"2025-05-22T16:04:13.984854Z","shell.execute_reply.started":"2025-05-22T16:04:13.973335Z","shell.execute_reply":"2025-05-22T16:04:13.984156Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import os\nimport argparse\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torchvision import transforms\n# model import UNet, AttentionUNet, ResidualAttentionUNet  # From original script\n#f#rom dataloader import bands_mean, bands_std, RandomRotationTransform, class_distr, gen_weights\n#from metrics import Evaluation\n#from customLosses import FocalLoss\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass MergedSegmentationDataset(Dataset):\n    \"\"\"\n    df_dataset1 : MARIDA dataset\n    df_dataset2 : LR dataset\n    \"\"\"\n    def __init__(self, df_dataset1, df_dataset2, bands_mean, bands_std, transform=None, standardization=None):\n        \"\"\"\n        df_dataset1 : MARIDA\n        df_dataset2 : Litter Windrows\n        \"\"\"\n        self.bands_mean = bands_mean\n        self.bands_std = bands_std\n        self.transform = transform\n        self.standardization = standardization\n        self.image_paths = []\n        self.mask_paths = []\n        self.dataset_ids = []\n        self.image_paths = df_dataset1['image'].tolist() + df_dataset2['image'].tolist() \n        self.mask_paths =  df_dataset1['mask'].tolist() + df_dataset2['mask'].tolist() \n        self.dataset_ids = [0] * len(df_dataset1['image']) + [1] * len(df_dataset2['image'])\n        # Generate shuffled indices\n        indices = np.random.permutation(len(self.image_paths))\n        self.image_paths = np.array(self.image_paths)[indices]\n        self.mask_paths = np.array(self.mask_paths)[indices]\n        self.dataset_ids = np.array(self.dataset_ids)[indices]        \n        #print(self.dataset_ids)\n        if self.transform is None:\n            self.transform = transforms.Compose([transforms.ToTensor()])\n        ## preloading images in memory \n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        #print(f'idx {idx}') \n        max_seed = 2**32 - 1  # NumPy seed limit\n        #index_seed_seed = (42 + idx) % max_seed\n        #np.random.seed(index_seed_seed)\n        # Load Classsification Mask np.random.seed(self.seed + index)  # Deterministic per item\n        dataset_id = self.dataset_ids[idx]\n        # Open t#he GeoTIFF image file\n        #print(f'image path {self.image_paths[idx]}')\n        #print(f'mask path {self.mask_paths[idx]}')\n        with rasterio.open(self.image_paths[idx]) as src:\n            #print(f#\"Number of bands: {dataset.count}\")  # Check the number of bands\n            # Read all bands as a NumPy array\n            image = src.read()\n            #print(f'image shape {image.shape}')\n            invalid_mask = get_invalid_mask(image, src.nodata)\n            #print(bands.shape)  # Shape will be (bands, height, width)\n            #print(f'invalid mask shape {invalid_mask.shape}')\n            with rasterio.open(self.mask_paths[idx]) as src_mask:\n                mask = src_mask.read().astype(int)\n            # if dataset_id == 0: #MARIDA\n            #     #print(f'sample from marida')\n            #     temp = mask.copy()\n            #     #assimilate several classes to marine debris\n            #     temp[temp==1]=2\n            #     temp[temp==2]=2          \n            #     temp[temp==3]=2          \n            #     temp[temp==4]=2          \n            #     temp[temp==9]=2          \n            #     # Leaving unlabeled pixels to 0 and pixels in classes not in [1,2,3, 4,9] to 1\n            #     temp[(temp != 0) & (temp != 2)] = 1\n                \n            #     # Categories from 1 to 0\n            #     mask = np.copy(temp)\n            # else : #LR\n            #     #print('sample from litter rows')\n            #     bg_mask = select_bg_pixels(image, mask[0], target_ratio=40)\n            #     #print(f'bg mask shape {bg_mask.shape}')\n            #     mask[mask==1] = 2\n            #     mask[bg_mask[None,...].astype(bool)] = 1\n            #print(f'mask before inputing {mask.shape}')\n            debris_before_invalid = np.sum(mask)\n            invalid_pixels = np.sum(np.any(invalid_mask, axis=0))\n            mask[np.any(invalid_mask.astype(bool), axis=0, keepdims=True)] = 0 #I guess it makes sense not to feed invalid pixels to the loss function\n            #print(f'before inputing 2')\n            image[invalid_mask.astype(bool)] = np.tile(self.bands_mean[:, np.newaxis, np.newaxis], (1, 256, 256))[invalid_mask.astype(bool)]\n            #print(f'after inputing')\n            ## Since the model sees unvalid pixels anyway, it's better (?) to replace those with mean values ? \n            #print(f'mask type before transh {type(mask)} - {mask.dtype}')\n            #print(f'image type before transh {type(image)} - {image.dtype}')\n            #############\n            debris_after_invalid = np.sum(mask)\n            #############\n            if self.transform is not None:\n                # applying the same rotation on the image-mask pair\n                #print(f'transform - image shape {image.shape}')\n                #print(f'transform - mask shape {mask.shape}')\n                stack = np.concatenate([image, mask], axis=0).astype(np.float32) \n                stack = np.transpose(stack,(1, 2, 0)) #to channel last\n                #print(f'stack shape before transfrom {stack.shape}')\n                stack = self.transform(stack) #expects channel last, returns channel first\n               \n                #print(f'stack shape after transfrom {stack.shape}')\n                image = stack[:-1,:,:]\n                mask = stack[-1,:,:].long()\n                #print(f'image type {image.dtype}')\n                #print(f'image shape after transform {image.shape}')\n                #print(f'mask shape after transform {mask.shape}')\n\n                   \n            \n            if self.standardization is not None:\n                image = self.standardization(image)\n                \n            #mask = mask - 1 Moved to collate function\n            if isinstance(mask, np.ndarray):\n                mask = torch.from_numpy(mask).to(torch.long)\n            else:\n                mask = mask.to(torch.long)\n            if isinstance(image, np.ndarray):\n                image = torch.from_numpy(image).to(torch.float32)\n            else:\n                im = image.to(torch.float32)\n            if torch.sum(mask) == 0 :\n                print(f'{self.mask_paths[idx]} has no debris pixels')\n                print(f'debris pixels before invalid mask : {debris_before_invalid}')\n                print(f'debris pixels after invalid mask : {debris_after_invalid}')\n                print(f'invalid pixels : {invalid_pixels}')\n           \n        ## Add logic for transform\n\n            return image, mask, dataset_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:13.985622Z","iopub.execute_input":"2025-05-22T16:04:13.985894Z","iopub.status.idle":"2025-05-22T16:04:18.133772Z","shell.execute_reply.started":"2025-05-22T16:04:13.985873Z","shell.execute_reply":"2025-05-22T16:04:18.133209Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def conv3x3(in_channels, out_channels, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, channels, ratio=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.mlp = nn.Sequential(nn.Conv2d(channels, channels // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(channels // 16, channels, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.mlp(self.avg_pool(x))\n        max_out = self.mlp(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, inputChannel, outputChannel, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(inputChannel, outputChannel, stride)\n        self.bn1 = nn.BatchNorm2d(outputChannel)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outputChannel, outputChannel)\n        self.bn2 = nn.BatchNorm2d(outputChannel)\n        self.downsample = downsample\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n        \n    # def forward(self, x):\n    #     residual = x\n    #     out = self.conv1(x)\n    #     out = self.bn1(out)\n    #     out = self.relu(out)\n    #     out = self.conv2(out)\n    #     out = self.bn2(out)\n    #     if self.downsample:\n    #         residual = self.downsample(x)\n    #     out += residual\n    #     out = self.relu(out)\n    #     caOutput = self.ca(out)\n    #     out = caOutput * out\n    #     saOutput = self.sa(out)\n    #     out = saOutput * out\n    #     return out, saOutput\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.bn2(out)\n        out = self.relu(out)\n        caOutput = self.ca(out)\n        out = caOutput * out\n        saOutput = self.sa(out)\n        out = saOutput * out\n        return out, saOutput\n\n\nclass DownSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.AvgPool2d(2)\n        )\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self,x):\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\n    \nclass UpSampleWithAttention(nn.Module):\n    def __init__(self, inputChannel, outputChannel):\n        super().__init__()\n        self.convolution = nn.Sequential(\n            nn.Conv2d(inputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(outputChannel, outputChannel, kernel_size=3, padding=1),\n            nn.BatchNorm2d(outputChannel),\n            nn.LeakyReLU(0.2)\n        )\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.ca = ChannelAttention(outputChannel)\n        self.sa = SpatialAttention()\n    \n    def forward(self, x):\n        x = self.upsample(x)\n        x = self.convolution(x)\n        caOutput = self.ca(x)\n        x = caOutput * x\n        saOutput = self.sa(x)\n        x = saOutput * x\n        return x, saOutput\n\nclass ResidualAttentionUNet(nn.Module):\n  def __init__(self, inputChannel, outputChannel):\n    super().__init__()\n    self.downsample1 = DownSampleWithAttention(inputChannel, 32)\n    self.downsample2 = DownSampleWithAttention(32, 64)\n    self.downsample3 = DownSampleWithAttention(64, 128)\n    self.downsample4 = DownSampleWithAttention(128, 256)\n    self.downsample5 = DownSampleWithAttention(256, 512)\n\n    self.residualBlock1 = ResidualBlock(512, 512)\n    self.residualBlock2 = ResidualBlock(512, 512)\n    self.residualBlock3 = ResidualBlock(512, 512)\n\n    self.upsample1 = UpSampleWithAttention(512, 256)\n    self.upsample2 = UpSampleWithAttention(512, 128)\n    self.upsample3 = UpSampleWithAttention(256, 64)\n    self.upsample4 = UpSampleWithAttention(128, 32)\n    self.upsample5 = UpSampleWithAttention(64, 32)\n    self.classification = nn.Sequential(\n            nn.Conv2d(32, outputChannel, kernel_size=1),\n        )\n\n  def forward(self, x):\n    scale128, sa128down = self.downsample1(x)\n    scale64, sa64down = self.downsample2(scale128)\n    scale32, sa32down = self.downsample3(scale64)\n    scale16, sa64down = self.downsample4(scale32)\n    scale8, sa8down = self.downsample5(scale16)\n    scale8, sa8down = self.residualBlock1(scale8)\n    scale8, sa8down = self.residualBlock2(scale8)\n    scale8, sa8down = self.residualBlock3(scale8)\n    upscale16, sa16up = self.upsample1(scale8)\n    upscale16 = torch.cat([upscale16, scale16], dim=1)\n    upscale32, sa32up = self.upsample2(upscale16)\n    upscale32 = torch.cat([upscale32, scale32], dim=1)\n    upscale64, sa64up = self.upsample3(upscale32)\n    upscale64 = torch.cat([upscale64, scale64], dim=1)\n    upscale128, sa128up = self.upsample4(upscale64)\n    upscale128 = torch.cat([upscale128, scale128], dim=1)\n    upscale256, sa256up = self.upsample5(upscale128)\n    finaloutput = self.classification(upscale256)\n    return finaloutput","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.134620Z","iopub.execute_input":"2025-05-22T16:04:18.135043Z","iopub.status.idle":"2025-05-22T16:04:18.153682Z","shell.execute_reply.started":"2025-05-22T16:04:18.135019Z","shell.execute_reply":"2025-05-22T16:04:18.152973Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def Evaluation(y_predicted, y_true):\n\n    micro_prec = precision_score(y_true, y_predicted, average='micro')\n    macro_prec = precision_score(y_true, y_predicted, average='macro')\n    weight_prec = precision_score(y_true, y_predicted, average='weighted')\n    \n    micro_rec = recall_score(y_true, y_predicted, average='micro')\n    macro_rec = recall_score(y_true, y_predicted, average='macro')\n    weight_rec = recall_score(y_true, y_predicted, average='weighted')\n        \n    macro_f1 = f1_score(y_true, y_predicted, average=\"macro\")\n    micro_f1 = f1_score(y_true, y_predicted, average=\"micro\")\n    weight_f1 = f1_score(y_true, y_predicted, average=\"weighted\")\n        \n    subset_acc = accuracy_score(y_true, y_predicted)\n    \n    iou_acc = jaccard_score(y_true, y_predicted, average='macro')\n\n    # Debris-specific metrics\n    debris_class = 1\n    debris_prec = precision_score(y_true, y_predicted, labels=[debris_class], average='macro')\n    debris_rec = recall_score(y_true, y_predicted, labels=[debris_class], average='macro')\n    debris_f1 = f1_score(y_true, y_predicted, labels=[debris_class], average='macro')\n    debris_iou = jaccard_score(y_true, y_predicted, labels=[debris_class], average='macro')\n\n    info = {\n            \"macroPrec\" : macro_prec,\n            \"microPrec\" : micro_prec,\n            \"weightPrec\" : weight_prec,\n            \"macroRec\" : macro_rec,\n            \"microRec\" : micro_rec,\n            \"weightRec\" : weight_rec,\n            \"macroF1\" : macro_f1,\n            \"microF1\" : micro_f1,\n            \"weightF1\" : weight_f1,\n            \"subsetAcc\" : subset_acc,\n            \"IoU\": iou_acc,\n            \"debris Prec\" : debris_prec,\n            \"debris Rec\" : debris_rec,\n            \"debris F1\" : debris_f1,\n            \"debris IoU\" : debris_iou\n            }\n    \n    return info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.154446Z","iopub.execute_input":"2025-05-22T16:04:18.154687Z","iopub.status.idle":"2025-05-22T16:04:18.193837Z","shell.execute_reply.started":"2025-05-22T16:04:18.154666Z","shell.execute_reply":"2025-05-22T16:04:18.193216Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"transformTrain = transforms.Compose([transforms.ToTensor(),\n                                    RandomRotationTransform([-90, 0, 90, 180]),\n                                    transforms.RandomHorizontalFlip()])\n    \ntransformTest = transforms.Compose([transforms.ToTensor()])\n    \nstandardization = transforms.Normalize(global_bands_mean.tolist(), global_bands_std.tolist())\nmerged_ds = MergedSegmentationDataset(marida_df_F, lr_df_F, global_bands_mean, global_bands_std, transform=transformTrain, standardization= standardization)\nval_ds = MergedSegmentationDataset(marida_val_df_F, lr_val_df_F, global_bands_mean, global_bands_std, transform=transformTest, standardization= standardization )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.194548Z","iopub.execute_input":"2025-05-22T16:04:18.194809Z","iopub.status.idle":"2025-05-22T16:04:18.214642Z","shell.execute_reply.started":"2025-05-22T16:04:18.194793Z","shell.execute_reply":"2025-05-22T16:04:18.214091Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"\ntrainLoader = DataLoader(merged_ds,\n                        batch_size=batch_size, \n                        shuffle=True,  \n                        #num_workers=2, \n                        #pin_memory=True,\n                        #prefetch_factor=2,\n                        collate_fn=custom_collate_fn\n                        # worker_init_fn=worker_init_fn,\n                        # generator=torch.Generator().manual_seed(seed) \n                        )\n\n\ntestLoader = DataLoader(val_ds, \n                        batch_size=batch_size, \n                        shuffle=False,\n                        collate_fn=custom_collate_fn\n                        # worker_init_fn=worker_init_fn,\n                        # generator=torch.Generator().manual_seed(seed) \n                        )\n                        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.215277Z","iopub.execute_input":"2025-05-22T16:04:18.215469Z","iopub.status.idle":"2025-05-22T16:04:18.230618Z","shell.execute_reply.started":"2025-05-22T16:04:18.215454Z","shell.execute_reply":"2025-05-22T16:04:18.230057Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"model = ResidualAttentionUNet(11, 2).to(device)\nweight = gen_weights(torch.from_numpy(class_distribution), c = 1.03).to(device)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean', weight=weight.to(torch.float32))\n#optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=1e-2)\noptimizer = torch.optim.AdamW(model.parameters(), lr=8e-4, weight_decay=1e-4)\n\n\n# assuming about 40 reductions => .9 ** 40 = 1e-2, starting from 8e-4 ending with 8e-6\n#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.231316Z","iopub.execute_input":"2025-05-22T16:04:18.231888Z","iopub.status.idle":"2025-05-22T16:04:18.709659Z","shell.execute_reply.started":"2025-05-22T16:04:18.231863Z","shell.execute_reply":"2025-05-22T16:04:18.709126Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"best_metric = -float('inf')  # Initialize to negative infinity (for maximization, e.g., accuracy)\nbest_model_path = '/kaggle/working/best_model.pth'\noutput_classes = 2\nmetrics_history = []\npatience = 10  # Number of epochs to wait for improvement\nepochs_no_improve = 0  # Counter for epochs without improvement\nepochs = 30\nfor epoch in range(1, epochs+1):\n    model.train()\n    pb = tqdm(trainLoader, desc=f\"epoch {epoch}/{epochs}: \")\n    yTrue = []\n    yPredicted = []\n\n    bg_yTrue = []\n    bg_yPredicted = []\n    for image, target, _ in pb:\n        image, target = image.to(device), target.to(device)\n        optimizer.zero_grad()\n\n        logits = model(image)\n        # print(f'logits shape : {logits.shape}')\n        # print(f'target shape : {target.shape}')\n        # print(f'image dtype {image.dtype}')\n        # print(f'logits dtype {logits.dtype}')\n        # print(f'target dtype {target.dtype}')\n        loss = criterion(logits, target)\n\n        loss.backward()\n        optimizer.step()\n        pb.set_postfix(loss=loss.item())\n\n        if epoch % 10 == 0:\n            with torch.no_grad():\n                logits = logits.detach()\n                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n                logits = logits.reshape((-1,output_classes))\n                target = target.reshape(-1)\n                ###################################################################################\n                mask = target != -1\n                ###################################################################################\n                \n                # bg_logits = logits[~mask]\n                # bg_target = target[~mask]\n    \n                # only considering annotated pixels\n                logits = logits[mask]\n                target = target[mask]\n    \n                probs = F.softmax(logits, dim=1).cpu().numpy()\n                target = target.cpu().numpy()\n                yPredicted += probs.argmax(1).tolist()\n                yTrue += target.tolist()\n        \n                \n                # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n                # bg_target = bg_target.cpu().numpy()\n                \n                # bg_yPredicted += bg_probs.argmax(1).tolist()\n                # bg_yTrue += bg_target.tolist()\n\n\n    if epoch % 10 == 0:\n        yPredicted = np.asarray(yPredicted)\n        yTrue = np.asarray(yTrue)\n        acc = Evaluation(yPredicted, yTrue)\n        print(acc)\n    \n        # bg_yPredicted = np.asarray(bg_yPredicted)\n        # bg_yTrue = np.asarray(bg_yTrue)\n        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n        # print(\"background:\", bg_acc)\n\n\n    model.eval()\n    yTrue = []\n    yPredicted = []\n    testLossF = []\n    \n    # bg_yTrue = []\n    # bg_yPredicted = []\n    with torch.no_grad():\n        for image, target, _ in testLoader:\n\n            image, target = image.to(device), target.to(device)\n            logits = model(image)\n            # print(f'image dtype {image.dtype}')\n            # print(f'logits dtype {logits.dtype}')\n            # print(f'target dtype {target.dtype}')\n            # print(f'test - target shape {target.shape}')\n            # print(f'test - logit shape {logits.shape}')\n            loss = criterion(logits, target)\n\n            logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n            logits = logits.reshape((-1,output_classes))\n            target = target.reshape(-1)\n            ###################################################################################\n            mask = target != -1\n            ###################################################################################\n            \n            # bg_logits = logits[~mask]\n            # bg_target = target[~mask]\n            \n            logits = logits[mask]\n            target = target[mask]\n            \n\n            probs = F.softmax(logits, dim=1).cpu().numpy()\n            target = target.cpu().numpy()\n            # testBatches += target.shape[0]\n            testLossF.append((loss.data*target.shape[0]).tolist())\n            yPredicted += probs.argmax(1).tolist()\n            yTrue += target.tolist()\n\n\n            # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n            # bg_target = bg_target.cpu().numpy()\n\n            # bg_yPredicted += bg_probs.argmax(1).tolist()\n            # bg_yTrue += bg_target.tolist()\n        \n        yPredicted = np.asarray(yPredicted)\n        yTrue = np.asarray(yTrue)\n        print('########### Validation Set Evaluation : #############')\n        acc = Evaluation(yPredicted, yTrue)\n        metrics_history.append(acc)\n        if acc['debris IoU'] > best_metric:\n            best_metric = acc['debris IoU']\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"Saved best model with validation metric: {best_metric}\")\n            epochs_no_improve = 0  # Reset counter\n        else:\n            epochs_no_improve += 1\n            print(f\"No improvement for {epochs_no_improve}/{patience} epochs\")\n        # bg_yPredicted = np.asarray(bg_yPredicted)\n        # bg_yTrue = np.asarray(bg_yTrue)\n        # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n        print(acc)\n        # Early stopping check\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered at epoch {epoch}\")\n            break\n        # print(\"background:\", bg_acc)\n    scheduler.step(sum(testLossF) / len(testLoader.dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T16:04:18.710319Z","iopub.execute_input":"2025-05-22T16:04:18.710493Z","iopub.status.idle":"2025-05-22T17:42:56.924794Z","shell.execute_reply.started":"2025-05-22T16:04:18.710480Z","shell.execute_reply":"2025-05-22T17:42:56.923967Z"}},"outputs":[{"name":"stderr","text":"epoch 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [04:01<00:00,  2.01s/it, loss=0.223]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.616450092817364\n{'macroPrec': 0.8400133134213373, 'microPrec': 0.9826431594705496, 'weightPrec': 0.9852306703200893, 'macroRec': 0.9237429451689239, 'microRec': 0.9826431594705496, 'weightRec': 0.9826431594705496, 'macroF1': 0.8768564834995111, 'microF1': 0.9826431594705496, 'weightF1': 0.9835940951371411, 'subsetAcc': 0.9826431594705496, 'IoU': 0.7992975828807718, 'debris Prec': 0.6847308871735223, 'debris Rec': 0.8607603349727503, 'debris F1': 0.7627208480565371, 'debris IoU': 0.616450092817364}\n","output_type":"stream"},{"name":"stderr","text":"epoch 2/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.31s/it, loss=0.22] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.644384894049923\n{'macroPrec': 0.8565902610836209, 'microPrec': 0.9845645173449936, 'weightPrec': 0.9863845425090869, 'macroRec': 0.9258278256047474, 'microRec': 0.9845645173449936, 'weightRec': 0.9845645173449936, 'macroF1': 0.8878681898831984, 'microF1': 0.9845645173449937, 'weightF1': 0.9852472606729231, 'subsetAcc': 0.9845645173449936, 'IoU': 0.8142526327231023, 'debris Prec': 0.7177998894416805, 'debris Rec': 0.863020071779875, 'debris F1': 0.7837397392563977, 'debris IoU': 0.644384894049923}\n","output_type":"stream"},{"name":"stderr","text":"epoch 3/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.30s/it, loss=0.185] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.6662938036706072, 'microPrec': 0.9374912494211155, 'weightPrec': 0.9763819934764805, 'macroRec': 0.9388886118541185, 'microRec': 0.9374912494211155, 'weightRec': 0.9374912494211155, 'macroF1': 0.7301955607977086, 'microF1': 0.9374912494211155, 'weightF1': 0.9513603063475217, 'subsetAcc': 0.9374912494211155, 'IoU': 0.6316423216792522, 'debris Prec': 0.3347132853898562, 'debris Rec': 0.9403828260002659, 'debris F1': 0.4937018039708294, 'debris IoU': 0.3277583451087586}\n","output_type":"stream"},{"name":"stderr","text":"epoch 4/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:38<00:00,  1.32s/it, loss=0.172] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.6459120138818005\n{'macroPrec': 0.8651483637805767, 'microPrec': 0.985055626756847, 'weightPrec': 0.9862595894389306, 'macroRec': 0.9155146659027233, 'microRec': 0.985055626756847, 'weightRec': 0.985055626756847, 'macroF1': 0.8885635798858893, 'microF1': 0.985055626756847, 'weightF1': 0.9855376346899671, 'subsetAcc': 0.985055626756847, 'IoU': 0.8152744133146965, 'debris Prec': 0.7356428737502906, 'debris Rec': 0.841153795028579, 'debris F1': 0.7848682170542636, 'debris IoU': 0.6459120138818005}\n","output_type":"stream"},{"name":"stderr","text":"epoch 5/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:37<00:00,  1.31s/it, loss=0.0833]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.6678614097968937\n{'macroPrec': 0.8615914690621342, 'microPrec': 0.9856285877373426, 'weightPrec': 0.9876092852581391, 'macroRec': 0.9402206924521926, 'microRec': 0.9856285877373426, 'weightRec': 0.9856285877373426, 'macroF1': 0.8967024501130747, 'microF1': 0.9856285877373426, 'weightF1': 0.9863329760767173, 'subsetAcc': 0.9856285877373426, 'IoU': 0.8265311675913372, 'debris Prec': 0.7268393108679163, 'debris Rec': 0.8916655589525455, 'debris F1': 0.8008595988538681, 'debris IoU': 0.6678614097968937}\n","output_type":"stream"},{"name":"stderr","text":"epoch 6/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.30s/it, loss=0.18]  \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.8364341746051973, 'microPrec': 0.9830071835521427, 'weightPrec': 0.9866845123169737, 'macroRec': 0.9498505668895196, 'microRec': 0.9830071835521427, 'weightRec': 0.9830071835521427, 'macroF1': 0.8841725683709134, 'microF1': 0.9830071835521427, 'weightF1': 0.9842316403796064, 'subsetAcc': 0.9830071835521427, 'IoU': 0.8090248696721809, 'debris Prec': 0.6757699297607938, 'debris Rec': 0.9143958527183305, 'debris F1': 0.777178364637763, 'debris IoU': 0.6355615096780154}\n","output_type":"stream"},{"name":"stderr","text":"epoch 7/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.30s/it, loss=0.0526]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 2/10 epochs\n{'macroPrec': 0.8575079399490495, 'microPrec': 0.985217175905483, 'weightPrec': 0.9874139778678797, 'macroRec': 0.9411001206375048, 'microRec': 0.985217175905483, 'weightRec': 0.985217175905483, 'macroF1': 0.8945299118516634, 'microF1': 0.985217175905483, 'weightF1': 0.9859905072248608, 'subsetAcc': 0.985217175905483, 'IoU': 0.8234568350486304, 'debris Prec': 0.718598065929369, 'debris Rec': 0.8939252957596704, 'debris F1': 0.7967301483872878, 'debris IoU': 0.662137547383449}\n","output_type":"stream"},{"name":"stderr","text":"epoch 8/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:35<00:00,  1.30s/it, loss=0.0731]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 3/10 epochs\n{'macroPrec': 0.8497545854009785, 'microPrec': 0.9848811536763201, 'weightPrec': 0.9879528414311287, 'macroRec': 0.9569535417289473, 'microRec': 0.9848811536763201, 'weightRec': 0.9848811536763201, 'macroF1': 0.8955628164723883, 'microF1': 0.9848811536763201, 'weightF1': 0.9858849338611837, 'subsetAcc': 0.9848811536763201, 'IoU': 0.8248323168944935, 'debris Prec': 0.701977756529616, 'debris Rec': 0.9270902565465905, 'debris F1': 0.7989804393275483, 'debris IoU': 0.6652518122853872}\n","output_type":"stream"},{"name":"stderr","text":"epoch 9/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.30s/it, loss=0.0935]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 4/10 epochs\n{'macroPrec': 0.8537937397048033, 'microPrec': 0.9850707046773863, 'weightPrec': 0.9876713964850358, 'macroRec': 0.9487007191216188, 'microRec': 0.9850707046773863, 'weightRec': 0.9850707046773863, 'macroF1': 0.8951159913935128, 'microF1': 0.9850707046773863, 'weightF1': 0.9859529992583758, 'subsetAcc': 0.9850707046773863, 'IoU': 0.8242435775563989, 'debris Prec': 0.7106369724342003, 'debris Rec': 0.9098099162568124, 'debris F1': 0.7979830365210295, 'debris IoU': 0.6638700290979631}\n","output_type":"stream"},{"name":"stderr","text":"epoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:36<00:00,  1.30s/it, loss=0.271] \n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.834692408636926, 'microPrec': 0.9788274111932933, 'weightPrec': 0.9843855039353927, 'macroRec': 0.9649072353794433, 'microRec': 0.9788274111932933, 'weightRec': 0.9788274111932933, 'macroF1': 0.8878296010265232, 'microF1': 0.9788274111932933, 'weightF1': 0.9805487382955257, 'subsetAcc': 0.9788274111932933, 'IoU': 0.8132487692535135, 'debris Prec': 0.671580004219747, 'debris Rec': 0.9497391787474406, 'debris F1': 0.7867986123541395, 'debris IoU': 0.6485309202298818}\n########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.6869486313454656\n{'macroPrec': 0.868345629353332, 'microPrec': 0.9866237304929403, 'weightPrec': 0.9884899730186247, 'macroRec': 0.9475119028158101, 'microRec': 0.9866237304929403, 'weightRec': 0.9866237304929403, 'macroF1': 0.9037446267658018, 'microF1': 0.9866237304929403, 'weightF1': 0.9872724634928588, 'subsetAcc': 0.9866237304929403, 'IoU': 0.8365839315865242, 'debris Prec': 0.7398740362688674, 'debris Rec': 0.9056892197261731, 'debris F1': 0.8144274444178821, 'debris IoU': 0.6869486313454656}\n","output_type":"stream"},{"name":"stderr","text":"epoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.125] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.8558061354608264, 'microPrec': 0.9852344078146709, 'weightPrec': 0.9876770524303308, 'macroRec': 0.9468582109021543, 'microRec': 0.9852344078146709, 'weightRec': 0.9852344078146709, 'macroF1': 0.8956929168798191, 'microF1': 0.9852344078146709, 'weightF1': 0.9860714484548526, 'subsetAcc': 0.9852344078146709, 'IoU': 0.8250681966104662, 'debris Prec': 0.7147951958881837, 'debris Rec': 0.9058221454207098, 'debris F1': 0.7990502154603817, 'debris IoU': 0.6653485647334505}\n","output_type":"stream"},{"name":"stderr","text":"epoch 12/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:35<00:00,  1.29s/it, loss=0.298] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 2/10 epochs\n{'macroPrec': 0.8630617350990786, 'microPrec': 0.9861670848994626, 'weightPrec': 0.9883463339148244, 'macroRec': 0.9503592946525985, 'microRec': 0.9861670848994626, 'weightRec': 0.9861670848994626, 'macroF1': 0.9016015953667578, 'microF1': 0.9861670848994626, 'weightF1': 0.9869090201541878, 'subsetAcc': 0.9861670848994626, 'IoU': 0.8334784796279022, 'debris Prec': 0.7290936138561258, 'debris Rec': 0.9120696530639373, 'debris F1': 0.8103814810440534, 'debris IoU': 0.681211218664681}\n","output_type":"stream"},{"name":"stderr","text":"epoch 13/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:35<00:00,  1.30s/it, loss=0.0917]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 3/10 epochs\n{'macroPrec': 0.8504379087364335, 'microPrec': 0.9838817029434255, 'weightPrec': 0.9859764545515062, 'macroRec': 0.9253786280959311, 'microRec': 0.9838817029434255, 'weightRec': 0.9838817029434255, 'macroF1': 0.8839563020075187, 'microF1': 0.9838817029434255, 'weightF1': 0.9846598911853801, 'subsetAcc': 0.9838817029434255, 'IoU': 0.8088846756163097, 'debris Prec': 0.705505135590457, 'debris Rec': 0.8628206832380699, 'debris F1': 0.7762729093790175, 'debris IoU': 0.6343513315416565}\n","output_type":"stream"},{"name":"stderr","text":"epoch 14/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.0801]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 4/10 epochs\n{'macroPrec': 0.8245791379042843, 'microPrec': 0.9816544786808974, 'weightPrec': 0.9865129057223498, 'macroRec': 0.958337410228983, 'microRec': 0.9816544786808974, 'weightRec': 0.9816544786808974, 'macroF1': 0.8788888223112507, 'microF1': 0.9816544786808974, 'weightF1': 0.9832195733142086, 'subsetAcc': 0.9816544786808974, 'IoU': 0.8017861707631653, 'debris Prec': 0.6514216800408182, 'debris Rec': 0.9334042270370863, 'debris F1': 0.7673268679142194, 'debris IoU': 0.6224901378484996}\n","output_type":"stream"},{"name":"stderr","text":"epoch 15/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:35<00:00,  1.30s/it, loss=0.165] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 5/10 epochs\n{'macroPrec': 0.8873121797239933, 'microPrec': 0.9852365618033193, 'weightPrec': 0.9849819059940088, 'macroRec': 0.8732119336674764, 'microRec': 0.9852365618033193, 'weightRec': 0.9852365618033193, 'macroF1': 0.8801207237915554, 'microF1': 0.9852365618033193, 'weightF1': 0.9850997051309679, 'subsetAcc': 0.9852365618033193, 'IoU': 0.8040333057404057, 'debris Prec': 0.7828729281767955, 'debris Rec': 0.7534228366343214, 'debris F1': 0.767865609970873, 'debris IoU': 0.6231995601979109}\n","output_type":"stream"},{"name":"stderr","text":"epoch 16/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.28s/it, loss=0.0405]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 6/10 epochs\n{'macroPrec': 0.8553230966033201, 'microPrec': 0.9853227213492585, 'weightPrec': 0.9879049594305013, 'macroRec': 0.9508865228189503, 'microRec': 0.9853227213492585, 'weightRec': 0.9853227213492585, 'macroF1': 0.8969182860236495, 'microF1': 0.9853227213492585, 'weightF1': 0.9861920945162644, 'subsetAcc': 0.9853227213492585, 'IoU': 0.8267836083528153, 'debris Prec': 0.713551935249559, 'debris Rec': 0.9140635384819885, 'debris F1': 0.8014568764568765, 'debris IoU': 0.6686925657606846}\n","output_type":"stream"},{"name":"stderr","text":"epoch 17/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.404] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.7048686632089677\n{'macroPrec': 0.9043973532832599, 'microPrec': 0.9886010920722448, 'weightPrec': 0.9887869126482796, 'macroRec': 0.9168007631807211, 'microRec': 0.9886010920722448, 'weightRec': 0.9886010920722448, 'macroF1': 0.910497808712442, 'microF1': 0.9886010920722448, 'weightF1': 0.9886871761412027, 'subsetAcc': 0.9886010920722448, 'IoU': 0.8465753717802084, 'debris Prec': 0.8141587219788714, 'debris Rec': 0.8400239266250166, 'debris F1': 0.8268891069676152, 'debris IoU': 0.7048686632089677}\n","output_type":"stream"},{"name":"stderr","text":"epoch 18/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.053] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.7143919672740796\n{'macroPrec': 0.8885680127594963, 'microPrec': 0.9884201570257725, 'weightPrec': 0.9894321366827438, 'macroRec': 0.9426588971452375, 'microRec': 0.9884201570257725, 'weightRec': 0.9884201570257725, 'macroF1': 0.9137036233128599, 'microF1': 0.9884201570257725, 'weightF1': 0.9887968659634132, 'subsetAcc': 0.9884201570257725, 'IoU': 0.8512333565319308, 'debris Prec': 0.7807129586623316, 'debris Rec': 0.8937259072178653, 'debris F1': 0.8334056399132321, 'debris IoU': 0.7143919672740796}\n","output_type":"stream"},{"name":"stderr","text":"epoch 19/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.104] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.8555766863037002, 'microPrec': 0.9857793669427362, 'weightPrec': 0.988694597159067, 'macroRec': 0.9629741665210089, 'microRec': 0.9857793669427362, 'weightRec': 0.9857793669427362, 'macroF1': 0.9015751154313463, 'microF1': 0.9857793669427362, 'weightF1': 0.9867115947527848, 'subsetAcc': 0.9857793669427362, 'IoU': 0.8333826898452723, 'debris Prec': 0.7132323232323232, 'debris Rec': 0.9385883291240197, 'debris F1': 0.8105377948688514, 'debris IoU': 0.681432155954449}\n","output_type":"stream"},{"name":"stderr","text":"epoch 20/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.0166]\n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.8610746671651757, 'microPrec': 0.9835709920381653, 'weightPrec': 0.9874756679760832, 'macroRec': 0.9783212572484183, 'microRec': 0.9835709920381653, 'weightRec': 0.9835709920381653, 'macroF1': 0.9105118827764619, 'microF1': 0.9835709920381653, 'weightF1': 0.9847171359157504, 'subsetAcc': 0.9835709920381653, 'IoU': 0.8458916635549206, 'debris Prec': 0.7233423881853311, 'debris Rec': 0.9726009074727604, 'debris F1': 0.8296544642308603, 'debris IoU': 0.708897021327654}\n########### Validation Set Evaluation : #############\nNo improvement for 2/10 epochs\n{'macroPrec': 0.8539670716231944, 'microPrec': 0.9857384411584151, 'weightPrec': 0.9888711420991648, 'macroRec': 0.9672568779395799, 'microRec': 0.9857384411584151, 'weightRec': 0.9857384411584151, 'macroF1': 0.9020667483312872, 'microF1': 0.9857384411584151, 'weightF1': 0.9867213527862075, 'subsetAcc': 0.9857384411584151, 'IoU': 0.8340716831297099, 'debris Prec': 0.7097127495395031, 'debris Rec': 0.9474943506579822, 'debris F1': 0.8115447015626334, 'debris IoU': 0.682856732289122}\n","output_type":"stream"},{"name":"stderr","text":"epoch 21/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:35<00:00,  1.29s/it, loss=0.0199]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 3/10 epochs\n{'macroPrec': 0.8698487151520267, 'microPrec': 0.9871536117004663, 'weightPrec': 0.98918765925018, 'macroRec': 0.9570036856257189, 'microRec': 0.9871536117004663, 'weightRec': 0.9871536117004663, 'macroF1': 0.9084213676474041, 'microF1': 0.9871536117004663, 'weightF1': 0.9878303465255184, 'subsetAcc': 0.9871536117004663, 'IoU': 0.8433632590101678, 'debris Prec': 0.7422383441800917, 'debris Rec': 0.9247640568921973, 'debris F1': 0.8235085227272728, 'debris IoU': 0.6999698158768488}\n","output_type":"stream"},{"name":"stderr","text":"epoch 22/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.0434]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 4/10 epochs\n{'macroPrec': 0.8680218405978372, 'microPrec': 0.9869856005858849, 'weightPrec': 0.9891210556815071, 'macroRec': 0.9577198252499968, 'microRec': 0.9869856005858849, 'weightRec': 0.9869856005858849, 'macroF1': 0.9075605340155826, 'microF1': 0.9869856005858849, 'weightF1': 0.9876921179614903, 'subsetAcc': 0.9869856005858849, 'IoU': 0.8420977436581345, 'debris Prec': 0.738529193599661, 'debris Rec': 0.9264256280739067, 'debris F1': 0.821875, 'debris IoU': 0.6976127320954907}\n","output_type":"stream"},{"name":"stderr","text":"epoch 23/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.0846]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 5/10 epochs\n{'macroPrec': 0.8556096285282683, 'microPrec': 0.9856458196465304, 'weightPrec': 0.9884472120139003, 'macroRec': 0.9590830719638306, 'microRec': 0.9856458196465304, 'weightRec': 0.9856458196465304, 'macroF1': 0.900166119852313, 'microF1': 0.9856458196465304, 'weightF1': 0.9865567594271512, 'subsetAcc': 0.9856458196465304, 'IoU': 0.831376493646389, 'debris Prec': 0.7135650224215246, 'debris Rec': 0.9306792502990828, 'debris F1': 0.8077877127199308, 'debris IoU': 0.6775535878453574}\n","output_type":"stream"},{"name":"stderr","text":"epoch 24/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:34<00:00,  1.29s/it, loss=0.0966]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.7334573478607495\n{'macroPrec': 0.8913424918304265, 'microPrec': 0.9891977469278737, 'weightPrec': 0.9903484298819905, 'macroRec': 0.9543984942291994, 'microRec': 0.9891977469278737, 'weightRec': 0.9891977469278737, 'macroF1': 0.9203193200479982, 'microF1': 0.9891977469278737, 'weightF1': 0.9896003515776232, 'subsetAcc': 0.9891977469278737, 'IoU': 0.8611620801529718, 'debris Prec': 0.7854744151630714, 'debris Rec': 0.9171872923036023, 'debris F1': 0.8462363942971026, 'debris IoU': 0.7334573478607495}\n","output_type":"stream"},{"name":"stderr","text":"epoch 25/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.035] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.7370568597318637\n{'macroPrec': 0.8908831335116033, 'microPrec': 0.9893119083262432, 'weightPrec': 0.9905459781404014, 'macroRec': 0.9579583876176725, 'microRec': 0.9893119083262432, 'weightRec': 0.9893119083262432, 'macroF1': 0.9215437979910204, 'microF1': 0.9893119083262432, 'weightF1': 0.9897340896358927, 'subsetAcc': 0.9893119083262432, 'IoU': 0.8630193324164153, 'debris Prec': 0.7843126198263223, 'debris Rec': 0.9244317426558554, 'debris F1': 0.84862721171446, 'debris IoU': 0.7370568597318637}\n","output_type":"stream"},{"name":"stderr","text":"epoch 26/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:32<00:00,  1.27s/it, loss=0.0578]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.8900423666550089, 'microPrec': 0.9876533370669136, 'weightPrec': 0.988237764825318, 'macroRec': 0.9223171459921236, 'microRec': 0.9876533370669136, 'weightRec': 0.9876533370669136, 'macroF1': 0.905484935938538, 'microF1': 0.9876533370669136, 'weightF1': 0.9878985869779908, 'subsetAcc': 0.9876533370669136, 'IoU': 0.8392165267266614, 'debris Prec': 0.7850410086913943, 'debris Rec': 0.8524524790642031, 'debris F1': 0.8173591639051746, 'debris IoU': 0.6911305097532061}\n","output_type":"stream"},{"name":"stderr","text":"epoch 27/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.0185]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 2/10 epochs\n{'macroPrec': 0.8724639176731644, 'microPrec': 0.987937663568513, 'weightPrec': 0.9901639064962032, 'macroRec': 0.9697744093045113, 'microRec': 0.987937663568513, 'weightRec': 0.987937663568513, 'macroF1': 0.9149929685038716, 'microF1': 0.987937663568513, 'weightF1': 0.9886341082675538, 'subsetAcc': 0.987937663568513, 'IoU': 0.8530665808514402, 'debris Prec': 0.7466060985797828, 'debris Rec': 0.9503522530905224, 'debris F1': 0.8362477337856015, 'debris IoU': 0.718578823056435}\n","output_type":"stream"},{"name":"stderr","text":"epoch 28/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:32<00:00,  1.27s/it, loss=0.057] \n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nNo improvement for 3/10 epochs\n{'macroPrec': 0.8716331297161337, 'microPrec': 0.9876425671236713, 'weightPrec': 0.989780947295493, 'macroRec': 0.9645793371466771, 'microRec': 0.9876425671236713, 'weightRec': 0.9876425671236713, 'macroF1': 0.9124775713167262, 'microF1': 0.9876425671236713, 'weightF1': 0.9883289918797936, 'subsetAcc': 0.9876425671236713, 'IoU': 0.8493291529298799, 'debris Prec': 0.7452964426877471, 'debris Rec': 0.9399175860693872, 'debris F1': 0.831368860409747, 'debris IoU': 0.7114039941646965}\n","output_type":"stream"},{"name":"stderr","text":"epoch 29/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:32<00:00,  1.27s/it, loss=0.0176]\n","output_type":"stream"},{"name":"stdout","text":"########### Validation Set Evaluation : #############\nSaved best model with validation metric: 0.7440892352030741\n{'macroPrec': 0.8941023416780092, 'microPrec': 0.9896716244305392, 'weightPrec': 0.990824277456518, 'macroRec': 0.9592041756609143, 'microRec': 0.9896716244305392, 'weightRec': 0.9896716244305392, 'macroF1': 0.923958555906869, 'microF1': 0.9896716244305392, 'weightF1': 0.9900655288836847, 'subsetAcc': 0.9896716244305392, 'IoU': 0.8667205430134173, 'debris Prec': 0.790676572335961, 'debris Rec': 0.9266250166157118, 'debris F1': 0.8532696838948561, 'debris IoU': 0.7440892352030741}\n","output_type":"stream"},{"name":"stderr","text":"epoch 30/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [02:33<00:00,  1.28s/it, loss=0.0894]\n","output_type":"stream"},{"name":"stdout","text":"{'macroPrec': 0.8740277127094431, 'microPrec': 0.9855758405804708, 'weightPrec': 0.988695540596221, 'macroRec': 0.9811933815090133, 'microRec': 0.9855758405804708, 'weightRec': 0.9855758405804708, 'macroF1': 0.9201012030655795, 'microF1': 0.9855758405804708, 'weightF1': 0.9864787885047327, 'subsetAcc': 0.9855758405804708, 'IoU': 0.8603707907291479, 'debris Prec': 0.7490804325518984, 'debris Rec': 0.9764180547981851, 'debris F1': 0.8477731672346718, 'debris IoU': 0.7357693321548743}\n########### Validation Set Evaluation : #############\nNo improvement for 1/10 epochs\n{'macroPrec': 0.8812201906380206, 'microPrec': 0.9884503128668511, 'weightPrec': 0.9900739822127839, 'macroRec': 0.9599220372628194, 'microRec': 0.9884503128668511, 'weightRec': 0.9884503128668511, 'macroF1': 0.9165670278013301, 'microF1': 0.9884503128668511, 'weightF1': 0.9889904488001161, 'subsetAcc': 0.9884503128668511, 'IoU': 0.855464189029663, 'debris Prec': 0.7648217020345658, 'debris Rec': 0.9294164562009837, 'debris F1': 0.8391239123912392, 'debris IoU': 0.7228367621213687}\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"\n\n# # Save everything in a checkpoint\n# checkpoint = {\n#     'model_state_dict': model.state_dict(),\n#     'optimizer_state_dict': optimizer.state_dict(),\n#     'scheduler_state_dict': scheduler.state_dict(),\n#     'epoch': 10  # Optional: Save the epoch number\n# }\n\n# torch.save(checkpoint, 'model_checkpoint_8_epochs_bs16_iou075.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:42:56.925738Z","iopub.execute_input":"2025-05-22T17:42:56.926292Z","iopub.status.idle":"2025-05-22T17:42:57.531888Z","shell.execute_reply.started":"2025-05-22T17:42:56.926240Z","shell.execute_reply":"2025-05-22T17:42:57.531124Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Load the saved state_dict\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\"))\nmodel = model.to(device)\n# Set the model to evaluation mode\nmodel.eval()\n\nmarida_test_df = create_marida_df(MARIDA_path, 'test')\nempty_df =  pd.DataFrame(columns=marida_test_df.columns)\nmarida_test_ds = MergedSegmentationDataset(marida_test_df, empty_df, global_bands_mean, global_bands_std, transform=transformTest, standardization= standardization )\n\nmarida_testLoader = DataLoader(marida_test_ds, \n                        batch_size=batch_size, \n                        shuffle=False,\n                        collate_fn=custom_collate_fn,\n                        #worker_init_fn=worker_init_fn,\n                        #generator=torch.Generator().manual_seed(seed) \n                        )\n\ntest_metrics_history = []\nmodel.eval()\nyTrue = []\nyPredicted = []\ntestLossF = []\nwith torch.no_grad():\n    for image, target, _ in marida_testLoader:\n\n        image, target = image.to(device), target.to(device)\n        logits = model(image)\n        # print(f'image dtype {image.dtype}')\n        # print(f'logits dtype {logits.dtype}')\n        # print(f'target dtype {target.dtype}')\n        # print(f'test - target shape {target.shape}')\n        #print(f'test - logit shape {logits.shape}')\n        loss = criterion(logits, target)\n\n        logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n        logits = logits.reshape((-1,output_classes))\n        target = target.reshape(-1)\n        ###################################################################################\n        mask = target != -1\n        ###################################################################################\n        \n        # bg_logits = logits[~mask]\n        # bg_target = target[~mask]\n        \n        logits = logits[mask]\n        target = target[mask]\n        \n\n        probs = F.softmax(logits, dim=1).cpu().numpy()\n        print(f'test - probs shape {probs.shape}')\n        target = target.cpu().numpy()\n        # testBatches += target.shape[0]\n        testLossF.append((loss.data*target.shape[0]).tolist())\n        yPredicted += probs.argmax(1).tolist()\n        yTrue += target.tolist()\n\n\n        # bg_probs = torch.nn.functional.softmax(bg_logits, dim=1).cpu().numpy()\n        # bg_target = bg_target.cpu().numpy()\n\n        # bg_yPredicted += bg_probs.argmax(1).tolist()\n        # bg_yTrue += bg_target.tolist()\n    \n    yPredicted = np.asarray(yPredicted)\n    yTrue = np.asarray(yTrue)\n    acc = Evaluation(yPredicted, yTrue)\n    test_metrics_history.append(acc)\n\n\n    # bg_yPredicted = np.asarray(bg_yPredicted)\n    # bg_yTrue = np.asarray(bg_yTrue)\n    # bg_acc = Evaluation(bg_yPredicted, bg_yTrue)\n    print(acc)\n                    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:42:57.532864Z","iopub.execute_input":"2025-05-22T17:42:57.533158Z","iopub.status.idle":"2025-05-22T17:43:41.324550Z","shell.execute_reply.started":"2025-05-22T17:42:57.533137Z","shell.execute_reply":"2025-05-22T17:43:41.323920Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1828429878.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/best_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"test - probs shape (7177, 2)\ntest - probs shape (17871, 2)\ntest - probs shape (2032, 2)\ntest - probs shape (4769, 2)\ntest - probs shape (3834, 2)\ntest - probs shape (3491, 2)\ntest - probs shape (18137, 2)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1683949948.py:4: RuntimeWarning: invalid value encountered in less\n  invalid_mask |= image < -1.5\n/tmp/ipykernel_31/1683949948.py:5: RuntimeWarning: invalid value encountered in greater\n  invalid_mask |= image > 1.5\n","output_type":"stream"},{"name":"stdout","text":"test - probs shape (21602, 2)\ntest - probs shape (9102, 2)\ntest - probs shape (7595, 2)\ntest - probs shape (5344, 2)\ntest - probs shape (14574, 2)\ntest - probs shape (1183, 2)\ntest - probs shape (19516, 2)\ntest - probs shape (6836, 2)\ntest - probs shape (3626, 2)\ntest - probs shape (1100, 2)\ntest - probs shape (11923, 2)\ntest - probs shape (23276, 2)\ntest - probs shape (5089, 2)\ntest - probs shape (1839, 2)\ntest - probs shape (4423, 2)\ntest - probs shape (524, 2)\n{'macroPrec': 0.8053284241759464, 'microPrec': 0.9916197533651848, 'weightPrec': 0.9940741322121762, 'macroRec': 0.9557934205227061, 'microRec': 0.9916197533651848, 'weightRec': 0.9916197533651848, 'macroF1': 0.86512869850745, 'microF1': 0.9916197533651848, 'weightF1': 0.9924475568580696, 'subsetAcc': 0.9916197533651848, 'IoU': 0.7859714204563271, 'debris Prec': 0.6116978066612511, 'debris Rec': 0.919039869812856, 'debris F1': 0.7345147130547879, 'debris IoU': 0.5804213771839671}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"! mv best_model.pth model_30_epochs_ratio_1_20_bs16_iou_078.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:51:03.978891Z","iopub.execute_input":"2025-05-22T17:51:03.979188Z","iopub.status.idle":"2025-05-22T17:51:04.131551Z","shell.execute_reply.started":"2025-05-22T17:51:03.979164Z","shell.execute_reply":"2025-05-22T17:51:04.130783Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# All black\n# /kaggle/input/litter-windrows-patches/patches/S2A_MSIL1C_20180916T101021_R022_T33TUL/S2A_MSIL1C_20180916T101021_R022_T33TUL_366560_5053920.tif","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:43:41.325231Z","iopub.execute_input":"2025-05-22T17:43:41.325455Z","iopub.status.idle":"2025-05-22T17:43:41.329006Z","shell.execute_reply.started":"2025-05-22T17:43:41.325439Z","shell.execute_reply":"2025-05-22T17:43:41.328239Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"#Lightning implementation. To be used later.\n\n# class BinaryClassificationModel(pl.LightningModule):\n#     def __init__(self, hparams):\n#         super().__init__()\n#         self.save_hyperparameters(hparams)\n\n#         # Model selection\n#         if hparams.model_name == \"resattunet\":\n#             self.model = ResidualAttentionUNet(11, 11)\n#             # Modify for binary classification\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)  # Binary output\n#             )\n#         elif hparams.model_name == \"attunet\":\n#             self.model = AttentionUNet(11, 11)\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)\n#             )\n#         elif hparams.model_name == \"unet\":\n#             self.model = UNet(11, 11)\n#             self.model.decoder = nn.Sequential(\n#                 self.model.decoder,\n#                 nn.AdaptiveAvgPool2d(1),\n#                 nn.Flatten(),\n#                 nn.Linear(11, 2)\n#             )\n#         else:\n#             raise ValueError(\"Invalid model name\")\n\n#         # Loss function\n#         if hparams.focal_loss:\n#             self.criterion = FocalLoss()\n#         else:\n#             weight = gen_weights(class_distr, c=1.03)[:2]  # Binary classes\n#             self.criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=-1)\n\n#         # Track best metrics\n#         self.best_macro_f1 = 0.0\n#         self.best_micro_f1 = 0.0\n#         self.best_weight_f1 = 0.0\n\n#     def forward(self, x):\n#         return self.model(x)\n\n#     def training_step(self, batch, batch_idx):\n#         images, labels, _ = batch\n#         logits = self(images)\n#         loss = self.criterion(logits, labels)\n#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def validation_step(self, batch, batch_idx):\n#         images, labels, _ = batch\n#         logits = self(images)\n#         loss = self.criterion(logits, labels)\n#         probs = torch.softmax(logits, dim=1).cpu().numpy()\n#         labels = labels.cpu().numpy()\n#         preds = probs.argmax(1)\n#         return {\"loss\": loss, \"preds\": preds.tolist(), \"labels\": labels.tolist()}\n\n#     def validation_epoch_end(self, outputs):\n#         preds = np.concatenate([o[\"preds\"] for o in outputs])\n#         labels = np.concatenate([o[\"labels\"] for o in outputs])\n#         loss = torch.stack([o[\"loss\"] for o in outputs]).mean()\n#         acc = Evaluation(preds, labels)\n\n#         self.log(\"val_loss\", loss, prog_bar=True)\n#         self.log(\"val_macro_precision\", acc[\"macroPrec\"], prog_bar=True)\n#         self.log(\"val_macro_recall\", acc[\"macroRec\"])\n#         self.log(\"val_macro_f1\", acc[\"macroF1\"])\n#         self.log(\"val_micro_precision\", acc[\"microPrec\"])\n#         self.log(\"val_micro_recall\", acc[\"microRec\"])\n#         self.log(\"val_micro_f1\", acc[\"microF1\"])\n#         self.log(\"val_weight_precision\", acc[\"weightPrec\"])\n#         self.log(\"val_weight_recall\", acc[\"weightRec\"])\n#         self.log(\"val_weight_f1\", acc[\"weightF1\"])\n#         self.log(\"val_iou\", acc[\"IoU\"])\n\n#         # Update best metrics\n#         if acc[\"macroF1\"] > self.best_macro_f1:\n#             self.best_macro_f1 = acc[\"macroF1\"]\n#         if acc[\"microF1\"] > self.best_micro_f1:\n#             self.best_micro_f1 = acc[\"microF1\"]\n#         if acc[\"weightF1\"] > self.best_weight_f1:\n#             self.best_weight_f1 = acc[\"weightF1\"]\n\n#     def configure_optimizers(self):\n#         optimizer = optim.Adam(\n#             self.parameters(),\n#             lr=self.hparams.initial_lr,\n#             weight_decay=self.hparams.decay_lr\n#         )\n#         if self.hparams.scheduler_lr == \"rop\":\n#             scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n#                 optimizer, mode=\"min\", factor=0.1, patience=10, verbose=True\n#             )\n#             return {\n#                 \"optimizer\": optimizer,\n#                 \"lr_scheduler\": scheduler,\n#                 \"monitor\": \"val_loss\"\n#             }\n#         else:\n#             scheduler = optim.lr_scheduler.MultiStepLR(\n#                 optimizer, milestones=[40, 80, 120, 160], gamma=0.5, verbose=True\n#             )\n#             return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n#     def train_dataloader(self):\n#         transform = transforms.Compose([\n#             transforms.ToTensor(),\n#             RandomRotationTransform([-90, 0, 90, 180]),\n#             transforms.RandomHorizontalFlip(),\n#             transforms.Normalize(bands_mean, bands_std)\n#         ])\n#         dataset = MergedSegmentationDataset(\n#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n#             transform=transform\n#         )\n#         return DataLoader(\n#             dataset,\n#             batch_size=self.hparams.train_batch_size,\n#             shuffle=True,\n#             num_workers=4,\n#             worker_init_fn=seed_worker,\n#             generator=torch.Generator().manual_seed(0)\n#         )\n\n#     def val_dataloader(self):\n#         transform = transforms.Compose([\n#             transforms.ToTensor(),\n#             transforms.Normalize(bands_mean, bands_std)\n#         ])\n#         dataset = MergedSegmentationDataset(\n#             dataset1_paths=(\"path/to/dataset1/images\", \"path/to/dataset1/masks\"),\n#             dataset2_paths=(\"path/to/dataset2/images\", \"path/to/dataset2/masks\"),\n#             transform=transform\n#         )\n#         return DataLoader(\n#             dataset,\n#             batch_size=self.hparams.test_batch_size,\n#             shuffle=False,\n#             num_workers=4,\n#             worker_init_fn=seed_worker,\n#             generator=torch.Generator().manual_seed(0)\n#         )\n\n# def seed_worker(worker_id):\n#     worker_seed = torch.initial_seed() % 2**32\n#     np.random.seed(worker_seed)\n#     random.seed(worker_seed)\n\n# def main():\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument('--train_batch_size', type=int, default=8)\n#     parser.add_argument('--test_batch_size', type=int, default=4)\n#     parser.add_argument('--total_epochs', type=int, default=50)\n#     parser.add_argument('--experiment_name', type=str, required=True)\n#     parser.add_argument('--initial_lr', type=float, default=1e-3)\n#     parser.add_argument('--decay_lr', type=float, default=0)\n#     parser.add_argument('--scheduler_lr', type=str, default=\"ms\")\n#     parser.add_argument('--focal_loss', type=bool, default=False)\n#     parser.add_argument('--model_name', type=str, default=\"resattunet\")\n#     args = parser.parse_args()\n\n#     # Set seeds for reproducibility\n#     pl.seed_everything(0, workers=True)\n\n#     # Initialize model\n#     model = BinaryClassificationModel(args)\n\n#     # Logger\n#     logger = TensorBoardLogger(save_dir=args.experiment_name, name=\"logs\")\n\n#     # Callbacks for saving best models\n#     checkpoint_macro = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestMacroF1Model\",\n#         monitor=\"val_macro_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n#     checkpoint_micro = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestMicroF1Model\",\n#         monitor=\"val_micro_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n#     checkpoint_weight = ModelCheckpoint(\n#         dirpath=args.experiment_name,\n#         filename=\"bestWeightF1Model\",\n#         monitor=\"val_weight_f1\",\n#         mode=\"max\",\n#         save_top_k=1\n#     )\n\n#     # Trainer\n#     trainer = pl.Trainer(\n#         max_epochs=args.total_epochs,\n#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n#         devices=1,\n#         logger=logger,\n#         callbacks=[checkpoint_macro, checkpoint_micro, checkpoint_weight],\n#         deterministic=True\n#     )\n\n#     # Train\n#     trainer.fit(model)\n\n# # if __name__ == \"__main__\":\n# #     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T17:43:41.329791Z","iopub.execute_input":"2025-05-22T17:43:41.330025Z","iopub.status.idle":"2025-05-22T17:43:41.345679Z","shell.execute_reply.started":"2025-05-22T17:43:41.330002Z","shell.execute_reply":"2025-05-22T17:43:41.345141Z"}},"outputs":[],"execution_count":52}]}